{"docstore/metadata": {"d28eff0a-18ab-4237-8d97-144c507fbb58": {"doc_hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984"}, "c5ce303c-ad6c-40dd-aa49-9e12eec83bfe": {"doc_hash": "793a20d5a47985f486f65f8e799165c30be91f7255fb1db34351942bd4f018a4"}, "9dacc483-da87-4551-8534-45462c631eea": {"doc_hash": "2f9b54693bb5c8bc524ba41cc5863c5be373d9c7bbe919c1bb183055d2185f76"}, "a171992a-3591-4a25-83be-636a42048c9b": {"doc_hash": "6d3371eec2cdb5afe6a3449961a30595962de03e34f867972e9c07009e7ee0f7"}, "033d406e-95b0-48f7-b993-eb307b58b658": {"doc_hash": "7b8d22da4035c5a27ba0392b6064a0971d2ab43b7606c4dc87b4b447b177118c"}, "2bdccaa9-8179-43af-becb-947517d6ab29": {"doc_hash": "5de057cd086be970a4d60d336e493ffca6ed9d21e7f2de1508686a3bdf1f178e"}, "1a936e59-3c43-42b2-9a75-7ce502ef62b4": {"doc_hash": "76ae21ab20697a0debc47e178e43fd84a56921d091b2a8f522308710a8daec73"}, "82947c5a-974e-476d-b8ff-e4ddeecfd338": {"doc_hash": "692a5f8c15570c81b73ab3f5b4b74fefeedae2d17c77120aa4143442e23bad05"}, "0e46527e-2f04-4c43-96d0-b596de07a7de": {"doc_hash": "1bcd2950be2a85c5e832182099844bb273f67297aca237f07ab973660a2df3af"}, "72a9d8da-62fd-4ffc-8985-27ea2399fae8": {"doc_hash": "2b101d443a48b2ccf86aedc55486bd75cd2775202288a3c4179214618721cd1e"}, "7c19e2b4-12da-4321-826f-5b370eadcfda": {"doc_hash": "1f97dcb3b2cbecb5cfa701c5cdd047239f43627341b80373020c1dd39a882a68"}, "9c903ede-5c20-4748-a114-aa6676812ea7": {"doc_hash": "1d8b60c97c8db1d5ca07d165ea9f575b6a0686ba6e014f636715d3f74b156163"}, "70b409e0-4886-46c1-af30-7609043d7cbe": {"doc_hash": "65217e0bc992bcf36c6a6f60e9ddf004d5a6909fe2d8eddd2cc4332df40cbe83"}, "aec571bd-a084-4a8b-8c3d-80fdd38296db": {"doc_hash": "bcfc5518d56c17538770a8062e8083735bf427698a1093caa922df792f0e44c7"}, "9d442fb3-aa81-4b8a-91cf-8539cdc3b542": {"doc_hash": "c52027d82e529d668b280f8995356f6964712fdc301d12b753a3b26285f547da"}, "a6f4cf13-8330-4b9f-8376-2c9faaec52d5": {"doc_hash": "0c374be705a6da7ad8c4b79f718ee9a863e964c8c43c250a8cd934a64fb25d64"}, "ee6b0828-3aa5-46ad-a086-b2aff9f5c5b7": {"doc_hash": "47d8ed5e8aeea5a06ad41bc606c3567b9a3f85abe77337369f0222482dc99586"}, "3d92f157-095c-41c6-ad0f-5579f681822b": {"doc_hash": "b42cef9ac6cba7079d0987113b50e342c0062b371e6620a3bd023a041f9fb00d"}, "760e0faa-dba3-467b-a73e-bbb6e5bf9ad7": {"doc_hash": "d9da94c36967595c1c690831356d60b3fe072ae9ec9a1642bb5ff983f7414ea0"}, "6700ab6e-e8c4-4e50-911e-60e895caf106": {"doc_hash": "de60b4aae61b4fb9b2bd1830eae335da7f8014dd040801e25985dd9992224695"}, "ae861b4c-bbb2-4f4e-b8a5-7cb33c71a5a4": {"doc_hash": "a5fd02a732bba832a3f4e185da5d0a2f758218800ca098f96055abee1cd1c611"}, "e8970bd2-ddc9-4ec7-a886-74734c577316": {"doc_hash": "d3a6905088499202db2715ca93549c6372a30e5edd88ac92ea7c4c862f05c10b"}, "0e8aef4d-f33b-4707-93d9-601d9cec0b34": {"doc_hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347"}, "3454decf-3339-4011-8950-c757b494954f": {"doc_hash": "82958aff0f45f7850c790b7ee183e553eda1055c6644e7c0c89216f713388881"}, "e2b65dd1-be59-4e27-b3f3-ee7ea3b5ffed": {"doc_hash": "c9da72056c979f26b679d8328cb62b02320bda26bf394a619dc1dfb70e96e12d"}, "dabb47b5-df7c-4d6a-ae0a-3523a3f56a71": {"doc_hash": "21aa88fadcc44b11a398e77f081c224881691ec3b34287c090ad493c412c5c65"}, "cbc0e080-9642-4480-9dc7-3d8d3291e033": {"doc_hash": "1a9f6810c633cfec66fa3749e896be6ebd45fa72817f4c0531c07b33f28328b8"}, "484c4621-a67c-4865-a4b5-c0304f8f5961": {"doc_hash": "47a42ed5250f74daf74ea47e1a37a2007dbc557002a6c55037fa727a65637e06"}, "05aede4a-45f8-4c4c-950f-e347a9e65901": {"doc_hash": "1a1356fd8060c549e85e018c38971a2b94f098bbb4a69a28a472f963d5075366"}, "79ecb65a-68f9-4a0b-9e89-bca2721d0e70": {"doc_hash": "3421fb09598cbcf6140549ee19fd418441524a887525c347d80c28fcded9973d"}, "f1d799b6-bdbc-4bd9-9ee3-09bf53fc77ad": {"doc_hash": "1f24556d1cab06b7da983d74163db8a2909a7f1378957fee6acb103660d279c4"}, "44c8cc16-cd61-4092-a3cd-c91558f7253b": {"doc_hash": "11bbf1d5053aeeace3ca21c1f821aa319f185b55933469a8ac72afd3f481c3bd"}, "43e996c9-4077-4398-b7f2-cf1fb19d243d": {"doc_hash": "3871380761a6ffdab7a79578cc28a7f0ad4283cc34da7dd2f1fdfe6b1be7aac1"}, "67520dd1-cfec-49d6-9d40-a0ea907e520b": {"doc_hash": "38b68b3147d10f87cc48fb4f41aee290981a9e55063aad6d2fc21fd87d7e7186"}, "cf8a2b13-b8db-4bf6-8ca3-23b817085865": {"doc_hash": "9c42a7a81604c90e400dd0a0d89f6fba0a7d405a02c73bedfa63b38873a4d9e0"}, "d3b611e2-c0e3-41d6-87da-2068c38a9da0": {"doc_hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2"}, "3c6913da-33d4-4e1f-b2ed-aa431e0e647b": {"doc_hash": "ea3332835be468ac60602a16727a3087d72b946ad8d15be776fe92e9d9ea1f80"}, "a2a375c1-9a18-423c-bf9c-7eb83fe2e533": {"doc_hash": "e1f54a1389e2cb0f88799ab8d977741525105a6dcffc2c036bb5a45a8bc4d231"}, "c7b7c6c8-917c-4718-ad37-77d919bbca4a": {"doc_hash": "0da4639d00644759d25ee246c208105e648eb4562cf15588ecd5af9a8551b045"}, "6cc23001-518b-4ab4-9853-e1d69e9ca2b2": {"doc_hash": "ebe70135d0c2c4f5b9fef2d4820596aef913bfea710a8f04601ae119ce964ebf"}, "b373fb97-7e0f-4685-89a2-84d10a145106": {"doc_hash": "4c1ea93f3dfcf18e6858821cc13bea91ce74527111472030ce466ea4044fbd54"}, "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32": {"doc_hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49"}, "0777f580-1526-4acb-94eb-331f78f1bde6": {"doc_hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8"}, "14ace246-d7c5-4b5d-b750-3e4795e2d6af": {"doc_hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436"}, "18d71101-b08d-4f4b-b097-ec770f400f1b": {"doc_hash": "1e0bdb713d09c6d6164efe01ac03f1291bc8ce681104a9cf8e51ecef7723ad6e"}, "4a6ba5cc-7170-41ee-bea9-ea051ed6394a": {"doc_hash": "ddf7906a430712c4faa74aff4ed3b7c9fa6f1a344184ccb7be6982d11fc45c59"}, "d277908f-9e75-4db2-ac71-1610cd8017f5": {"doc_hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b"}, "154b557d-9b90-41c7-9626-4b0adc9e788b": {"doc_hash": "95924f4a82b0c1d6d07d3819f91161b2a05ecbc5cd0a93a203aa54cd98b08822"}, "47553964-3793-41d2-a284-6165612d1a74": {"doc_hash": "f56e75bfd665276405cc0f2450f37c59a46ace8cb5a642a73c2fac190c00ea73"}, "695bb35a-8a71-4d1d-82da-2bc699f404ac": {"doc_hash": "e100811cadd7f5183bed52089fb2154dbb9b16cd22dd3406bc88a8d509212d04"}, "17fcb1f3-98fa-4246-9ddb-972116280b92": {"doc_hash": "0f817ee69d87bf7800d3d885b4dad6f356c0f55e55734abc84d19a306e66fa4b"}, "557e1cdd-dc48-4cff-912a-420b02eb869e": {"doc_hash": "b93730dc9738e19a05b008e9988e5bad9ef0bcfb7b67d4a983d56f9b3f145030"}, "63b8f18b-e3f6-45a7-8cbb-f46400494271": {"doc_hash": "7bb0cc0a2e51dd466e92dfb9e5d3155d5eb59a045a34db30afde95177b0136db"}, "813c12ae-09fd-41fa-86ab-0cfe7f758fa0": {"doc_hash": "dd09aa4b32d81d4e4a31a153d7664e4f812160b0856d1a1c2c0e621eaf26c4e2"}, "8342d416-b313-49ab-9337-880afb976b83": {"doc_hash": "532acafbd0685fd01f62e2793748300a87e2a07e0f8b1e8944101bf5a57ca65b"}, "af595f59-96f0-4ad5-b2fe-595af188c282": {"doc_hash": "0b2db8e4a234a0d89a08035ca4b24488b9139aeb76dfa78f5266db1ebba773e3"}, "1f63904d-b61d-4390-a64a-3d17ec05955c": {"doc_hash": "bdce76e236cc9569ba8b71197de2b5fefbe62846e586aebf59494d12f2579dc2"}, "7bd8f822-b31f-4222-9459-aa9c63ad01f3": {"doc_hash": "b1cd265339e8c459ed9efbfdd93e2a3c56605ba5a1b7627e88cd9ed4f92ebf06"}, "e6c8e69e-c2d2-4984-9d58-18ed3f69a0ec": {"doc_hash": "4ed33576e6ba9ca6b571aafb2031e492540190ca9812dc69b66996e062628f30"}, "7b636796-19bc-48ab-982d-4afb8f11f79f": {"doc_hash": "1fd4d19ef8b7bb42b7e4a09335d4de2b0190f36f79fdc7ab6d87af9246ea3f5a"}, "accdef0a-adb2-46ee-9e97-c1478718c699": {"doc_hash": "f18694540a41319234b7973f92e038abf26ca9553c8fe4812395f238ed1f5074"}, "ac3c5d25-9808-4aa2-9bbb-d11790a2927d": {"doc_hash": "97e0884dd3c6a41edbf3bcdfe1b5514890b83e27a32e48ae62a77b03884bc35e"}, "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22": {"doc_hash": "6b77265eecf682c1232138abd321af2c2c4e8090ded7787d21be2839815d690e"}, "3e44ad02-ee97-4c02-9ae1-8930d56a3c02": {"doc_hash": "48bd42ce99cc84d4843101bfddbdba8ef5d2cb70b95e45420cec6e6f0c1fc107"}, "e0560772-764c-463e-b831-a8178937b7d4": {"doc_hash": "376bdf465ace8efb636ceda2cfd49301cec23459830c295da410155d8d2584a0"}, "77f8abb2-f02a-439c-a585-319ef7020a2b": {"doc_hash": "693c52155242aba63a2967fc8b2ada1a1cfdbf2ff520e4e903baf403d0277d3a"}, "10ad1a3b-5890-4bbe-86a4-fa5804de378a": {"doc_hash": "897e8fac7a55ac98995c0dbcdb7c503a94800198f102940227b76f69bee91a77"}, "c3ba77db-2be1-4d05-af44-1c8934130a30": {"doc_hash": "3aa9247a6ac2d777a39b39c4b7ee65171c45011ca2a5bfce3fa73e3d1a173f7b"}, "5a04119e-10eb-4d21-ac85-6a31aa62323c": {"doc_hash": "6e518eeb2f0434605ffa037d61250734f854ac9148882d831647ddadbcecf825"}, "fd045767-fee8-42ca-8c2d-f37d35d96f27": {"doc_hash": "1654039863612671777b41bd8d593c735bad6f17cfaae0d4f6900485ff8ad5ed"}, "b50c87bf-bbeb-421f-9d26-e7977876a719": {"doc_hash": "c0b0468290918ed11e1d2a529b2d763187c2cc46d577d09992f8ed2e5450d4d9"}, "27f46b58-61aa-4430-9c6f-873c68eb215d": {"doc_hash": "005faa8c025c31e312025bddf6288073b083ee4d3df3853f8c9e3b84891b3474"}, "7b1ed82c-a0d9-4eb1-8eac-7fd7d6affd80": {"doc_hash": "7d83c57056a532898c04110f362a085281f590f0b213579ff7e6247aa73782e3"}, "7e0fb88f-703b-4650-ab94-c22f631a0b06": {"doc_hash": "2775d715022787d3194f22b79368ce2fad59702bd83401db2efe0f4eb9ee40d8"}, "948df15a-be8c-4f88-b3e4-9355ee98ac2c": {"doc_hash": "d57d5aa7e829f18c3e2ca2a6ce5d31203fb1ec7b84d47230294552d6ca6541cd"}, "62797bc1-3ba9-4c2d-b966-f2d4647412f5": {"doc_hash": "8f8ec2e8a0127a216f1881290a36959c2dae3ee9ba74b9cdd2174a05b0c6d845"}, "b1206b8e-d52b-41ed-b71f-9004c9955b76": {"doc_hash": "9dd2b136680958fa93f61d397b4843f728f7cb56ab80e150b6540779944c99ad"}, "16d4c693-e855-46f8-8439-c17d78b39714": {"doc_hash": "05fa79fed3a7d93b3333abcaa2de154b5da570699804e97c143069a323d11d8a"}, "f57e34bb-b97f-44dc-8f63-38cb2a3086ce": {"doc_hash": "ce9d246ab0a3019aaf4f2f192ef539c64c523d32f0cf4373c48bd816a38b41eb"}, "3b3b2f75-4fad-430c-a8cb-b92f356a8be0": {"doc_hash": "e166bb60326e3a24bf1ba78fd5e22576067bf99b2c673ddd01bb3050118ff23b"}, "23af0791-bc75-461b-b544-b96bfcefad33": {"doc_hash": "b0614ebd322edbd6b4915b9b7c7a43dbf33098bf2f6b9a70c22126bc1a647eab"}, "7d65309a-30f3-4042-9418-7357a09a55fb": {"doc_hash": "c03010834e2188d0e4a359405c0e86adfdb6a7fafe43798c8ba4c51e3e2c7605"}, "ce509453-c3c8-4755-a4af-2b30c81d511a": {"doc_hash": "ab71c52a5200f358708c29eaa92c54ca25d6702631ec74671ea72926c39df987"}, "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33": {"doc_hash": "0ac460b3980e89486e186236e4f53e4b6985db57dde899ff7369ed958595a13a"}, "6df731ef-d2b7-4d19-9a5d-228650b089d4": {"doc_hash": "b29187684c9b799a3795c53db9cb8ee281109baa8cd3c5de222a7b8eca231909"}, "88627a8d-7413-4d6e-98a5-abec56874b3d": {"doc_hash": "1e96a8ecf61fb925c7ece6491c67b39aa32de3e4b7bac1a91cdd2f951866215e"}, "8553a56e-f227-4bb6-9150-43d1dd6ebcba": {"doc_hash": "0c673820765bc53ac84768f0f86f6a615810d7f61dcbc79176e4102a4dcded30"}, "fc6ffe91-1fa2-4a8d-a328-db99a4b52247": {"doc_hash": "fdd8d66e2b93af1f5fef746d2aa43e8ce3a7fa09a3203cfc63f13d42dac84e99"}, "a6d77317-c6aa-44b1-99e4-34bbd4578c52": {"doc_hash": "8209a2b949ab81bfd749746ed7c391ce2ed0050c76bff4f708fc52000843a2df", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "3fa3a030-8a05-4b6d-94c5-b64404555559": {"doc_hash": "1acbe08e00bf45d0a4da5a8e84d47415b63b943d4ef167eb2ce2bf9434b0cdc9", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "65efbccb-84d5-4dd0-bebf-d83f486dc840": {"doc_hash": "9cb52cd1bd8d7380c4dfa8538501d53fa12225fa723a3c9e357abfdaa5c91a0f", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "c47579a0-ad0d-4c6c-8915-9736b526418d": {"doc_hash": "7a7d61ba11be29f955df114466bbc7e581e8ccac9ea3f21ad647fbcc1e846c23", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "eb343a12-8086-4941-9fde-e183e8cd0eff": {"doc_hash": "ff8aec81337e60146c492183dae3111d9efab1aadf571a87beae6cc4fe693eff", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "bc244b08-d363-4258-acd7-45d946aa851b": {"doc_hash": "168bb936ad8ff2989cee8cb889ce2400dfad3575be3aa6905e42c92324fb5c9d", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "937eb72f-8c84-42ff-979c-74e235cf5d2b": {"doc_hash": "08d3abfbc811b1b2f9ac6a325d0c2a82eb5de36be6e0208c8da834675986704a", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "3250d162-a53d-4095-b7d4-29284cef8792": {"doc_hash": "a249086e02051c2a9696d5adcdd32516a4be675166508994317faaafbb41c315", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "80f61b93-5c13-4c13-a94e-a89553dec243": {"doc_hash": "656dff46a49bfdfe691ff317e1f16d92459636de5094cd3f59a4ba7e4e09da26", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "701acdd0-badf-4138-9b89-2d665b832cc3": {"doc_hash": "f544304525edf497efcea0ca130ca7d83bb17346e3a241d1b836957b92d81391", "ref_doc_id": "d28eff0a-18ab-4237-8d97-144c507fbb58"}, "aa9c193c-d6f3-4e56-bbec-91b154dc0a2e": {"doc_hash": "793a20d5a47985f486f65f8e799165c30be91f7255fb1db34351942bd4f018a4", "ref_doc_id": "c5ce303c-ad6c-40dd-aa49-9e12eec83bfe"}, "76e7a31e-38b4-41dc-ae82-38664ffabcd2": {"doc_hash": "4b9ddbb24f4ad6089ce5de7e20164cadfc6a91e2ef97a0f6616bec2c055056ff", "ref_doc_id": "9dacc483-da87-4551-8534-45462c631eea"}, "a5cc26f4-4862-4fbf-97d0-d67637bf14a2": {"doc_hash": "11ca9dc3403eb305fe655fbfdd11babdf175ae646f276c06f9f5f517210db502", "ref_doc_id": "9dacc483-da87-4551-8534-45462c631eea"}, "ad7185ff-1ab6-4925-8217-9fb039c28f47": {"doc_hash": "6d3371eec2cdb5afe6a3449961a30595962de03e34f867972e9c07009e7ee0f7", "ref_doc_id": "a171992a-3591-4a25-83be-636a42048c9b"}, "c546e1af-8537-4161-b2da-dbb3bc04d1ec": {"doc_hash": "7b8d22da4035c5a27ba0392b6064a0971d2ab43b7606c4dc87b4b447b177118c", "ref_doc_id": "033d406e-95b0-48f7-b993-eb307b58b658"}, "3c5165e0-121f-4207-94e5-81a92756a531": {"doc_hash": "5de057cd086be970a4d60d336e493ffca6ed9d21e7f2de1508686a3bdf1f178e", "ref_doc_id": "2bdccaa9-8179-43af-becb-947517d6ab29"}, "0858a46f-6a9d-4907-90a1-6386fc5fcdc5": {"doc_hash": "2dd1409b0b39daef13c3c9a88cb369593772fac22469f43e0dba9c78eb9cd4dd", "ref_doc_id": "1a936e59-3c43-42b2-9a75-7ce502ef62b4"}, "0ac9a1af-9b0f-4eef-8734-05f4b649c364": {"doc_hash": "30fc4a4731ee87476c4aff4f508deb6871d3ba86dae1a75eb99198929e333fb2", "ref_doc_id": "1a936e59-3c43-42b2-9a75-7ce502ef62b4"}, "0b71cb1e-55df-4435-a5f6-9e9da521d3bb": {"doc_hash": "0aa55601ca7885e79669fbc534ec28e55c872cc3034ed6c9358a114b7475bc7f", "ref_doc_id": "82947c5a-974e-476d-b8ff-e4ddeecfd338"}, "9f21cfc5-7a60-415b-b610-477fa0624b2f": {"doc_hash": "700817a260bcaf3854fa7e95beda5ebddf17582d3f613d8dcbd8b9a2d5e8752e", "ref_doc_id": "82947c5a-974e-476d-b8ff-e4ddeecfd338"}, "b8975607-154e-4eaf-a87f-c0d6ee9ee9eb": {"doc_hash": "ed7c83f4ff0a3540cb190945e989e6a07a368d7f4c6524ff0ba13266b1596ec3", "ref_doc_id": "0e46527e-2f04-4c43-96d0-b596de07a7de"}, "abf9a9c9-2ea4-4ffc-b52e-01544df2a36c": {"doc_hash": "edc6f0e5e91655ee1853b43354384d4d72a97e9b6867e9f5087e1f2ab915d1df", "ref_doc_id": "0e46527e-2f04-4c43-96d0-b596de07a7de"}, "0c39921d-d2e4-4f64-8e60-edf35a8dc280": {"doc_hash": "f8b17420f7f746af377bf8b193ff3ae151066aaadf35c9af5bea8e4796325119", "ref_doc_id": "72a9d8da-62fd-4ffc-8985-27ea2399fae8"}, "0cee5212-1b72-4996-82e3-90c0f22f0c3b": {"doc_hash": "a6a96cf1fa79612e58be8c537210d18e8eee3467cfa06ed926d96044c5ff6557", "ref_doc_id": "72a9d8da-62fd-4ffc-8985-27ea2399fae8"}, "b82a2947-9dee-465a-a58a-cd5e94d57623": {"doc_hash": "ad761c0aaf1540c0dd5cb6ded8a0d7d6bd01871c156ec1572c7b31203895c34b", "ref_doc_id": "7c19e2b4-12da-4321-826f-5b370eadcfda"}, "d7a90f2b-6ef2-4ec8-bd47-acac86917bb5": {"doc_hash": "6d4d0ad7dc97ac172416256d8a5c0e8feb8b92ea52474ec5996cc6ac81c23969", "ref_doc_id": "7c19e2b4-12da-4321-826f-5b370eadcfda"}, "9ea7df02-9a12-45e3-b916-e62ac2770a28": {"doc_hash": "6a4e13f8b25e34f22b391a0e133b4fd312d22d992f73e284672a38c07e7c9296", "ref_doc_id": "9c903ede-5c20-4748-a114-aa6676812ea7"}, "11fd5d26-4837-4783-a61a-b0219f25c704": {"doc_hash": "90ea718c85a6d052ab4a331e5e5bb00e12ea86145e8da9936e95d54108119744", "ref_doc_id": "9c903ede-5c20-4748-a114-aa6676812ea7"}, "4bcf8e40-381a-4712-816b-37878a30424b": {"doc_hash": "41b9843934038be0317b5cb1e8a290e780b35fc859029604ece56b7a7e585298", "ref_doc_id": "9c903ede-5c20-4748-a114-aa6676812ea7"}, "c711ea87-076a-4a57-af9a-6437ba9fb728": {"doc_hash": "65217e0bc992bcf36c6a6f60e9ddf004d5a6909fe2d8eddd2cc4332df40cbe83", "ref_doc_id": "70b409e0-4886-46c1-af30-7609043d7cbe"}, "4c8adbba-9ecc-4a58-8a8b-5c136496892a": {"doc_hash": "bcfc5518d56c17538770a8062e8083735bf427698a1093caa922df792f0e44c7", "ref_doc_id": "aec571bd-a084-4a8b-8c3d-80fdd38296db"}, "6d29d547-7660-47bf-87b4-2a0da43c65a9": {"doc_hash": "c52027d82e529d668b280f8995356f6964712fdc301d12b753a3b26285f547da", "ref_doc_id": "9d442fb3-aa81-4b8a-91cf-8539cdc3b542"}, "c8b979a5-d94a-4032-a1eb-48472e43da59": {"doc_hash": "0c374be705a6da7ad8c4b79f718ee9a863e964c8c43c250a8cd934a64fb25d64", "ref_doc_id": "a6f4cf13-8330-4b9f-8376-2c9faaec52d5"}, "da801ed9-b23a-4bd4-8b8c-3e4ef7ac8ede": {"doc_hash": "47d8ed5e8aeea5a06ad41bc606c3567b9a3f85abe77337369f0222482dc99586", "ref_doc_id": "ee6b0828-3aa5-46ad-a086-b2aff9f5c5b7"}, "c38ead00-8652-4782-80e1-3d388ef027bf": {"doc_hash": "b42cef9ac6cba7079d0987113b50e342c0062b371e6620a3bd023a041f9fb00d", "ref_doc_id": "3d92f157-095c-41c6-ad0f-5579f681822b"}, "5a7be869-f1a5-4c92-8d37-55b1a39d68a2": {"doc_hash": "d9da94c36967595c1c690831356d60b3fe072ae9ec9a1642bb5ff983f7414ea0", "ref_doc_id": "760e0faa-dba3-467b-a73e-bbb6e5bf9ad7"}, "71f65d53-3dc9-491f-b0ab-01fe4b8990d5": {"doc_hash": "de60b4aae61b4fb9b2bd1830eae335da7f8014dd040801e25985dd9992224695", "ref_doc_id": "6700ab6e-e8c4-4e50-911e-60e895caf106"}, "2ccc3e48-e57a-41a8-b549-0b387b2fcfd3": {"doc_hash": "a5fd02a732bba832a3f4e185da5d0a2f758218800ca098f96055abee1cd1c611", "ref_doc_id": "ae861b4c-bbb2-4f4e-b8a5-7cb33c71a5a4"}, "9a7bc50d-adbd-49d9-9e73-9c48cefdca4c": {"doc_hash": "d3a6905088499202db2715ca93549c6372a30e5edd88ac92ea7c4c862f05c10b", "ref_doc_id": "e8970bd2-ddc9-4ec7-a886-74734c577316"}, "f805a092-451a-4e9c-b43c-76e21f96743b": {"doc_hash": "6ccd4de55ccb1d76ac7f907bc0f6b059a3c857b18ec1f20abf00465a0f891a2b", "ref_doc_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34"}, "915693c2-07c6-4bd3-bfb9-7821e5c996c4": {"doc_hash": "607a868080d6fd5f7e6c1f9cbcb99ba4843bb521a439dd97727b8892a3dfc465", "ref_doc_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34"}, "6b9855d3-8ca2-4a48-972d-094a329a1dae": {"doc_hash": "62020d275375bfacffa6ea4901e8cf1ce98d15b256ab4346400d81fd8a22b81c", "ref_doc_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34"}, "d67da453-7b7b-4d95-8a03-6cd2bacbf83f": {"doc_hash": "231d6926048a4ab7897e46a3e82634c75fa3718f6e31ee37bc40e7b0b3446df6", "ref_doc_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34"}, "c9c804f4-9482-46e3-b3a7-e0b9a03e484a": {"doc_hash": "a3096976e658c0fa984ea2a72f10009b82c8cb8ca2166cf15f796e59dc5fc0c0", "ref_doc_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34"}, "827f7d6c-2e71-4c37-86a1-846d963fac74": {"doc_hash": "82958aff0f45f7850c790b7ee183e553eda1055c6644e7c0c89216f713388881", "ref_doc_id": "3454decf-3339-4011-8950-c757b494954f"}, "ea85e861-a31a-49f1-bcea-cb88aa532bef": {"doc_hash": "c9da72056c979f26b679d8328cb62b02320bda26bf394a619dc1dfb70e96e12d", "ref_doc_id": "e2b65dd1-be59-4e27-b3f3-ee7ea3b5ffed"}, "01d3f905-6874-490a-8df0-ffb08bed369b": {"doc_hash": "21aa88fadcc44b11a398e77f081c224881691ec3b34287c090ad493c412c5c65", "ref_doc_id": "dabb47b5-df7c-4d6a-ae0a-3523a3f56a71"}, "02ddf15b-ed7b-47a2-b809-520014d427fd": {"doc_hash": "1a9f6810c633cfec66fa3749e896be6ebd45fa72817f4c0531c07b33f28328b8", "ref_doc_id": "cbc0e080-9642-4480-9dc7-3d8d3291e033"}, "1b43f3e2-2428-4ab3-b96a-187a87b126db": {"doc_hash": "47a42ed5250f74daf74ea47e1a37a2007dbc557002a6c55037fa727a65637e06", "ref_doc_id": "484c4621-a67c-4865-a4b5-c0304f8f5961"}, "58f1fe67-ee1d-4376-96b1-8b405ac2143f": {"doc_hash": "1a1356fd8060c549e85e018c38971a2b94f098bbb4a69a28a472f963d5075366", "ref_doc_id": "05aede4a-45f8-4c4c-950f-e347a9e65901"}, "f8bb3c28-c97f-4101-8d0b-4610277d25da": {"doc_hash": "3421fb09598cbcf6140549ee19fd418441524a887525c347d80c28fcded9973d", "ref_doc_id": "79ecb65a-68f9-4a0b-9e89-bca2721d0e70"}, "73bdee10-6d14-4ee7-92e7-54b32b6c5f12": {"doc_hash": "1f24556d1cab06b7da983d74163db8a2909a7f1378957fee6acb103660d279c4", "ref_doc_id": "f1d799b6-bdbc-4bd9-9ee3-09bf53fc77ad"}, "571ef8c0-d73e-4d49-8df0-4f828c8368ac": {"doc_hash": "11bbf1d5053aeeace3ca21c1f821aa319f185b55933469a8ac72afd3f481c3bd", "ref_doc_id": "44c8cc16-cd61-4092-a3cd-c91558f7253b"}, "f45c9a17-eb5f-4cde-a17e-c19a94884a92": {"doc_hash": "3871380761a6ffdab7a79578cc28a7f0ad4283cc34da7dd2f1fdfe6b1be7aac1", "ref_doc_id": "43e996c9-4077-4398-b7f2-cf1fb19d243d"}, "cbffa9d4-6e37-476f-bb51-313d7592b715": {"doc_hash": "38b68b3147d10f87cc48fb4f41aee290981a9e55063aad6d2fc21fd87d7e7186", "ref_doc_id": "67520dd1-cfec-49d6-9d40-a0ea907e520b"}, "c704674a-d39a-49a1-8725-ab3cb59db398": {"doc_hash": "9c42a7a81604c90e400dd0a0d89f6fba0a7d405a02c73bedfa63b38873a4d9e0", "ref_doc_id": "cf8a2b13-b8db-4bf6-8ca3-23b817085865"}, "29a3533c-7201-468f-b0bf-6371bdf6ff1c": {"doc_hash": "62f6e6877036feb3552efce66d63c0a73821f82c11201331d808f58e0827bb09", "ref_doc_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0"}, "ab1d5049-59c2-484c-ab5e-9c915156cdbd": {"doc_hash": "12d2927e9f9aff930fc98933c454131661665efba96dc3dcf1422a330d8dd605", "ref_doc_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0"}, "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30": {"doc_hash": "3f776bc93bf27bf8196cc73211123325a0658b68efbe58f29eb7a78f739e796c", "ref_doc_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0"}, "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc": {"doc_hash": "2b255a890f93a3b74a109630100707c8cf30e1ad6c06447441777bc20a4dcf3d", "ref_doc_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0"}, "5b5533f4-9c8c-4f90-834a-5234003f3ce1": {"doc_hash": "31f7583de25e0e3762d2d26b3c2fb16c177fc98af4ace443c588c090ac7c6b9b", "ref_doc_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0"}, "8166559f-689f-4923-be10-3464ac53348d": {"doc_hash": "ea3332835be468ac60602a16727a3087d72b946ad8d15be776fe92e9d9ea1f80", "ref_doc_id": "3c6913da-33d4-4e1f-b2ed-aa431e0e647b"}, "5e923a96-3ea6-46de-b35d-098a3c6fdffb": {"doc_hash": "e1f54a1389e2cb0f88799ab8d977741525105a6dcffc2c036bb5a45a8bc4d231", "ref_doc_id": "a2a375c1-9a18-423c-bf9c-7eb83fe2e533"}, "5fe86272-f8fe-44ea-8dea-f78f291817ae": {"doc_hash": "87fcbca43027b1e46eec96be3b344a9a9218db68885c6e6c8a1323efa81e895f", "ref_doc_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a"}, "c7245f55-14cd-4955-8336-6e510cbffbce": {"doc_hash": "885061b08c967bbb191db953ae5c1d6d983205c8295caf41931cee415b6f1087", "ref_doc_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a"}, "27fee787-e2b8-4b3e-a3a0-62b6947902b5": {"doc_hash": "b733f4050f8631addc6f9e24b64d2876568c25a8529a720c802643527fbae2fc", "ref_doc_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a"}, "d5fd340d-ce41-4f6e-bb0a-c3a2ebcfae1a": {"doc_hash": "0f7f993212b35fe1580595dfac33957edad5678bcd68a59013a37627cdc787bb", "ref_doc_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2"}, "facc419e-78a7-419c-ac16-c23755051f8f": {"doc_hash": "fa807c39ffe279a18bdcde8d2738ed6c8f3ee72176e09d32bad2fae6001a6a6c", "ref_doc_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2"}, "6ddc8cbc-3165-46f8-a6d4-9a0d050275be": {"doc_hash": "5b210b222589d67223a3d8f2355fd9e2502aee1eaf45022422cab64ca4b8eaf8", "ref_doc_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2"}, "792f14e3-361a-424f-a47a-7c2c434a373a": {"doc_hash": "e4a7d8aeb0ab278c1da16beb7c92385ec563e9a73f2f0f72b2b024e8e7ad7726", "ref_doc_id": "b373fb97-7e0f-4685-89a2-84d10a145106"}, "029a94c3-83f9-432e-b31f-43363ed39bf8": {"doc_hash": "04c53f5941220b33787b3b01843f7958b9c68f421e3bafe71d333c058ff4421a", "ref_doc_id": "b373fb97-7e0f-4685-89a2-84d10a145106"}, "c63a1ce9-6451-4e1f-96d0-c3a0a10ade8b": {"doc_hash": "d4a3dae4b88ea5bfbaf88ca5a1b52e34fc403b6753bc7ea5f3b108f21c1fbe6b", "ref_doc_id": "b373fb97-7e0f-4685-89a2-84d10a145106"}, "dc697b9b-d9a7-47e3-922c-7c2bfaf342cc": {"doc_hash": "3583a126e6bb7aceb7027654f28ce8f5a85ae28f631e3288611eedf10898c337", "ref_doc_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32"}, "66a64a94-fe82-4f09-af53-4b2d23d05914": {"doc_hash": "f62ba6b70a899d11564a23ae70ff63315f5b274d98a106b60f34b8423c400a11", "ref_doc_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32"}, "18c5c911-ac59-4517-8e56-1edf2ca7558d": {"doc_hash": "ec194972f1a5a3c21cf8865cc71321f694b48134a64f978f533217cddfd1febb", "ref_doc_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32"}, "a9e2837e-078c-4189-8f23-48896e9f608e": {"doc_hash": "e3087a9fa3a914a17fb176b4ed7fda55185382524b018f7b53c5bf89bef6f3e1", "ref_doc_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32"}, "5b6b8481-2fa7-4ad9-b0c0-f261e7dafea5": {"doc_hash": "1befc6e1dd7c0eab918b76a09d365b16ec94c7c96f9f458dd2fbe47f5a307a01", "ref_doc_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32"}, "cc12134c-79e9-4c83-ade9-f22e23555424": {"doc_hash": "387384d99f2ca4adc9736e6ad927587f8b0a5317fe9f7cb8b6a1bba9c12efcb3", "ref_doc_id": "0777f580-1526-4acb-94eb-331f78f1bde6"}, "29b47d7c-a151-489e-b940-2c1caaae1368": {"doc_hash": "463e2c91f90f5a1f1e4c3cf6418896d600298c803fe8758c6b29188163558ffd", "ref_doc_id": "0777f580-1526-4acb-94eb-331f78f1bde6"}, "e6fdf905-7832-4676-b275-54bc2a882bdf": {"doc_hash": "4c3d231c5f36b13e7a63b0539f94161be4e73761755bc8eca027716897527a3c", "ref_doc_id": "0777f580-1526-4acb-94eb-331f78f1bde6"}, "3ac3dae5-c9da-49d1-a729-1d0b3f19b541": {"doc_hash": "72980040ba35f5802c8d2b5c5d12ebcaa5c1f7de22e6f7b40debf7ceddb7b768", "ref_doc_id": "0777f580-1526-4acb-94eb-331f78f1bde6"}, "9bf8b686-7209-4077-a1fd-3a46f2205907": {"doc_hash": "7071aab1f2cc0a0c6fa82003209dfef6728fdbad6b48917486feee9b50958c0a", "ref_doc_id": "0777f580-1526-4acb-94eb-331f78f1bde6"}, "42da3e91-6ad8-4ddf-9860-ebdd3d442590": {"doc_hash": "87f9215ae94ca75c2b1431a5c77c663786141b62573020cec8e1544081b82894", "ref_doc_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af"}, "9e769163-1759-442c-a118-beb1c7a10663": {"doc_hash": "e8de213e45d53d7017e74e0cf06359af6c960fb18215bb313384f987cfdf361e", "ref_doc_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af"}, "72b124c1-6962-42fc-b351-43e43cba60c0": {"doc_hash": "ff28b23f7132ca8c7e32eacc34b1450a6d5fb462471ed69579fb21d9367e37e5", "ref_doc_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af"}, "dbf3f779-5b03-43fe-b409-44041797e372": {"doc_hash": "e22f244388a98358a532d126988f0db2495a58b40140f8ad5b36ff86dc193e1a", "ref_doc_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af"}, "236ab30c-c795-4e00-824a-eba599684cd9": {"doc_hash": "03e8e76bc323e20e31dbcbfeac1210b56b885cc86b748b5c18911dc8461fa173", "ref_doc_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af"}, "f67a25b4-6a7f-4eb7-bfac-b0fcfde581b4": {"doc_hash": "b2a1cbcda96a29c9c2f45d4b347f8a52a6ffb09f946bcb75355f0b4a8625d437", "ref_doc_id": "18d71101-b08d-4f4b-b097-ec770f400f1b"}, "5093d403-fa1b-4c55-a734-309cd7e9c39f": {"doc_hash": "df379729b860d0e1beea6cf2816d015b55956caaa61ec2d6f7a6b4f7bf41ea62", "ref_doc_id": "18d71101-b08d-4f4b-b097-ec770f400f1b"}, "b74a45a1-a6d7-4c2e-a63d-ff362cca0d0e": {"doc_hash": "40644c521817b233b9f4c8e8d2d8f6ad8082026982f4cc647de18cc9fa738e2b", "ref_doc_id": "18d71101-b08d-4f4b-b097-ec770f400f1b"}, "1e5a3fc0-dd9d-45fd-9b78-d3ab674b6fd1": {"doc_hash": "ddf7906a430712c4faa74aff4ed3b7c9fa6f1a344184ccb7be6982d11fc45c59", "ref_doc_id": "4a6ba5cc-7170-41ee-bea9-ea051ed6394a"}, "f19d0c72-290e-4331-b25c-b281a83c7358": {"doc_hash": "c58072be15551e3c119c9538c517ded98b34db3eb9c42bb6e269c2f194d7e24f", "ref_doc_id": "d277908f-9e75-4db2-ac71-1610cd8017f5"}, "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6": {"doc_hash": "0bcd596210269c7b747e3ad09b587947af1a484dcf14b24efcaa1e74b2a9477a", "ref_doc_id": "d277908f-9e75-4db2-ac71-1610cd8017f5"}, "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a": {"doc_hash": "767c3825073b258e94a4e974a9b8c876c373043035ebc489a3aea26305d0d522", "ref_doc_id": "d277908f-9e75-4db2-ac71-1610cd8017f5"}, "a6361a14-10d0-48ce-9edf-28ba37f8d8df": {"doc_hash": "ca0b90ac7bc53c590de738b5dcbbe47bfb2ce157b57a9ef6a828165813e12ad7", "ref_doc_id": "d277908f-9e75-4db2-ac71-1610cd8017f5"}, "28266af3-571a-4be1-a0c8-b109a69299f8": {"doc_hash": "0ba559e87f8ff1fc2dffdf3b4970196c4d51fa9f5afbe4aa48d30f18ca3a9428", "ref_doc_id": "d277908f-9e75-4db2-ac71-1610cd8017f5"}, "c0f888e1-5c95-41bf-baca-deac6f6d5557": {"doc_hash": "95924f4a82b0c1d6d07d3819f91161b2a05ecbc5cd0a93a203aa54cd98b08822", "ref_doc_id": "154b557d-9b90-41c7-9626-4b0adc9e788b"}, "d1188255-3d7d-4aba-a56d-47c677f7b037": {"doc_hash": "f56e75bfd665276405cc0f2450f37c59a46ace8cb5a642a73c2fac190c00ea73", "ref_doc_id": "47553964-3793-41d2-a284-6165612d1a74"}, "7de2885f-c2f1-4538-9dd1-bf9ef5bb9f64": {"doc_hash": "7131bcc71afef32d425994208a8ae505580ad5022a2903ae82f8bcc66d1fbc89", "ref_doc_id": "695bb35a-8a71-4d1d-82da-2bc699f404ac"}, "ade827da-f08c-4898-b1ba-8de933f3be08": {"doc_hash": "f9c4231e4347cc704080247150fa495062a27407805c923febaf72f509f4e65b", "ref_doc_id": "695bb35a-8a71-4d1d-82da-2bc699f404ac"}, "fbfa6d89-e710-4a1a-bf56-43958cf0191e": {"doc_hash": "0f817ee69d87bf7800d3d885b4dad6f356c0f55e55734abc84d19a306e66fa4b", "ref_doc_id": "17fcb1f3-98fa-4246-9ddb-972116280b92"}, "4d012bfa-95e0-4682-b9af-b5adf95fc1a6": {"doc_hash": "eda62da4c5ba8dbd30051d205ba6adae9a10e2222c6b473d2490940f646c1a2b", "ref_doc_id": "557e1cdd-dc48-4cff-912a-420b02eb869e"}, "fddfdf19-0c97-4792-b0b7-cf6a784a4c88": {"doc_hash": "164aa88f60a205a4cd33d5de18a02956bdd2427f7dffc170cf32bfc6bea3a854", "ref_doc_id": "557e1cdd-dc48-4cff-912a-420b02eb869e"}, "521d154e-458a-4840-a373-cd6ac9839c2c": {"doc_hash": "f20e8b158f51fe94cd00e04ff0f2489a39bb2655eef16be22626c74139b584dd", "ref_doc_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271"}, "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e": {"doc_hash": "1375f305986bf24d63e2096cb262455420ae9beb3dff768475e6769a32379abf", "ref_doc_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271"}, "aed0be1a-b71d-4406-9e50-5986d5e5b682": {"doc_hash": "81507100f4bf9d8f69e91ce8ac19b64e3b67dbf65152baa2a2d830f41091e71a", "ref_doc_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271"}, "d173dbb9-852b-4462-9366-adb9df3ae483": {"doc_hash": "a86e34c6b72720ac30d3e02a964039a5b81c7fb2d772ac87e8cad8d46c096cc3", "ref_doc_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0"}, "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee": {"doc_hash": "6899b90eacf362b7f30288d8477443015d31a183c219058f3532e89d2111f38f", "ref_doc_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0"}, "6109f391-6fe8-4425-b959-e2698431710d": {"doc_hash": "5c3d169a90beb5b72a7bc9b523e83adb79947abfa45c83aa865de761982ec2fa", "ref_doc_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0"}, "d4701020-2d1c-42bb-a342-abd2aae3b734": {"doc_hash": "d13a958ca93343356aad5d9cefe1952bb76dbdb8270f7e9037c5a10a2b5676ba", "ref_doc_id": "8342d416-b313-49ab-9337-880afb976b83"}, "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12": {"doc_hash": "433b761b77b36d9c217f574b20bf07500fcdce0eccab6fc350557654bed1c794", "ref_doc_id": "8342d416-b313-49ab-9337-880afb976b83"}, "4810f6a0-da22-439f-a356-b8a8c9499178": {"doc_hash": "8d096aa7146e971ea99782775a23acc18ceb814123aaa63d09079db591c9a01b", "ref_doc_id": "8342d416-b313-49ab-9337-880afb976b83"}, "8706dd23-37b9-44c7-b557-1a014ac1a5dd": {"doc_hash": "2637e0b1f405364106cd9d5e7f546a4e273cf680c637d99abc65ae9e682fee05", "ref_doc_id": "8342d416-b313-49ab-9337-880afb976b83"}, "e6c5946b-a3a0-4fc7-8eeb-b4dc83b2c934": {"doc_hash": "a8d7763c5aa9150f736278a71c50faea1066c4380d03f33dbc310929a04c4956", "ref_doc_id": "af595f59-96f0-4ad5-b2fe-595af188c282"}, "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa": {"doc_hash": "bd932f2f75c64b1ec24fbf1995d1c48789ebfe80c7e7912bbb67f747d4f67cbd", "ref_doc_id": "af595f59-96f0-4ad5-b2fe-595af188c282"}, "e778b564-a77d-40a9-a6be-06d9b6f686e1": {"doc_hash": "08d09d8ddbfbd24a6833465eeb6f48d4d49bbdaf37531e3c176d4ed13d0bcbf7", "ref_doc_id": "af595f59-96f0-4ad5-b2fe-595af188c282"}, "cf6572da-1693-4bcd-83e9-94c4d74420ff": {"doc_hash": "bdce76e236cc9569ba8b71197de2b5fefbe62846e586aebf59494d12f2579dc2", "ref_doc_id": "1f63904d-b61d-4390-a64a-3d17ec05955c"}, "dfd847e6-f636-45e3-ab68-c5cc60d6c43d": {"doc_hash": "9195832cdd1da5dd4dfc0f13a3426116f4232f3ac35eb1f734dc0527c6a242fe", "ref_doc_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3"}, "93b64e36-3259-4e10-97e3-1eba300af2d6": {"doc_hash": "52bb5bbebcb162ca46388e15d0a7e371225893adfedce33b72a65c02de0ea1ac", "ref_doc_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3"}, "5614ab58-bc2e-47c2-98cb-160db9a0d561": {"doc_hash": "061a86754b950df99346875748eacb052a2dbb6cba68b6439a4a5d21fca63a80", "ref_doc_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3"}, "388df85a-7c6f-40dc-b614-495177d33923": {"doc_hash": "4ed33576e6ba9ca6b571aafb2031e492540190ca9812dc69b66996e062628f30", "ref_doc_id": "e6c8e69e-c2d2-4984-9d58-18ed3f69a0ec"}, "ad897de4-9733-491b-898a-f5941f6095e7": {"doc_hash": "1fd4d19ef8b7bb42b7e4a09335d4de2b0190f36f79fdc7ab6d87af9246ea3f5a", "ref_doc_id": "7b636796-19bc-48ab-982d-4afb8f11f79f"}, "8b3f046e-ad9c-432a-82cf-b1a8b736215b": {"doc_hash": "f18694540a41319234b7973f92e038abf26ca9553c8fe4812395f238ed1f5074", "ref_doc_id": "accdef0a-adb2-46ee-9e97-c1478718c699"}, "96bd7eda-0367-4214-b279-61ae73bf4ae6": {"doc_hash": "3e7f4c6035d3f85136206ee14f670f1e6f398ec1ef9c91b4f18a3c46b213a927", "ref_doc_id": "ac3c5d25-9808-4aa2-9bbb-d11790a2927d"}, "b0d3f4b1-d799-43e2-9c42-4cadbc01c333": {"doc_hash": "57714a83929b37550db554f74ebb6affe95d7eb38cafc4b015f4454e2b12c022", "ref_doc_id": "ac3c5d25-9808-4aa2-9bbb-d11790a2927d"}, "207a472e-dcbd-421c-8941-1e922dc01d2d": {"doc_hash": "dfd995542f1ba8c7ead3691a17f6ae8a8c226d52d40a3983658e3c43b1240492", "ref_doc_id": "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22"}, "8d474c66-4e1d-4bbf-8f94-9a7c5c3bb5ea": {"doc_hash": "a0893d268a6280bcfc9c2f14926fc836a3eecb9dead85398f3be909c0a142fa7", "ref_doc_id": "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22"}, "ab650d6f-7533-4afe-9813-3f41d37a3542": {"doc_hash": "6b400ee6e5e30e46c43fbb5a1a274e3d05c29656f5a19758ab0546d2b8201b42", "ref_doc_id": "3e44ad02-ee97-4c02-9ae1-8930d56a3c02"}, "35aff9dd-35c9-4525-8c3c-03105d4edcc7": {"doc_hash": "bf9e9695794daaf13485e848a981d72ee53829175fa272dfab31c3d1a2b937bf", "ref_doc_id": "3e44ad02-ee97-4c02-9ae1-8930d56a3c02"}, "d7d0ba13-a46e-40eb-87b3-ca533b8ad8e7": {"doc_hash": "908b1d522b742dfb004a7f7431b9eb3d0ac54444cf9ce9851c12e7a7450085a0", "ref_doc_id": "e0560772-764c-463e-b831-a8178937b7d4"}, "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31": {"doc_hash": "0a8464ba739c99940895e7bd4c3c02e101f479bacd8e0770ac44c0b21c76e5a2", "ref_doc_id": "e0560772-764c-463e-b831-a8178937b7d4"}, "28c8e780-953d-40d7-a465-a71710f1e7db": {"doc_hash": "2a6efd97960b5c8486aea58a59ef273837d2cc9efc9a1be6c382ec1340e923d4", "ref_doc_id": "e0560772-764c-463e-b831-a8178937b7d4"}, "4fb46b48-1cd3-43de-8af5-f9b96e548faa": {"doc_hash": "7133444ca395e7098ee1a30d70d5ef148ee3b8a97a26675abeaa5a75115656fb", "ref_doc_id": "77f8abb2-f02a-439c-a585-319ef7020a2b"}, "f822e542-7938-4bf1-9a1c-42fe9a5232b7": {"doc_hash": "dfb1d899a7810e4318e0347fc430649976f40509b92f7916f3a4021bbc8772fc", "ref_doc_id": "77f8abb2-f02a-439c-a585-319ef7020a2b"}, "6d757296-6ed4-451b-bd32-a5c52d2318cd": {"doc_hash": "3bc628bc595056c6ad3053e19248c41747c26263b693821c4e6fdc159d480db8", "ref_doc_id": "77f8abb2-f02a-439c-a585-319ef7020a2b"}, "f7ac9ba5-d01e-4d89-9589-6be215a991dc": {"doc_hash": "897e8fac7a55ac98995c0dbcdb7c503a94800198f102940227b76f69bee91a77", "ref_doc_id": "10ad1a3b-5890-4bbe-86a4-fa5804de378a"}, "dfd7e271-868e-4a7d-93e4-f20ef7da4686": {"doc_hash": "3aa9247a6ac2d777a39b39c4b7ee65171c45011ca2a5bfce3fa73e3d1a173f7b", "ref_doc_id": "c3ba77db-2be1-4d05-af44-1c8934130a30"}, "abf985da-680c-4ab9-a966-4883d746e7d2": {"doc_hash": "d5372bf9073e2e4f95b5752714f6daa7dbb5d7c8ca7fb7da53e4b3be14dfed47", "ref_doc_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c"}, "de069ee8-15e0-4a77-b460-025085a77607": {"doc_hash": "29f5eb8233fdf5d3cdbae4a3b5bc01d0dc298255438ce8e9333d4816398bb5e7", "ref_doc_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c"}, "e8cad10f-ff3c-4bc2-b99c-cdb5de573e44": {"doc_hash": "b5f280dde42eda75846d4d71f1d99b4d8f46bf020cc76c7e45a003f303df708f", "ref_doc_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c"}, "34e80170-48f2-49e7-bb72-69e0534ad3b9": {"doc_hash": "1654039863612671777b41bd8d593c735bad6f17cfaae0d4f6900485ff8ad5ed", "ref_doc_id": "fd045767-fee8-42ca-8c2d-f37d35d96f27"}, "d9ddbd84-288b-48d8-b701-c981fdd012ab": {"doc_hash": "c0b0468290918ed11e1d2a529b2d763187c2cc46d577d09992f8ed2e5450d4d9", "ref_doc_id": "b50c87bf-bbeb-421f-9d26-e7977876a719"}, "0b75435b-e088-436c-89d9-b1086b434674": {"doc_hash": "005faa8c025c31e312025bddf6288073b083ee4d3df3853f8c9e3b84891b3474", "ref_doc_id": "27f46b58-61aa-4430-9c6f-873c68eb215d"}, "9e3f65eb-97ce-4094-b82b-2f6e3821a049": {"doc_hash": "7d83c57056a532898c04110f362a085281f590f0b213579ff7e6247aa73782e3", "ref_doc_id": "7b1ed82c-a0d9-4eb1-8eac-7fd7d6affd80"}, "480a2e20-07f5-40c3-a2ae-7545892c6f50": {"doc_hash": "d6c6aa2ad062077c2108b9a3c011f3768b18cb29b379623fb2c266a30a780086", "ref_doc_id": "7e0fb88f-703b-4650-ab94-c22f631a0b06"}, "6ef5e339-cb13-41c2-a2f7-578d1ac83ba1": {"doc_hash": "aed4f25b163dc2a33ef69127f59f276222fb21d504ab304a7e165ff7fd5d337a", "ref_doc_id": "7e0fb88f-703b-4650-ab94-c22f631a0b06"}, "e928b1d1-a4b7-4da4-819a-32bdbf76b8ef": {"doc_hash": "afbed650543152e325d76efbfd947225fc3a9ef5f59e9bafc5b38c25137cb489", "ref_doc_id": "948df15a-be8c-4f88-b3e4-9355ee98ac2c"}, "fb09da69-8d87-48d3-b930-d74f1818aa3e": {"doc_hash": "dc4ed1b6345aab0b8580745d682296cdbc3c350df36cdc909eacf410eb48508a", "ref_doc_id": "948df15a-be8c-4f88-b3e4-9355ee98ac2c"}, "00d07363-1f81-4911-9c6b-fdbff23b8f1e": {"doc_hash": "231ed9025abc9b061dfb2b067a07a67082d083c79c67d449119cee8eea959faa", "ref_doc_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5"}, "467e535b-0cc5-41c4-8591-6aec81837bc5": {"doc_hash": "8511dcc70b127b114dde940a2833faf9f6b23e997cc2c23777f89b0f97c690cb", "ref_doc_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5"}, "6db883b5-f8ea-439a-bae2-e0beba4f1737": {"doc_hash": "3277b6559388c629fd0db429cc703fba9503b4bf4b4310bba492664b28822d11", "ref_doc_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5"}, "ced29a7b-3cd1-434c-9bf7-b8c084f66827": {"doc_hash": "37a3e9a0c2dc9fdbd8a7191d221c96b6fbd25321ad3c72518bf9aa60d91bc651", "ref_doc_id": "b1206b8e-d52b-41ed-b71f-9004c9955b76"}, "a469519b-76ac-4572-8a11-37d9989c63ab": {"doc_hash": "f1052ac6951ee2b28ae97192ce6aecb87862a1185e071c557b3f3496bfc3c737", "ref_doc_id": "b1206b8e-d52b-41ed-b71f-9004c9955b76"}, "5ca6df4c-9ebe-456e-ab22-8b6f7564a99b": {"doc_hash": "a32475116730ec15483bf394e55ebf64b46ee2c483a864d01fe889613b2339cb", "ref_doc_id": "16d4c693-e855-46f8-8439-c17d78b39714"}, "4896015e-29c6-4834-bf04-6663361e199a": {"doc_hash": "5596b1b8212ff2c68fdd007f1c44b4ea8c89a6d8f018e79d5d43f4122636911d", "ref_doc_id": "16d4c693-e855-46f8-8439-c17d78b39714"}, "01bd8dc9-3e21-41fb-84f6-fb6d5b8e56b7": {"doc_hash": "a3382ac823ebbf77f63ced384e71f4b713ea7aaa4e57bf59cee576d5cfd10505", "ref_doc_id": "16d4c693-e855-46f8-8439-c17d78b39714"}, "1897c040-236d-447e-ae6f-bb8bc9e91de3": {"doc_hash": "68697dd2dce665ab871c7fd6061b3c95792315ae890d61e9d2f3c2fbfa9d7d7e", "ref_doc_id": "f57e34bb-b97f-44dc-8f63-38cb2a3086ce"}, "d5ed5758-34db-4f3d-9f4a-6c30293b846a": {"doc_hash": "dfa06f39f40cf3189219f0f209f7d4ad54ccadfec290d5530b04d9dc68ae8255", "ref_doc_id": "f57e34bb-b97f-44dc-8f63-38cb2a3086ce"}, "5b2570f0-a410-48f7-9374-c353541636a1": {"doc_hash": "4e55310721ae9c31710d509df41ff6a7910c72bbee62f9d92c25657e787ae3d7", "ref_doc_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0"}, "1fd7682c-ca4f-4dac-9811-2acaf94365ea": {"doc_hash": "eafcef60dab93995aeceaa736c7d574e6c0eaadd3e2376a1c4998f5e407e048c", "ref_doc_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0"}, "79be6e7a-a858-444b-a485-947c9f10fa6d": {"doc_hash": "6c53ed490d521c410ce07b89cf787ce62a02596cfa9cf7ca5e13158bc378706f", "ref_doc_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0"}, "d7e7d15d-540a-4439-a376-2938a16e888b": {"doc_hash": "e8bded132a3f77e1f33e37eed7f7ad62b78c571da26c6838ac6227a8ae7d358a", "ref_doc_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0"}, "d6c523c5-4a40-4f62-8aa9-11ffc3c634c6": {"doc_hash": "8a0a24189c64486da4b578c018081d02dd9d7055aa24049f7b24a260938cb8a8", "ref_doc_id": "23af0791-bc75-461b-b544-b96bfcefad33"}, "6f5a9f63-51d1-4780-be50-7a515ebe9b86": {"doc_hash": "fa73c8ac6725f03e80a03d02bfa1d3458399cebae0595981d75a92ff89969d68", "ref_doc_id": "23af0791-bc75-461b-b544-b96bfcefad33"}, "e8e3b501-714b-4c95-b5e4-ee363f0bfb9e": {"doc_hash": "6f42e6e51090fef53b7b1540bc30399d305ee827cd74dc0667e80e1a169a7160", "ref_doc_id": "7d65309a-30f3-4042-9418-7357a09a55fb"}, "a00a814d-74d9-40b5-8149-c35d9f3d6d92": {"doc_hash": "d0d00f7548630321102d24f14a01e3d3b9263fd96f01e061b3f01cf71abed924", "ref_doc_id": "7d65309a-30f3-4042-9418-7357a09a55fb"}, "45fbc532-0c50-45df-92e0-517a89809400": {"doc_hash": "ff142d14153bc9f3971cf4b8529d1c21b61eccda43140748ee722739498f0c2d", "ref_doc_id": "ce509453-c3c8-4755-a4af-2b30c81d511a"}, "e18c9b52-1936-42d9-9d21-44261de37f05": {"doc_hash": "f01e07d6638be6467372aea573aee4e60725e91b7345543ee237f8310a5ad08c", "ref_doc_id": "ce509453-c3c8-4755-a4af-2b30c81d511a"}, "c08946c8-2884-4f54-aa95-bdde7eaa708f": {"doc_hash": "f4a3d229d9599ddac53f9b72e847a102d45fc18d261f85c5bfa1356ad8d20ffc", "ref_doc_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33"}, "3d4c6473-e909-4d95-a17f-76ad7eaebc32": {"doc_hash": "4b0df29de66d6e50c035323fcdda7e46b0397a4622354b6d4927132a2f0e2ffc", "ref_doc_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33"}, "0ec41f89-cb7d-4c05-99c2-a987a86a61c3": {"doc_hash": "f56667a8c2d549a7cd13581a1aae6ed9c8c39f851ff085dc0af971fdaa02f847", "ref_doc_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33"}, "dcd40d9e-1ffb-413a-a322-3942959ae138": {"doc_hash": "7ebafe0f1281e1de595fe37abf1bcacad7253a9871a10aad04256ff660cc481b", "ref_doc_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4"}, "3974065c-d586-45f5-9973-c393091b3fa8": {"doc_hash": "4b0df29de66d6e50c035323fcdda7e46b0397a4622354b6d4927132a2f0e2ffc", "ref_doc_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4"}, "4f54352d-70a9-4611-b913-392e982160c0": {"doc_hash": "3ae01bb914e46bc9c5f7d9d0a87bda7e8c0ff9be7c5f144c79915dc1211e5431", "ref_doc_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4"}, "0076127e-a61a-4746-9a49-a6a3165dd4c1": {"doc_hash": "d18bb3e1c41c905545440327713fef4505b4d045c6adf316a11ef741a36d6466", "ref_doc_id": "88627a8d-7413-4d6e-98a5-abec56874b3d"}, "0d8d73a2-270a-48d5-8b39-7102d1eb81fc": {"doc_hash": "cd99ae05ad66aeb32a168a4c5aa9e2baeb9dd6006fd0d9fb98cd96500e14b9a4", "ref_doc_id": "88627a8d-7413-4d6e-98a5-abec56874b3d"}, "ff855896-fac4-4b18-bd18-1345ae2f2f15": {"doc_hash": "d2f6f2a6aa057bb0611c25eb36c53abefceff9719661dfa12cc3b08b8b34e750", "ref_doc_id": "88627a8d-7413-4d6e-98a5-abec56874b3d"}, "67026b4b-9a29-4ee6-9f9a-a9225bfc2905": {"doc_hash": "f462ffabc406ef572f64e0117874efef89ffa160fb7d5dea0d257c17b5145c1e", "ref_doc_id": "88627a8d-7413-4d6e-98a5-abec56874b3d"}, "312c279d-ded8-4747-9202-05446d614c2d": {"doc_hash": "73fc7f1dee9997973b565b0e114de3db8a6d9aaa5c74acc87945cc3c32e133ae", "ref_doc_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba"}, "25ed184e-4b92-4430-bec7-bfe9eea868a8": {"doc_hash": "cd99ae05ad66aeb32a168a4c5aa9e2baeb9dd6006fd0d9fb98cd96500e14b9a4", "ref_doc_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba"}, "17a23ce6-88ff-4ef8-8df4-6b083d6a825f": {"doc_hash": "6d0fd258be6d73ead73dd58f4596fd9a14069ba1bf1b873bb7434a3c334ab082", "ref_doc_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba"}, "715490fc-5e71-4eb1-a421-cd770317c340": {"doc_hash": "ae7f83d7a57d8462f476f2c1e836baaf78f27a2b77b78d05644de33779cff631", "ref_doc_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba"}, "3e1761b3-ffce-4e80-9890-147a2c920c2d": {"doc_hash": "212c00abc1cacf34fe20407d095be8c84c199b12075a194d7212f0034f083bd8", "ref_doc_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247"}, "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0": {"doc_hash": "eafcef60dab93995aeceaa736c7d574e6c0eaadd3e2376a1c4998f5e407e048c", "ref_doc_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247"}, "ad03d4b4-459c-42ca-8dac-aa586bede9fd": {"doc_hash": "6c53ed490d521c410ce07b89cf787ce62a02596cfa9cf7ca5e13158bc378706f", "ref_doc_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247"}, "f2d3db21-d2f0-4ea6-87da-7743ac38e862": {"doc_hash": "357f71771f5b283ea2be6d6f70c75b6f692e12dabe8513d8e02c229b6d65b308", "ref_doc_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247"}}, "docstore/data": {"a6d77317-c6aa-44b1-99e4-34bbd4578c52": {"__data__": {"id_": "a6d77317-c6aa-44b1-99e4-34bbd4578c52", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fa3a030-8a05-4b6d-94c5-b64404555559", "node_type": "1", "metadata": {}, "hash": "9661efb6e8a2cf7ae57deabc9b2d552c953db293a41b1438f5124101efdcfed2", "class_name": "RelatedNodeInfo"}}, "text": "domain recon_mdp {\n  \t\n\trequirements = { \n//\t\tconstrained-state,\n\t\treward-deterministic  \n\t};\n\n\ttypes { \n\t\tx_pos : object;\n\t\ty_pos : object; \n\t\tobj : object;\n\t\tagent: object;\n\t\ttool : object;\n\t};\n      \t\n\tpvariables { \n\t\n\t\t//connecting up the locations\n\t\tADJACENT-UP(y_pos,y_pos)     : { non-fluent, bool, default = false };\n\t\tADJACENT-DOWN(y_pos,y_pos)   : { non-fluent, bool, default = false };\n\t\tADJACENT-RIGHT(x_pos, x_pos) : { non-fluent, bool, default = false };\n\t\tADJACENT-LEFT(x_pos,x_pos)   : { non-fluent, bool, default = false };\n\n\t\t//whether or not an object is at a location\n\t\tobjAt(obj, x_pos, y_pos) : { non-fluent, bool, default = false };\n\t\t\n\t\t//whether this locaion is a hazard (might damage the tools)\n\t\tHAZARD(x_pos, y_pos) : { non-fluent, bool, default = false };\n\n\t\t//probability of the tool being damaged, and its detection capabilities, without and with damage\n\t\tDAMAGE_PROB(tool):   { non-fluent, real, default = 0.0 };\n\t\tDETECT_PROB:         { non-fluent, real, default = 0.8 };\n\t\tDETECT_PROB_DAMAGED: { non-fluent, real, default = 0.4 };\n\t\t\n\t\t//types of tools\n\t\tCAMERA_TOOL(tool) : { non-fluent, bool, default = false };\n\t\tLIFE_TOOL(tool)   : { non-fluent, bool, default = false };\n\t\tWATER_TOOL(tool)  : { non-fluent, bool, default = false };\n\n\t\t//Base where you can repair the tools\n\t\tBASE(x_pos, y_pos): { non-fluent, bool, default = false };\n\n\t\t//weights for the reward function, good pics are one where you detected life, bad pics are where you did not\n\t\tGOOD_PIC_WEIGHT : { non-fluent, real,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fa3a030-8a05-4b6d-94c5-b64404555559": {"__data__": {"id_": "3fa3a030-8a05-4b6d-94c5-b64404555559", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6d77317-c6aa-44b1-99e4-34bbd4578c52", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "8209a2b949ab81bfd749746ed7c391ce2ed0050c76bff4f708fc52000843a2df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65efbccb-84d5-4dd0-bebf-d83f486dc840", "node_type": "1", "metadata": {}, "hash": "3310ecf0bd4559db1d8d8802c5f2b192e95c222584a2231caef938e70a5637a7", "class_name": "RelatedNodeInfo"}}, "text": "real, default = 0.0 };\n\t\tDETECT_PROB:         { non-fluent, real, default = 0.8 };\n\t\tDETECT_PROB_DAMAGED: { non-fluent, real, default = 0.4 };\n\t\t\n\t\t//types of tools\n\t\tCAMERA_TOOL(tool) : { non-fluent, bool, default = false };\n\t\tLIFE_TOOL(tool)   : { non-fluent, bool, default = false };\n\t\tWATER_TOOL(tool)  : { non-fluent, bool, default = false };\n\n\t\t//Base where you can repair the tools\n\t\tBASE(x_pos, y_pos): { non-fluent, bool, default = false };\n\n\t\t//weights for the reward function, good pics are one where you detected life, bad pics are where you did not\n\t\tGOOD_PIC_WEIGHT : { non-fluent, real, default = 1.0 };\n\t\tBAD_PIC_WEIGHT  : { non-fluent, real, default = 2.0 };\n\n\t\tdamaged(tool) : { state-fluent, bool, default = false };\n\t\t\n\t\t//after you check for water once, there is the observation you will always get back \n\t\t//you can think of the test as contaminating the sampled object\n\t\twaterChecked(obj)  : { state-fluent, bool, default = false };\n\t\twaterDetected(obj) : { state-fluent, bool, default = false };\n\n\t\t//rechecking for life might be needed as the test is unreliable\n\t\t//again, the test is contaminating, but only after the second try\n\t\tlifeChecked(obj)  : { state-fluent, bool, default = false };\n\t\tlifeChecked2(obj) : { state-fluent, bool, default = false };\n\t\tlifeDetected(obj) : { state-fluent, bool, default = false };\n\n\t\tpictureTaken(obj) : { state-fluent, bool, default = false };\n\t\tagentAt(agent, x_pos, y_pos) : { state-fluent, bool, default = false };\n\n\t\t//actions\n\t\tup(agent) : {action-fluent, bool, default = false};\n\t\tdown(agent) : {action-fluent, bool, default = false};", "mimetype": "text/plain", "start_char_idx": 922, "end_char_idx": 2526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65efbccb-84d5-4dd0-bebf-d83f486dc840": {"__data__": {"id_": "65efbccb-84d5-4dd0-bebf-d83f486dc840", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fa3a030-8a05-4b6d-94c5-b64404555559", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "1acbe08e00bf45d0a4da5a8e84d47415b63b943d4ef167eb2ce2bf9434b0cdc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c47579a0-ad0d-4c6c-8915-9736b526418d", "node_type": "1", "metadata": {}, "hash": "af93783e821c44c4bb640a082e4bde9ed92357836abe2f44ba50c01382ac6a82", "class_name": "RelatedNodeInfo"}}, "text": "waterDetected(obj) : { state-fluent, bool, default = false };\n\n\t\t//rechecking for life might be needed as the test is unreliable\n\t\t//again, the test is contaminating, but only after the second try\n\t\tlifeChecked(obj)  : { state-fluent, bool, default = false };\n\t\tlifeChecked2(obj) : { state-fluent, bool, default = false };\n\t\tlifeDetected(obj) : { state-fluent, bool, default = false };\n\n\t\tpictureTaken(obj) : { state-fluent, bool, default = false };\n\t\tagentAt(agent, x_pos, y_pos) : { state-fluent, bool, default = false };\n\n\t\t//actions\n\t\tup(agent) : {action-fluent, bool, default = false};\n\t\tdown(agent) : {action-fluent, bool, default = false};\n\t\tleft(agent) : {action-fluent, bool, default = false};\n\t\tright(agent) : {action-fluent, bool, default = false};\n\t\tuseToolOn(agent, tool, obj) : {action-fluent, bool, default = false};\n\t\trepair(agent, tool) : {action-fluent, bool, default = false};\n\t};\n  \n\tcpfs {\n\t\t\n\t\t//you can fix damage at the base and tools can be damaged by hazards, even if you are just adjacent to them\n\t\t//NOTE: if the hazard is in your cell, you are subject to the full damage probability,\n\t\t//      if the hazard is only in an adjacent cell, you are subject to half the full damage probability,\n\t\t//      however, the chance is not cumulative (you are subject to one of {full,half,zero} damage probability) \n\t\tdamaged'(?t) = \n\t\t\tif (damaged(?t) ^ ~(exists_{?x : x_pos, ?y : y_pos, ?a: agent} [agentAt(?a, ?x, ?y) ^ BASE(?x, ?y) ^ repair(?a, ?t)])) \n\t\t\t\tthen KronDelta( true )\n\t\t\telse if (exists_{?x : x_pos, ?y : y_pos, ?a: agent} [agentAt(?a, ?x,", "mimetype": "text/plain", "start_char_idx": 1880, "end_char_idx": 3451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c47579a0-ad0d-4c6c-8915-9736b526418d": {"__data__": {"id_": "c47579a0-ad0d-4c6c-8915-9736b526418d", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65efbccb-84d5-4dd0-bebf-d83f486dc840", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9cb52cd1bd8d7380c4dfa8538501d53fa12225fa723a3c9e357abfdaa5c91a0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb343a12-8086-4941-9fde-e183e8cd0eff", "node_type": "1", "metadata": {}, "hash": "e34b5935704c3c95d5afeca995ab40d533be2048ccedc5ed67d893df6ab31c8f", "class_name": "RelatedNodeInfo"}}, "text": "even if you are just adjacent to them\n\t\t//NOTE: if the hazard is in your cell, you are subject to the full damage probability,\n\t\t//      if the hazard is only in an adjacent cell, you are subject to half the full damage probability,\n\t\t//      however, the chance is not cumulative (you are subject to one of {full,half,zero} damage probability) \n\t\tdamaged'(?t) = \n\t\t\tif (damaged(?t) ^ ~(exists_{?x : x_pos, ?y : y_pos, ?a: agent} [agentAt(?a, ?x, ?y) ^ BASE(?x, ?y) ^ repair(?a, ?t)])) \n\t\t\t\tthen KronDelta( true )\n\t\t\telse if (exists_{?x : x_pos, ?y : y_pos, ?a: agent} [agentAt(?a, ?x, ?y) ^ ~BASE(?x, ?y) ^ HAZARD(?x, ?y) ])  \n\t\t\t\tthen Bernoulli( DAMAGE_PROB(?t) )\n\t\t\telse if (exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?x2 :x_pos} [agentAt(?a, ?x, ?y) ^ ~BASE(?x, ?y) ^ HAZARD(?x2, ?y) ^ (ADJACENT-LEFT(?x, ?x2) | ADJACENT-RIGHT(?x, ?x2)) ])  \n\t\t\t\tthen Bernoulli( DAMAGE_PROB(?t) / 2.0 )\n\t\t\telse if (exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?y2 :y_pos} [agentAt(?a, ?x, ?y) ^  ~BASE(?x, ?y) ^ HAZARD(?x, ?y2) ^ (ADJACENT-UP(?y, ?y2) | ADJACENT-DOWN(?y, ?y2)) ])  \n\t\t\t\tthen Bernoulli( DAMAGE_PROB(?t) / 2.0 )\n\t\t\telse \n\t\t\t\tKronDelta( false );  //silly else structure needed because of the way adjacency is encoded\n\n\t\t//keeps track of whether you checked for water (which fixes the value,", "mimetype": "text/plain", "start_char_idx": 2866, "end_char_idx": 4155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb343a12-8086-4941-9fde-e183e8cd0eff": {"__data__": {"id_": "eb343a12-8086-4941-9fde-e183e8cd0eff", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c47579a0-ad0d-4c6c-8915-9736b526418d", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "7a7d61ba11be29f955df114466bbc7e581e8ccac9ea3f21ad647fbcc1e846c23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc244b08-d363-4258-acd7-45d946aa851b", "node_type": "1", "metadata": {}, "hash": "b902b0097abf69e955c1ddbf063f478f76a521a107bf2703680f47361456ed34", "class_name": "RelatedNodeInfo"}}, "text": "?x2) | ADJACENT-RIGHT(?x, ?x2)) ])  \n\t\t\t\tthen Bernoulli( DAMAGE_PROB(?t) / 2.0 )\n\t\t\telse if (exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?y2 :y_pos} [agentAt(?a, ?x, ?y) ^  ~BASE(?x, ?y) ^ HAZARD(?x, ?y2) ^ (ADJACENT-UP(?y, ?y2) | ADJACENT-DOWN(?y, ?y2)) ])  \n\t\t\t\tthen Bernoulli( DAMAGE_PROB(?t) / 2.0 )\n\t\t\telse \n\t\t\t\tKronDelta( false );  //silly else structure needed because of the way adjacency is encoded\n\n\t\t//keeps track of whether you checked for water (which fixes the value, even if you used the damaged tool).\n\t\twaterChecked'(?o) = \n\t\t\tKronDelta( waterChecked(?o) \n\t\t\t\t|  exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ WATER_TOOL(?t)]);\n\n\t\t//first 2 cases are if you have found water and whether it has been checked, the next two are based on the tool being damaged or not\n\t\twaterDetected'(?o) = \n\t\t\tif (waterDetected(?o)) \n\t\t\t\tthen KronDelta( true ) \n\t\t\telse if (waterChecked(?o)) // Only one chance to detect water\n\t\t\t\tthen KronDelta( false ) \n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [WATER_TOOL(?t) ^ damaged(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o)]) \n\t\t\t\tthen Bernoulli(DETECT_PROB_DAMAGED)\n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos,", "mimetype": "text/plain", "start_char_idx": 3675, "end_char_idx": 4960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc244b08-d363-4258-acd7-45d946aa851b": {"__data__": {"id_": "bc244b08-d363-4258-acd7-45d946aa851b", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb343a12-8086-4941-9fde-e183e8cd0eff", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "ff8aec81337e60146c492183dae3111d9efab1aadf571a87beae6cc4fe693eff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "937eb72f-8c84-42ff-979c-74e235cf5d2b", "node_type": "1", "metadata": {}, "hash": "cdedaa56c9d2f7031e0f564a2c88c9a0cb6e956ac94fe61c20a240a4ddaafefc", "class_name": "RelatedNodeInfo"}}, "text": "the next two are based on the tool being damaged or not\n\t\twaterDetected'(?o) = \n\t\t\tif (waterDetected(?o)) \n\t\t\t\tthen KronDelta( true ) \n\t\t\telse if (waterChecked(?o)) // Only one chance to detect water\n\t\t\t\tthen KronDelta( false ) \n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [WATER_TOOL(?t) ^ damaged(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o)]) \n\t\t\t\tthen Bernoulli(DETECT_PROB_DAMAGED)\n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [WATER_TOOL(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o)]) \n\t\t\t\tthen Bernoulli(DETECT_PROB)\n\t\t\telse\n\t\t\t\tKronDelta( false );\n\n\t\t//two lifeChecked variables, only after the second check is the value fixed \n\t\tlifeChecked'(?o) = \n\t\t\tKronDelta( \n\t\t\t\tlifeChecked(?o) \n\t\t\t\t|  exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ LIFE_TOOL(?t)]);\n\t\t\n\t\tlifeChecked2'(?o) = \n\t\t\tKronDelta( \n\t\t\t\tlifeChecked2(?o) \n\t\t\t\t|  lifeChecked(?o) ^ exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ LIFE_TOOL(?t)]);", "mimetype": "text/plain", "start_char_idx": 4472, "end_char_idx": 5627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "937eb72f-8c84-42ff-979c-74e235cf5d2b": {"__data__": {"id_": "937eb72f-8c84-42ff-979c-74e235cf5d2b", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc244b08-d363-4258-acd7-45d946aa851b", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "168bb936ad8ff2989cee8cb889ce2400dfad3575be3aa6905e42c92324fb5c9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3250d162-a53d-4095-b7d4-29284cef8792", "node_type": "1", "metadata": {}, "hash": "659b23c2989e4aa917e0470414c3db4653169d01c0fb9213bedc223c0698813c", "class_name": "RelatedNodeInfo"}}, "text": "?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ LIFE_TOOL(?t)]);\n\t\t\n\t\tlifeChecked2'(?o) = \n\t\t\tKronDelta( \n\t\t\t\tlifeChecked2(?o) \n\t\t\t\t|  lifeChecked(?o) ^ exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ LIFE_TOOL(?t)]);\n\n\t\t//similar to waterDetected\n\t\tlifeDetected'(?o) = \n\t\t\tif (lifeDetected(?o)) \n\t\t\t\tthen KronDelta( true ) \n\t\t\telse if (lifeChecked2(?o) | ~waterDetected(?o)) // Never detect life after 2nd try or if no water\n\t\t\t\tthen KronDelta( false )\n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [ LIFE_TOOL(?t) ^ damaged(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ]) \n\t\t\t\tthen Bernoulli(DETECT_PROB_DAMAGED)\n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [ LIFE_TOOL(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ]) \n\t\t\t\tthen Bernoulli(DETECT_PROB)\n\t\t\telse\n\t\t\t\tKronDelta( false );\t\n\n\t\t//inhibits reward for future pictures of this object\n\t\tpictureTaken'(?o) =\n\t\t\tKronDelta( exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [CAMERA_TOOL(?t) ^ agentAt(?a, ?x,", "mimetype": "text/plain", "start_char_idx": 5286, "end_char_idx": 6466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3250d162-a53d-4095-b7d4-29284cef8792": {"__data__": {"id_": "3250d162-a53d-4095-b7d4-29284cef8792", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "937eb72f-8c84-42ff-979c-74e235cf5d2b", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "08d3abfbc811b1b2f9ac6a325d0c2a82eb5de36be6e0208c8da834675986704a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80f61b93-5c13-4c13-a94e-a89553dec243", "node_type": "1", "metadata": {}, "hash": "dd3b67b423dbbba27f1973c24b92dcce118e8eeacc3ab750bffaefe18d800a89", "class_name": "RelatedNodeInfo"}}, "text": "?x, ?y) ^ useToolOn(?a, ?t, ?o) ]) \n\t\t\t\tthen Bernoulli(DETECT_PROB_DAMAGED)\n\t\t\telse if (exists_{?t : tool, ?x : x_pos, ?y : y_pos, ?a: agent} [ LIFE_TOOL(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ]) \n\t\t\t\tthen Bernoulli(DETECT_PROB)\n\t\t\telse\n\t\t\t\tKronDelta( false );\t\n\n\t\t//inhibits reward for future pictures of this object\n\t\tpictureTaken'(?o) =\n\t\t\tKronDelta( exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [CAMERA_TOOL(?t) ^ agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ ~damaged(?t)] );\n\n\t\tagentAt'(?a, ?x, ?y) = \n\t\t\tKronDelta(\n\t\t\t\t[agentAt(?a, ?x, ?y) ^ ~(up(?a) | down(?a) | right(?a) | left(?a))] |\n\t\t\t\t[left(?a) ^ exists_{?x2 : x_pos} [agentAt(?a, ?x2, ?y) ^ ADJACENT-LEFT(?x2, ?x)]] |\n\t\t\t\t[right(?a) ^ exists_{?x2 : x_pos} [agentAt(?a, ?x2, ?y) ^ ADJACENT-RIGHT(?x2, ?x)]] |\n\t\t\t\t[up(?a) ^ exists_{?y2 : y_pos} [agentAt(?a, ?x, ?y2) ^ ADJACENT-UP(?y2, ?y)]] |\n\t\t\t\t[down(?a) ^ exists_{?y2 : y_pos} [agentAt(?a, ?x, ?y2) ^ ADJACENT-DOWN(?y2, ?y)]] );\n\t};", "mimetype": "text/plain", "start_char_idx": 5994, "end_char_idx": 7003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80f61b93-5c13-4c13-a94e-a89553dec243": {"__data__": {"id_": "80f61b93-5c13-4c13-a94e-a89553dec243", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3250d162-a53d-4095-b7d4-29284cef8792", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "a249086e02051c2a9696d5adcdd32516a4be675166508994317faaafbb41c315", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "701acdd0-badf-4138-9b89-2d665b832cc3", "node_type": "1", "metadata": {}, "hash": "a2722b10fcf6cb27995404a76c9b8b876ed135b4abda404f854d6c590eaadc92", "class_name": "RelatedNodeInfo"}}, "text": "?x2, ?y) ^ ADJACENT-LEFT(?x2, ?x)]] |\n\t\t\t\t[right(?a) ^ exists_{?x2 : x_pos} [agentAt(?a, ?x2, ?y) ^ ADJACENT-RIGHT(?x2, ?x)]] |\n\t\t\t\t[up(?a) ^ exists_{?y2 : y_pos} [agentAt(?a, ?x, ?y2) ^ ADJACENT-UP(?y2, ?y)]] |\n\t\t\t\t[down(?a) ^ exists_{?y2 : y_pos} [agentAt(?a, ?x, ?y2) ^ ADJACENT-DOWN(?y2, ?y)]] );\n\t};\n    \t\n\t//we may want to change the way lifeDetected works because right now the same domain has different possible rewards\n\t// Only get rewarded for a good or bad picture the first time the action is *taken*\n\treward = [sum_{?o : obj}  \n\t\t\t\t (GOOD_PIC_WEIGHT * \n\t\t\t\t  [ ~pictureTaken(?o) ^ lifeDetected(?o) ^ exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ CAMERA_TOOL(?t) ^ ~damaged(?t)]])\n\t\t\t ] +\n\t\t\t [sum_{?o : obj} \n\t\t\t\t -(BAD_PIC_WEIGHT * \n\t\t\t\t  [ ~lifeDetected(?o) ^ exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ CAMERA_TOOL(?t)]])\n\t\t\t ]; \n\n//\t//only 1 of each kind of tool -- actually might be more fun if there were multiple of each one \t\n//\tstate-action-constraints {\n//\t\t(sum_{?t: tool}[WATER_TOOL(?t)])  >= 1;\n//\t\t(sum_{?t: tool}[CAMERA_TOOL(?t)]) >= 1;", "mimetype": "text/plain", "start_char_idx": 6699, "end_char_idx": 7913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "701acdd0-badf-4138-9b89-2d665b832cc3": {"__data__": {"id_": "701acdd0-badf-4138-9b89-2d665b832cc3", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28eff0a-18ab-4237-8d97-144c507fbb58", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "9919786863f658d605d627e9fb0622809603c9c38608390ef0f572b5485a9984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80f61b93-5c13-4c13-a94e-a89553dec243", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "656dff46a49bfdfe691ff317e1f16d92459636de5094cd3f59a4ba7e4e09da26", "class_name": "RelatedNodeInfo"}}, "text": "?t, ?o) ^ CAMERA_TOOL(?t) ^ ~damaged(?t)]])\n\t\t\t ] +\n\t\t\t [sum_{?o : obj} \n\t\t\t\t -(BAD_PIC_WEIGHT * \n\t\t\t\t  [ ~lifeDetected(?o) ^ exists_{?x : x_pos, ?y : y_pos, ?a: agent, ?t: tool} [agentAt(?a, ?x, ?y) ^ objAt(?o, ?x, ?y) ^ useToolOn(?a, ?t, ?o) ^ CAMERA_TOOL(?t)]])\n\t\t\t ]; \n\n//\t//only 1 of each kind of tool -- actually might be more fun if there were multiple of each one \t\n//\tstate-action-constraints {\n//\t\t(sum_{?t: tool}[WATER_TOOL(?t)])  >= 1;\n//\t\t(sum_{?t: tool}[CAMERA_TOOL(?t)]) >= 1;\n//\t\t(sum_{?t: tool}[LIFE_TOOL(?t)])   >= 1;\n//\t};\n}", "mimetype": "text/plain", "start_char_idx": 7422, "end_char_idx": 7965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa9c193c-d6f3-4e56-bbec-91b154dc0a2e": {"__data__": {"id_": "aa9c193c-d6f3-4e56-bbec-91b154dc0a2e", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5ce303c-ad6c-40dd-aa49-9e12eec83bfe", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "793a20d5a47985f486f65f8e799165c30be91f7255fb1db34351942bd4f018a4", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__1 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1};\n\t\ty_pos : {y0,y1};\n\t\tobj : {o0,o1,o2,o3};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x1);\n\t\tADJACENT-UP(y1, y1);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x0,y1);\n\t\tobjAt(o0,x1,y1);\n\t\tobjAt(o1,x1,y0);\n\t\tobjAt(o2,x0,y0);\n\t\tobjAt(o3,x1,y0);\n\t\tHAZARD(x1,y0);\n\t\tDAMAGE_PROB(w1) = 0.29898286;\n\t\tDAMAGE_PROB(l1) = 0.3058349;\n\t\tGOOD_PIC_WEIGHT = 0.18377236;\n\t\tBAD_PIC_WEIGHT = 0.7311116;\n\t};\n}\ninstance recon_inst_mdp__1 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__1;\n\tinit-state { \n\t\tagentAt(a1,x0,y1);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76e7a31e-38b4-41dc-ae82-38664ffabcd2": {"__data__": {"id_": "76e7a31e-38b4-41dc-ae82-38664ffabcd2", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dacc483-da87-4551-8534-45462c631eea", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "2f9b54693bb5c8bc524ba41cc5863c5be373d9c7bbe919c1bb183055d2185f76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5cc26f4-4862-4fbf-97d0-d67637bf14a2", "node_type": "1", "metadata": {}, "hash": "f52c741a5c6b23fbc39ca766cf587015d32000ae19f26c8b083028a6939066b1", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__10 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3,x4};\n\t\ty_pos : {y0,y1,y2,y3,y4};\n\t\tobj : {o0,o1,o2,o3,o4,o5,o6};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x3,y1);\n\t\tobjAt(o0,x1,y2);\n\t\tobjAt(o1,x4,y0);\n\t\tobjAt(o2,x3,y2);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5cc26f4-4862-4fbf-97d0-d67637bf14a2": {"__data__": {"id_": "a5cc26f4-4862-4fbf-97d0-d67637bf14a2", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dacc483-da87-4551-8534-45462c631eea", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "2f9b54693bb5c8bc524ba41cc5863c5be373d9c7bbe919c1bb183055d2185f76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76e7a31e-38b4-41dc-ae82-38664ffabcd2", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "4b9ddbb24f4ad6089ce5de7e20164cadfc6a91e2ef97a0f6616bec2c055056ff", "class_name": "RelatedNodeInfo"}}, "text": "y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x3,y1);\n\t\tobjAt(o0,x1,y2);\n\t\tobjAt(o1,x4,y0);\n\t\tobjAt(o2,x3,y2);\n\t\tobjAt(o3,x1,y3);\n\t\tobjAt(o4,x0,y2);\n\t\tobjAt(o5,x2,y4);\n\t\tobjAt(o6,x4,y3);\n\t\tHAZARD(x0,y0);\n\t\tDAMAGE_PROB(w1) = 0.53341925;\n\t\tDAMAGE_PROB(l1) = 0.42891037;\n\t\tGOOD_PIC_WEIGHT = 0.32691407;\n\t\tBAD_PIC_WEIGHT = 0.6918667;\n\t};\n}\ninstance recon_inst_mdp__10 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__10;\n\tinit-state { \n\t\tagentAt(a1,x3,y1);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 508, "end_char_idx": 1252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad7185ff-1ab6-4925-8217-9fb039c28f47": {"__data__": {"id_": "ad7185ff-1ab6-4925-8217-9fb039c28f47", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a171992a-3591-4a25-83be-636a42048c9b", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "6d3371eec2cdb5afe6a3449961a30595962de03e34f867972e9c07009e7ee0f7", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__2 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1};\n\t\ty_pos : {y0,y1};\n\t\tobj : {o0,o1,o2,o3};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x1);\n\t\tADJACENT-UP(y1, y1);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x0,y0);\n\t\tobjAt(o0,x1,y0);\n\t\tobjAt(o1,x1,y1);\n\t\tobjAt(o2,x0,y1);\n\t\tobjAt(o3,x1,y1);\n\t\tHAZARD(x1,y1);\n\t\tDAMAGE_PROB(w1) = 0.4789483;\n\t\tDAMAGE_PROB(l1) = 0.42719078;\n\t\tGOOD_PIC_WEIGHT = 0.17735426;\n\t\tBAD_PIC_WEIGHT = 0.18638955;\n\t};\n}\ninstance recon_inst_mdp__2 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__2;\n\tinit-state { \n\t\tagentAt(a1,x0,y0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c546e1af-8537-4161-b2da-dbb3bc04d1ec": {"__data__": {"id_": "c546e1af-8537-4161-b2da-dbb3bc04d1ec", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033d406e-95b0-48f7-b993-eb307b58b658", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "7b8d22da4035c5a27ba0392b6064a0971d2ab43b7606c4dc87b4b447b177118c", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__3 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2};\n\t\ty_pos : {y0,y1,y2};\n\t\tobj : {o0,o1,o2,o3,o4};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x2);\n\t\tADJACENT-UP(y2, y2);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y2);\n\t\tobjAt(o0,x0,y0);\n\t\tobjAt(o1,x0,y1);\n\t\tobjAt(o2,x0,y2);\n\t\tobjAt(o3,x1,y1);\n\t\tobjAt(o4,x1,y0);\n\t\tHAZARD(x2,y2);\n\t\tDAMAGE_PROB(w1) = 0.28236946;\n\t\tDAMAGE_PROB(l1) = 0.30418548;\n\t\tGOOD_PIC_WEIGHT = 0.7597893;\n\t\tBAD_PIC_WEIGHT = 0.33286658;\n\t};\n}\ninstance recon_inst_mdp__3 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__3;\n\tinit-state { \n\t\tagentAt(a1,x1,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c5165e0-121f-4207-94e5-81a92756a531": {"__data__": {"id_": "3c5165e0-121f-4207-94e5-81a92756a531", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2bdccaa9-8179-43af-becb-947517d6ab29", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "5de057cd086be970a4d60d336e493ffca6ed9d21e7f2de1508686a3bdf1f178e", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__4 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2};\n\t\ty_pos : {y0,y1,y2};\n\t\tobj : {o0,o1,o2,o3,o4};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x2);\n\t\tADJACENT-UP(y2, y2);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x2,y2);\n\t\tobjAt(o0,x0,y1);\n\t\tobjAt(o1,x0,y0);\n\t\tobjAt(o2,x1,y1);\n\t\tobjAt(o3,x2,y1);\n\t\tobjAt(o4,x0,y2);\n\t\tHAZARD(x1,y1);\n\t\tDAMAGE_PROB(w1) = 0.42995942;\n\t\tDAMAGE_PROB(l1) = 0.30572772;\n\t\tGOOD_PIC_WEIGHT = 0.15562552;\n\t\tBAD_PIC_WEIGHT = 0.69803196;\n\t};\n}\ninstance recon_inst_mdp__4 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__4;\n\tinit-state { \n\t\tagentAt(a1,x2,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0858a46f-6a9d-4907-90a1-6386fc5fcdc5": {"__data__": {"id_": "0858a46f-6a9d-4907-90a1-6386fc5fcdc5", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a936e59-3c43-42b2-9a75-7ce502ef62b4", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "76ae21ab20697a0debc47e178e43fd84a56921d091b2a8f522308710a8daec73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ac9a1af-9b0f-4eef-8734-05f4b649c364", "node_type": "1", "metadata": {}, "hash": "88530df681beb871323fad3bbb6825cd1226d71a849e4e4b3cbb432d1b9b8ba6", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__5 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3};\n\t\ty_pos : {y0,y1,y2,y3};\n\t\tobj : {o0,o1,o2,o3,o4,o5};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y0);\n\t\tobjAt(o0,x0,y0);\n\t\tobjAt(o1,x2,y0);\n\t\tobjAt(o2,x1,y3);\n\t\tobjAt(o3,x2,y3);\n\t\tobjAt(o4,x3,y3);\n\t\tobjAt(o5,x2,y2);\n\t\tHAZARD(x3,y3);\n\t\tDAMAGE_PROB(w1) = 0.2766014;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ac9a1af-9b0f-4eef-8734-05f4b649c364": {"__data__": {"id_": "0ac9a1af-9b0f-4eef-8734-05f4b649c364", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a936e59-3c43-42b2-9a75-7ce502ef62b4", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "76ae21ab20697a0debc47e178e43fd84a56921d091b2a8f522308710a8daec73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0858a46f-6a9d-4907-90a1-6386fc5fcdc5", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "2dd1409b0b39daef13c3c9a88cb369593772fac22469f43e0dba9c78eb9cd4dd", "class_name": "RelatedNodeInfo"}}, "text": "ADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y0);\n\t\tobjAt(o0,x0,y0);\n\t\tobjAt(o1,x2,y0);\n\t\tobjAt(o2,x1,y3);\n\t\tobjAt(o3,x2,y3);\n\t\tobjAt(o4,x3,y3);\n\t\tobjAt(o5,x2,y2);\n\t\tHAZARD(x3,y3);\n\t\tDAMAGE_PROB(w1) = 0.2766014;\n\t\tDAMAGE_PROB(l1) = 0.32337266;\n\t\tGOOD_PIC_WEIGHT = 0.7984295;\n\t\tBAD_PIC_WEIGHT = 0.5658803;\n\t};\n}\ninstance recon_inst_mdp__5 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__5;\n\tinit-state { \n\t\tagentAt(a1,x1,y0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 505, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b71cb1e-55df-4435-a5f6-9e9da521d3bb": {"__data__": {"id_": "0b71cb1e-55df-4435-a5f6-9e9da521d3bb", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82947c5a-974e-476d-b8ff-e4ddeecfd338", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "692a5f8c15570c81b73ab3f5b4b74fefeedae2d17c77120aa4143442e23bad05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f21cfc5-7a60-415b-b610-477fa0624b2f", "node_type": "1", "metadata": {}, "hash": "8f7353955b1114bddf09dd9512fa7f5591c41a71cc80c4b224527ea8d9f05b98", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__6 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3};\n\t\ty_pos : {y0,y1,y2,y3};\n\t\tobj : {o0,o1,o2,o3,o4,o5};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y2);\n\t\tobjAt(o0,x1,y3);\n\t\tobjAt(o1,x2,y3);\n\t\tobjAt(o2,x2,y1);\n\t\tobjAt(o3,x0,y3);\n\t\tobjAt(o4,x0,y1);\n\t\tobjAt(o5,x0,y2);\n\t\tHAZARD(x3,y0);\n\t\tDAMAGE_PROB(w1) = 0.28587285;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f21cfc5-7a60-415b-b610-477fa0624b2f": {"__data__": {"id_": "9f21cfc5-7a60-415b-b610-477fa0624b2f", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82947c5a-974e-476d-b8ff-e4ddeecfd338", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "692a5f8c15570c81b73ab3f5b4b74fefeedae2d17c77120aa4143442e23bad05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b71cb1e-55df-4435-a5f6-9e9da521d3bb", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "0aa55601ca7885e79669fbc534ec28e55c872cc3034ed6c9358a114b7475bc7f", "class_name": "RelatedNodeInfo"}}, "text": "ADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y2);\n\t\tobjAt(o0,x1,y3);\n\t\tobjAt(o1,x2,y3);\n\t\tobjAt(o2,x2,y1);\n\t\tobjAt(o3,x0,y3);\n\t\tobjAt(o4,x0,y1);\n\t\tobjAt(o5,x0,y2);\n\t\tHAZARD(x3,y0);\n\t\tDAMAGE_PROB(w1) = 0.28587285;\n\t\tDAMAGE_PROB(l1) = 0.36778316;\n\t\tGOOD_PIC_WEIGHT = 0.5747618;\n\t\tBAD_PIC_WEIGHT = 0.99322593;\n\t};\n}\ninstance recon_inst_mdp__6 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__6;\n\tinit-state { \n\t\tagentAt(a1,x1,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 505, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8975607-154e-4eaf-a87f-c0d6ee9ee9eb": {"__data__": {"id_": "b8975607-154e-4eaf-a87f-c0d6ee9ee9eb", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e46527e-2f04-4c43-96d0-b596de07a7de", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "1bcd2950be2a85c5e832182099844bb273f67297aca237f07ab973660a2df3af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf9a9c9-2ea4-4ffc-b52e-01544df2a36c", "node_type": "1", "metadata": {}, "hash": "8905fb970846c943fb102749a37d4caef485be38510ce39c586d7c7d00d1cf0d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__7 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3};\n\t\ty_pos : {y0,y1,y2,y3};\n\t\tobj : {o0,o1,o2,o3,o4,o5};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x0,y1);\n\t\tobjAt(o0,x3,y1);\n\t\tobjAt(o1,x2,y1);\n\t\tobjAt(o2,x1,y1);\n\t\tobjAt(o3,x0,y0);\n\t\tobjAt(o4,x2,y3);\n\t\tobjAt(o5,x2,y2);\n\t\tHAZARD(x1,y3);\n\t\tDAMAGE_PROB(w1) = 0.5287535;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf9a9c9-2ea4-4ffc-b52e-01544df2a36c": {"__data__": {"id_": "abf9a9c9-2ea4-4ffc-b52e-01544df2a36c", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e46527e-2f04-4c43-96d0-b596de07a7de", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "1bcd2950be2a85c5e832182099844bb273f67297aca237f07ab973660a2df3af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8975607-154e-4eaf-a87f-c0d6ee9ee9eb", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "ed7c83f4ff0a3540cb190945e989e6a07a368d7f4c6524ff0ba13266b1596ec3", "class_name": "RelatedNodeInfo"}}, "text": "ADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x3);\n\t\tADJACENT-UP(y3, y3);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x0,y1);\n\t\tobjAt(o0,x3,y1);\n\t\tobjAt(o1,x2,y1);\n\t\tobjAt(o2,x1,y1);\n\t\tobjAt(o3,x0,y0);\n\t\tobjAt(o4,x2,y3);\n\t\tobjAt(o5,x2,y2);\n\t\tHAZARD(x1,y3);\n\t\tDAMAGE_PROB(w1) = 0.5287535;\n\t\tDAMAGE_PROB(l1) = 0.37929258;\n\t\tGOOD_PIC_WEIGHT = 0.2864477;\n\t\tBAD_PIC_WEIGHT = 0.9880673;\n\t};\n}\ninstance recon_inst_mdp__7 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__7;\n\tinit-state { \n\t\tagentAt(a1,x0,y1);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 505, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c39921d-d2e4-4f64-8e60-edf35a8dc280": {"__data__": {"id_": "0c39921d-d2e4-4f64-8e60-edf35a8dc280", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72a9d8da-62fd-4ffc-8985-27ea2399fae8", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "2b101d443a48b2ccf86aedc55486bd75cd2775202288a3c4179214618721cd1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cee5212-1b72-4996-82e3-90c0f22f0c3b", "node_type": "1", "metadata": {}, "hash": "b39af0c442df458d521c1a0a84cb551135a6ec603bb3405365055f9d0bdfe328", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__8 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3,x4};\n\t\ty_pos : {y0,y1,y2,y3,y4};\n\t\tobj : {o0,o1,o2,o3,o4,o5,o6};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y3);\n\t\tobjAt(o0,x4,y0);\n\t\tobjAt(o1,x3,y4);\n\t\tobjAt(o2,x2,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cee5212-1b72-4996-82e3-90c0f22f0c3b": {"__data__": {"id_": "0cee5212-1b72-4996-82e3-90c0f22f0c3b", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72a9d8da-62fd-4ffc-8985-27ea2399fae8", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "2b101d443a48b2ccf86aedc55486bd75cd2775202288a3c4179214618721cd1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c39921d-d2e4-4f64-8e60-edf35a8dc280", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "f8b17420f7f746af377bf8b193ff3ae151066aaadf35c9af5bea8e4796325119", "class_name": "RelatedNodeInfo"}}, "text": "y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x1,y3);\n\t\tobjAt(o0,x4,y0);\n\t\tobjAt(o1,x3,y4);\n\t\tobjAt(o2,x2,y4);\n\t\tobjAt(o3,x3,y2);\n\t\tobjAt(o4,x4,y4);\n\t\tobjAt(o5,x0,y0);\n\t\tobjAt(o6,x2,y0);\n\t\tHAZARD(x1,y0);\n\t\tHAZARD(x1,y2);\n\t\tDAMAGE_PROB(w1) = 0.3081831;\n\t\tDAMAGE_PROB(l1) = 0.26840475;\n\t\tGOOD_PIC_WEIGHT = 0.58386946;\n\t\tBAD_PIC_WEIGHT = 0.28460833;\n\t};\n}\ninstance recon_inst_mdp__8 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__8;\n\tinit-state { \n\t\tagentAt(a1,x1,y3);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 507, "end_char_idx": 1266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b82a2947-9dee-465a-a58a-cd5e94d57623": {"__data__": {"id_": "b82a2947-9dee-465a-a58a-cd5e94d57623", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c19e2b4-12da-4321-826f-5b370eadcfda", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "1f97dcb3b2cbecb5cfa701c5cdd047239f43627341b80373020c1dd39a882a68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7a90f2b-6ef2-4ec8-bd47-acac86917bb5", "node_type": "1", "metadata": {}, "hash": "149de9dbebc559c72cb2d3c426cf9378fa17b9e2f830b9041f40e7de97504d9f", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_recon_inst_mdp__9 {\n\tdomain = recon_mdp; \n\tobjects { \n\t\tx_pos : {x0,x1,x2,x3,x4};\n\t\ty_pos : {y0,y1,y2,y3,y4};\n\t\tobj : {o0,o1,o2,o3,o4,o5,o6};\n\t\tagent : {a1};\n\t\ttool : {l1,w1,p1};\n\n\t}; \n\tnon-fluents {\n\t\tADJACENT-LEFT(x0, x0);\n\t\tADJACENT-DOWN(y0, y0);\n\t\tADJACENT-RIGHT(x0, x1);\n\t\tADJACENT-UP(y0, y1);\n\t\tADJACENT-LEFT(x1, x0);\n\t\tADJACENT-DOWN(y1, y0);\n\t\tADJACENT-RIGHT(x1, x2);\n\t\tADJACENT-UP(y1, y2);\n\t\tADJACENT-LEFT(x2, x1);\n\t\tADJACENT-DOWN(y2, y1);\n\t\tADJACENT-RIGHT(x2, x3);\n\t\tADJACENT-UP(y2, y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x2,y4);\n\t\tobjAt(o0,x3,y0);\n\t\tobjAt(o1,x3,y2);\n\t\tobjAt(o2,x1,y0);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7a90f2b-6ef2-4ec8-bd47-acac86917bb5": {"__data__": {"id_": "d7a90f2b-6ef2-4ec8-bd47-acac86917bb5", "embedding": null, "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c19e2b4-12da-4321-826f-5b370eadcfda", "node_type": "4", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "1f97dcb3b2cbecb5cfa701c5cdd047239f43627341b80373020c1dd39a882a68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b82a2947-9dee-465a-a58a-cd5e94d57623", "node_type": "1", "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}, "hash": "ad761c0aaf1540c0dd5cb6ded8a0d7d6bd01871c156ec1572c7b31203895c34b", "class_name": "RelatedNodeInfo"}}, "text": "y3);\n\t\tADJACENT-LEFT(x3, x2);\n\t\tADJACENT-DOWN(y3, y2);\n\t\tADJACENT-RIGHT(x3, x4);\n\t\tADJACENT-UP(y3, y4);\n\t\tADJACENT-LEFT(x4, x3);\n\t\tADJACENT-DOWN(y4, y3);\n\t\tADJACENT-RIGHT(x4, x4);\n\t\tADJACENT-UP(y4, y4);\n\t\tWATER_TOOL(w1);\n\t\tLIFE_TOOL(l1);\n\t\tCAMERA_TOOL(p1);\n\t\tBASE(x2,y4);\n\t\tobjAt(o0,x3,y0);\n\t\tobjAt(o1,x3,y2);\n\t\tobjAt(o2,x1,y0);\n\t\tobjAt(o3,x4,y0);\n\t\tobjAt(o4,x4,y2);\n\t\tobjAt(o5,x2,y0);\n\t\tobjAt(o6,x3,y1);\n\t\tHAZARD(x0,y4);\n\t\tDAMAGE_PROB(w1) = 0.44076487;\n\t\tDAMAGE_PROB(l1) = 0.2609291;\n\t\tGOOD_PIC_WEIGHT = 0.57421285;\n\t\tBAD_PIC_WEIGHT = 0.50724465;\n\t};\n}\ninstance recon_inst_mdp__9 { \n\tdomain = recon_mdp; \n \tnon-fluents = nf_recon_inst_mdp__9;\n\tinit-state { \n\t\tagentAt(a1,x2,y4);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 507, "end_char_idx": 1249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ea7df02-9a12-45e3-b916-e62ac2770a28": {"__data__": {"id_": "9ea7df02-9a12-45e3-b916-e62ac2770a28", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c903ede-5c20-4748-a114-aa6676812ea7", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "1d8b60c97c8db1d5ca07d165ea9f575b6a0686ba6e014f636715d3f74b156163", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11fd5d26-4837-4783-a61a-b0219f25c704", "node_type": "1", "metadata": {}, "hash": "f738c3f19879fc8e32162329b740b0c00452084d324d72b6f4c48dcf35f1ca1f", "class_name": "RelatedNodeInfo"}}, "text": "domain crossing_traffic_mdp {\n\trequirements = {\n//\t\tconstrained-state,\n\t\treward-deterministic\n\t};\n\t\n\ttypes {\n\t\txpos : object;\n\t\typos : object;\n\t};\n\t\n\tpvariables {\n\n\t\tNORTH(ypos, ypos) : {non-fluent, bool, default = false};\n\t\tSOUTH(ypos, ypos) : {non-fluent, bool, default = false};\n\t\tEAST(xpos, xpos)  : {non-fluent, bool, default = false};\n\t\tWEST(xpos, xpos)  : {non-fluent, bool, default = false};\n\n\t\tMIN-XPOS(xpos) : {non-fluent, bool, default = false};\n\t\tMAX-XPOS(xpos) : {non-fluent, bool, default = false};\n\t\tMIN-YPOS(ypos) : {non-fluent, bool, default = false};\n\t\tMAX-YPOS(ypos) : {non-fluent, bool, default = false};\n\t\n\t\tINPUT-RATE : {non-fluent, real, default = 0.2};\n\t\t\n\t\tGOAL(xpos,ypos) : {non-fluent, bool, default = false};\n\t\t\n\t\t// Fluents\n\t\trobot-at(xpos, ypos)    : {state-fluent, bool, default = false};\n\t\tobstacle-at(xpos, ypos) : {state-fluent, bool, default = false};\n\t\t\n\t\t// Actions\n\t\tmove-north : {action-fluent, bool, default = false};\n\t\tmove-south : {action-fluent, bool, default = false};\n\t\tmove-east  : {action-fluent, bool, default = false};\n\t\tmove-west  : {action-fluent, bool, default = false};\n\t};\n\t\n\tcpfs {\n\t\n\t\trobot-at'(?x,?y) =\n\t\t\n\t\t\t// Goal is absorbing so robot stays put \n\t\t\tif ( GOAL(?x,?y) ^ robot-at(?x,?y)  )\n\t\t\tthen \n\t\t\t\tKronDelta(true)\n\t\t\telse if ( exists_{?x2 : xpos, ?y2 : ypos} [ GOAL(?x2,?y2) ^ robot-at(?x2,?y2)  ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // because of fall-through we know (?x,y) != (?x2,?y2)\n\t\t\t\t\n\t\t\t// Check for legal robot movement (robot disappears if at an obstacle)\n\t\t\telse if ( move-north ^ exists_{?y2 : ypos} [ NORTH(?y2,?y) ^ robot-at(?x,?y2) ^ ~obstacle-at(?x,?y2) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-north ^ exists_{?y2 : ypos} [ NORTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // robot leaves this location\n\t\t\telse if ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y2,?y) ^ robot-at(?x,?y2) ^ ~obstacle-at(?x,?y2) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y,?y2) ^ robot-at(?x,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11fd5d26-4837-4783-a61a-b0219f25c704": {"__data__": {"id_": "11fd5d26-4837-4783-a61a-b0219f25c704", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c903ede-5c20-4748-a114-aa6676812ea7", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "1d8b60c97c8db1d5ca07d165ea9f575b6a0686ba6e014f636715d3f74b156163", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ea7df02-9a12-45e3-b916-e62ac2770a28", "node_type": "1", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "6a4e13f8b25e34f22b391a0e133b4fd312d22d992f73e284672a38c07e7c9296", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bcf8e40-381a-4712-816b-37878a30424b", "node_type": "1", "metadata": {}, "hash": "04f311f16674b850474f1ada535eb5e2f9bbcf1192a150f9d877945529ea327a", "class_name": "RelatedNodeInfo"}}, "text": "?y) ^ robot-at(?x,?y2) ^ ~obstacle-at(?x,?y2) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-north ^ exists_{?y2 : ypos} [ NORTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // robot leaves this location\n\t\t\telse if ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y2,?y) ^ robot-at(?x,?y2) ^ ~obstacle-at(?x,?y2) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // robot leaves this location\n\t\t\telse if ( move-east ^ exists_{?x2 : xpos} [ EAST(?x2,?x) ^ robot-at(?x2,?y) ^ ~obstacle-at(?x2,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-east ^ exists_{?x2 : xpos} [ EAST(?x,?x2) ^ robot-at(?x,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // robot leaves this location\n\t\t\telse if ( move-west ^ exists_{?x2 : xpos} [ WEST(?x2,?x) ^ robot-at(?x2,?y) ^ ~obstacle-at(?x2,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(true) // robot moves to this location\n\t\t\telse if ( move-west ^ exists_{?x2 : xpos} [ WEST(?x,?x2) ^ robot-at(?x,?y) ] )\n\t\t\tthen \n\t\t\t\tKronDelta(false) // robot leaves this location\n\t\t\t\t\n\t\t\t// A noop or illegal movement, so state unchanged\n\t\t\telse \n\t\t\t\tKronDelta( robot-at(?x,?y) ^ ~obstacle-at(?x,?y) );\n\t\t\t\n\t\tobstacle-at'(?x, ?y) =\n\t\t\n\t\t\t// No obstacles in top or bottom row (these rows are safe havens)\n\t\t\tif ( MIN-YPOS(?y) | MAX-YPOS(?y) )\n\t\t\t\tthen KronDelta( false )\n\t\t\n\t\t\t// Check for RHS border input cell\n\t\t\telse if ( MAX-XPOS(?x) )\n\t\t\t\tthen Bernoulli(\tINPUT-RATE )\n\t\t\t\n\t\t\t// Not a top or bottom row and not a border input cell -- inherits obstacle to east\n\t\t\telse\n\t\t\t\tKronDelta( exists_{?x2 : xpos} [EAST(?x,?x2) ^ obstacle-at(?x2,?y)] );\n\t\t\t\t\n\t};\n\t\n\t// 0 reward for reaching goal, -1 in all other cases\n\treward = [sum_{?x : xpos, ?y : ypos} -(GOAL(?x,?y) ^ ~robot-at(?x,?y))]; \n\t\n//\tstate-action-constraints {\n//\t\n//\t\t// Robot at exactly one position\n//\t\t[sum_{?x : xpos, ?y : ypos} robot-at(?x,?y)] <= 1;\n//\t\t\n//\t\t// EAST, WEST, NORTH,", "mimetype": "text/plain", "start_char_idx": 1585, "end_char_idx": 3635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bcf8e40-381a-4712-816b-37878a30424b": {"__data__": {"id_": "4bcf8e40-381a-4712-816b-37878a30424b", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c903ede-5c20-4748-a114-aa6676812ea7", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "1d8b60c97c8db1d5ca07d165ea9f575b6a0686ba6e014f636715d3f74b156163", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11fd5d26-4837-4783-a61a-b0219f25c704", "node_type": "1", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "90ea718c85a6d052ab4a331e5e5bb00e12ea86145e8da9936e95d54108119744", "class_name": "RelatedNodeInfo"}}, "text": "?x2) ^ obstacle-at(?x2,?y)] );\n\t\t\t\t\n\t};\n\t\n\t// 0 reward for reaching goal, -1 in all other cases\n\treward = [sum_{?x : xpos, ?y : ypos} -(GOAL(?x,?y) ^ ~robot-at(?x,?y))]; \n\t\n//\tstate-action-constraints {\n//\t\n//\t\t// Robot at exactly one position\n//\t\t[sum_{?x : xpos, ?y : ypos} robot-at(?x,?y)] <= 1;\n//\t\t\n//\t\t// EAST, WEST, NORTH, SOUTH defined properly (unique and symmetric)\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} WEST(?x1,?x2)) <= 1];\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} EAST(?x1,?x2)) <= 1];\n//\t\tforall_{?y1 : ypos} [(sum_{?y2 : ypos} NORTH(?y1,?y2)) <= 1];\n//\t\tforall_{?y1 : ypos} [(sum_{?y2 : ypos} SOUTH(?y1,?y2)) <= 1];\n//\t\tforall_{?x1 : xpos, ?x2 : xpos} [ EAST(?x1,?x2) <=> WEST(?x2,?x1) ];\n//\t\tforall_{?y1 : ypos, ?y2 : ypos} [ SOUTH(?y1,?y2) <=> NORTH(?y2,?y1) ];\n//\n//\t\t// Definition verification\n//\t\t[ sum_{?x : xpos} MIN-XPOS(?x) ] == 1;\n//\t\t[ sum_{?x : xpos} MAX-XPOS(?x) ] == 1;\n//\t\t[ sum_{?y : ypos} MIN-YPOS(?y) ] == 1;\n//\t\t[ sum_{?y : ypos} MAX-YPOS(?y) ] == 1;\n//\t\t[ sum_{?x : xpos, ?y : ypos} GOAL(?x,?y) ] == 1;\n//\t\t\n//\t};\n\t\n}", "mimetype": "text/plain", "start_char_idx": 3306, "end_char_idx": 4365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c711ea87-076a-4a57-af9a-6437ba9fb728": {"__data__": {"id_": "c711ea87-076a-4a57-af9a-6437ba9fb728", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70b409e0-4886-46c1-af30-7609043d7cbe", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "65217e0bc992bcf36c6a6f60e9ddf004d5a6909fe2d8eddd2cc4332df40cbe83", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__1 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3};\n\t\typos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x3);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y3);\n\n\t\tGOAL(x3,y3);\n\n\t\tINPUT-RATE = 0.3;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__1 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__1;\n\tinit-state {\n\t\trobot-at(x3,y1);\n\t\tobstacle-at(x1,y2);\n\t\tobstacle-at(x3,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c8adbba-9ecc-4a58-8a8b-5c136496892a": {"__data__": {"id_": "4c8adbba-9ecc-4a58-8a8b-5c136496892a", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aec571bd-a084-4a8b-8c3d-80fdd38296db", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "bcfc5518d56c17538770a8062e8083735bf427698a1093caa922df792f0e44c7", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__10 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5,x6,x7};\n\t\typos : {y1,y2,y3,y4,y5,y6,y7};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\t\tNORTH(y5,y6);\n\t\tSOUTH(y6,y5);\n\t\tNORTH(y6,y7);\n\t\tSOUTH(y7,y6);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\t\tEAST(x5,x6);\n\t\tWEST(x6,x5);\n\t\tEAST(x6,x7);\n\t\tWEST(x7,x6);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x7);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y7);\n\n\t\tGOAL(x7,y7);\n\n\t\tINPUT-RATE = 0.3;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__10 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__10;\n\tinit-state {\n\t\trobot-at(x7,y1);\n\t\tobstacle-at(x1,y2);\n\t\tobstacle-at(x1,y4);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x2,y4);\n\t\tobstacle-at(x2,y5);\n\t\tobstacle-at(x2,y6);\n\t\tobstacle-at(x3,y2);\n\t\tobstacle-at(x3,y3);\n\t\tobstacle-at(x3,y5);\n\t\tobstacle-at(x5,y3);\n\t\tobstacle-at(x5,y6);\n\t\tobstacle-at(x6,y4);\n\t\tobstacle-at(x6,y5);\n\t\tobstacle-at(x7,y2);\n\t\tobstacle-at(x7,y5);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d29d547-7660-47bf-87b4-2a0da43c65a9": {"__data__": {"id_": "6d29d547-7660-47bf-87b4-2a0da43c65a9", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d442fb3-aa81-4b8a-91cf-8539cdc3b542", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "c52027d82e529d668b280f8995356f6964712fdc301d12b753a3b26285f547da", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__2 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3};\n\t\typos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x3);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y3);\n\n\t\tGOAL(x3,y3);\n\n\t\tINPUT-RATE = 0.6;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__2 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__2;\n\tinit-state {\n\t\trobot-at(x3,y1);\n\t\tobstacle-at(x2,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8b979a5-d94a-4032-a1eb-48472e43da59": {"__data__": {"id_": "c8b979a5-d94a-4032-a1eb-48472e43da59", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6f4cf13-8330-4b9f-8376-2c9faaec52d5", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "0c374be705a6da7ad8c4b79f718ee9a863e964c8c43c250a8cd934a64fb25d64", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__3 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4};\n\t\typos : {y1,y2,y3,y4};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x4);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y4);\n\n\t\tGOAL(x4,y4);\n\n\t\tINPUT-RATE = 0.3;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__3 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__3;\n\tinit-state {\n\t\trobot-at(x4,y1);\n\t\tobstacle-at(x1,y3);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x3,y3);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da801ed9-b23a-4bd4-8b8c-3e4ef7ac8ede": {"__data__": {"id_": "da801ed9-b23a-4bd4-8b8c-3e4ef7ac8ede", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee6b0828-3aa5-46ad-a086-b2aff9f5c5b7", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "47d8ed5e8aeea5a06ad41bc606c3567b9a3f85abe77337369f0222482dc99586", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__4 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4};\n\t\typos : {y1,y2,y3,y4};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x4);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y4);\n\n\t\tGOAL(x4,y4);\n\n\t\tINPUT-RATE = 0.6;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__4 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__4;\n\tinit-state {\n\t\trobot-at(x4,y1);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x3,y2);\n\t\tobstacle-at(x3,y3);\n\t\tobstacle-at(x4,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c38ead00-8652-4782-80e1-3d388ef027bf": {"__data__": {"id_": "c38ead00-8652-4782-80e1-3d388ef027bf", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d92f157-095c-41c6-ad0f-5579f681822b", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "b42cef9ac6cba7079d0987113b50e342c0062b371e6620a3bd023a041f9fb00d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__5 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5};\n\t\typos : {y1,y2,y3,y4,y5};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x5);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y5);\n\n\t\tGOAL(x5,y5);\n\n\t\tINPUT-RATE = 0.2;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__5 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__5;\n\tinit-state {\n\t\trobot-at(x5,y1);\n\t\tobstacle-at(x1,y2);\n\t\tobstacle-at(x1,y4);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x3,y3);\n\t\tobstacle-at(x4,y3);\n\t\tobstacle-at(x5,y2);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7be869-f1a5-4c92-8d37-55b1a39d68a2": {"__data__": {"id_": "5a7be869-f1a5-4c92-8d37-55b1a39d68a2", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "760e0faa-dba3-467b-a73e-bbb6e5bf9ad7", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "d9da94c36967595c1c690831356d60b3fe072ae9ec9a1642bb5ff983f7414ea0", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__6 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5};\n\t\typos : {y1,y2,y3,y4,y5};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x5);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y5);\n\n\t\tGOAL(x5,y5);\n\n\t\tINPUT-RATE = 0.4;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__6 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__6;\n\tinit-state {\n\t\trobot-at(x5,y1);\n\t\tobstacle-at(x1,y2);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x3,y2);\n\t\tobstacle-at(x3,y4);\n\t\tobstacle-at(x4,y2);\n\t\tobstacle-at(x4,y3);\n\t\tobstacle-at(x5,y4);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71f65d53-3dc9-491f-b0ab-01fe4b8990d5": {"__data__": {"id_": "71f65d53-3dc9-491f-b0ab-01fe4b8990d5", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6700ab6e-e8c4-4e50-911e-60e895caf106", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "de60b4aae61b4fb9b2bd1830eae335da7f8014dd040801e25985dd9992224695", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__7 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5,x6};\n\t\typos : {y1,y2,y3,y4,y5,y6};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\t\tNORTH(y5,y6);\n\t\tSOUTH(y6,y5);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\t\tEAST(x5,x6);\n\t\tWEST(x6,x5);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x6);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y6);\n\n\t\tGOAL(x6,y6);\n\n\t\tINPUT-RATE = 0.2;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__7 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__7;\n\tinit-state {\n\t\trobot-at(x6,y1);\n\t\tobstacle-at(x1,y4);\n\t\tobstacle-at(x1,y5);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x2,y4);\n\t\tobstacle-at(x3,y2);\n\t\tobstacle-at(x3,y5);\n\t\tobstacle-at(x4,y2);\n\t\tobstacle-at(x4,y3);\n\t\tobstacle-at(x4,y4);\n\t\tobstacle-at(x4,y5);\n\t\tobstacle-at(x5,y5);\n\t\tobstacle-at(x6,y5);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ccc3e48-e57a-41a8-b549-0b387b2fcfd3": {"__data__": {"id_": "2ccc3e48-e57a-41a8-b549-0b387b2fcfd3", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae861b4c-bbb2-4f4e-b8a5-7cb33c71a5a4", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "a5fd02a732bba832a3f4e185da5d0a2f758218800ca098f96055abee1cd1c611", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__8 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5,x6};\n\t\typos : {y1,y2,y3,y4,y5,y6};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\t\tNORTH(y5,y6);\n\t\tSOUTH(y6,y5);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\t\tEAST(x5,x6);\n\t\tWEST(x6,x5);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x6);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y6);\n\n\t\tGOAL(x6,y6);\n\n\t\tINPUT-RATE = 0.4;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__8 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__8;\n\tinit-state {\n\t\trobot-at(x6,y1);\n\t\tobstacle-at(x1,y4);\n\t\tobstacle-at(x1,y5);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x2,y4);\n\t\tobstacle-at(x2,y5);\n\t\tobstacle-at(x3,y2);\n\t\tobstacle-at(x3,y3);\n\t\tobstacle-at(x3,y4);\n\t\tobstacle-at(x4,y2);\n\t\tobstacle-at(x5,y2);\n\t\tobstacle-at(x6,y2);\n\t\tobstacle-at(x6,y4);\n\t\tobstacle-at(x6,y5);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a7bc50d-adbd-49d9-9e73-9c48cefdca4c": {"__data__": {"id_": "9a7bc50d-adbd-49d9-9e73-9c48cefdca4c", "embedding": null, "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8970bd2-ddc9-4ec7-a886-74734c577316", "node_type": "4", "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}, "hash": "d3a6905088499202db2715ca93549c6372a30e5edd88ac92ea7c4c862f05c10b", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_crossing_traffic_inst_mdp__9 {\n\tdomain = crossing_traffic_mdp;\n\tobjects {\n\t\txpos : {x1,x2,x3,x4,x5,x6,x7};\n\t\typos : {y1,y2,y3,y4,y5,y6,y7};\n\t};\n\tnon-fluents {\n\t\tNORTH(y1,y2);\n\t\tSOUTH(y2,y1);\n\t\tNORTH(y2,y3);\n\t\tSOUTH(y3,y2);\n\t\tNORTH(y3,y4);\n\t\tSOUTH(y4,y3);\n\t\tNORTH(y4,y5);\n\t\tSOUTH(y5,y4);\n\t\tNORTH(y5,y6);\n\t\tSOUTH(y6,y5);\n\t\tNORTH(y6,y7);\n\t\tSOUTH(y7,y6);\n\n\t\tEAST(x1,x2);\n\t\tWEST(x2,x1);\n\t\tEAST(x2,x3);\n\t\tWEST(x3,x2);\n\t\tEAST(x3,x4);\n\t\tWEST(x4,x3);\n\t\tEAST(x4,x5);\n\t\tWEST(x5,x4);\n\t\tEAST(x5,x6);\n\t\tWEST(x6,x5);\n\t\tEAST(x6,x7);\n\t\tWEST(x7,x6);\n\n\t\tMIN-XPOS(x1);\n\t\tMAX-XPOS(x7);\n\t\tMIN-YPOS(y1);\n\t\tMAX-YPOS(y7);\n\n\t\tGOAL(x7,y7);\n\n\t\tINPUT-RATE = 0.1;\n\t};\n}\n\ninstance crossing_traffic_inst_mdp__9 {\n\tdomain = crossing_traffic_mdp;\n\tnon-fluents = nf_crossing_traffic_inst_mdp__9;\n\tinit-state {\n\t\trobot-at(x7,y1);\n\t\tobstacle-at(x1,y3);\n\t\tobstacle-at(x2,y2);\n\t\tobstacle-at(x2,y3);\n\t\tobstacle-at(x2,y4);\n\t\tobstacle-at(x2,y5);\n\t\tobstacle-at(x2,y6);\n\t\tobstacle-at(x3,y3);\n\t\tobstacle-at(x3,y5);\n\t\tobstacle-at(x4,y2);\n\t\tobstacle-at(x4,y4);\n\t\tobstacle-at(x5,y5);\n\t\tobstacle-at(x6,y2);\n\t\tobstacle-at(x6,y4);\n\t\tobstacle-at(x6,y5);\n\t\tobstacle-at(x6,y6);\n\t\tobstacle-at(x7,y2);\n\t\tobstacle-at(x7,y3);\n\t\tobstacle-at(x7,y6);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f805a092-451a-4e9c-b43c-76e21f96743b": {"__data__": {"id_": "f805a092-451a-4e9c-b43c-76e21f96743b", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "915693c2-07c6-4bd3-bfb9-7821e5c996c4", "node_type": "1", "metadata": {}, "hash": "acaaec7c1c649f6eb45b3444729e3413f4c5514180169f4b305874e487062c45", "class_name": "RelatedNodeInfo"}}, "text": "domain elevators_mdp {\n\t\n\trequirements = { \n\t\tconstrained-state,\n\t\treward-deterministic\n\t};\n\t\n\ttypes {\n  \t\televator : object;\n \t  \tfloor    : object;\n\t}; \n\t\n\tpvariables { \n\t\n\t\t// Probability someone arrives at the floor (up or down)\n\t\tARRIVE-PARAM(floor) : { non-fluent, real, default = 0.0 }; \t\t  \t\t\n\t\t\n\t\t// Penalty for persons in the elevator going in right/wrong direction\n\t\t// Note: a constant 1.0 penalty for people waiting at a floor \n\t\tELEVATOR-PENALTY-RIGHT-DIR : { non-fluent, real, default = 0.75 };\n\t\tELEVATOR-PENALTY-WRONG-DIR : { non-fluent, real, default = 3.00 };\n\n\t\t// Useful definitions\n\t\tTOP-FLOOR(floor)          : { non-fluent, bool, default = false };\n   \t\tBOTTOM-FLOOR(floor)       : { non-fluent, bool, default = false };\n\t\tADJACENT-UP(floor, floor) : { non-fluent, bool, default = false }; \t\t  \t\t\n\t\t\n\t\t// Person waiting state\n\t\tperson-waiting-up(floor)   : { state-fluent, bool, default = false };\n\t\tperson-waiting-down(floor) : { state-fluent, bool, default = false };\n\t\tperson-in-elevator-going-up(elevator)   : { state-fluent, bool, default = false };\n\t\tperson-in-elevator-going-down(elevator) : { state-fluent, bool, default = false };\n\t\t\n\t\t// Elevator state\n\t\televator-dir-up(elevator) : { state-fluent, bool, default = true };\n\t\televator-closed(elevator) : { state-fluent, bool, default = true };\n\t\televator-at-floor(elevator, floor) : { state-fluent, bool, default = false };\n\n\t\t// Actions: the elevator must move in one direction, it can only switch\n\t\t//          direction by signaling the change when the door opens\n\t\t//          (i.e., the passengers must know which direction the \n\t\t//           elevator is going before they get on... then the elevator\n\t\t//           is constrained to go in that direction when the door closes).\n\t\tmove-current-dir(elevator)     : { action-fluent, bool, default = false };\n\t\topen-door-going-up(elevator)   : { action-fluent, bool, default = false };\n\t\topen-door-going-down(elevator) : { action-fluent, bool, default = false };\n\t\tclose-door(elevator)           : { action-fluent, bool, default = false };\n\t};\n  \n\tcpfs {\n\t\t\n\t\t// We might even allow people to get off the elevator if it switches\n\t\t// directions on them while they're in it, but we won't model this now.\n\t\t\n\t\t// A person is waiting unless they get on an elevator going in their\n\t\t// direction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "915693c2-07c6-4bd3-bfb9-7821e5c996c4": {"__data__": {"id_": "915693c2-07c6-4bd3-bfb9-7821e5c996c4", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f805a092-451a-4e9c-b43c-76e21f96743b", "node_type": "1", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "6ccd4de55ccb1d76ac7f907bc0f6b059a3c857b18ec1f20abf00465a0f891a2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b9855d3-8ca2-4a48-972d-094a329a1dae", "node_type": "1", "metadata": {}, "hash": "96cae404dfc38bc0abdf0dbbebfbc6a1fb6452fda81b085a35d8409e8629793e", "class_name": "RelatedNodeInfo"}}, "text": "move-current-dir(elevator)     : { action-fluent, bool, default = false };\n\t\topen-door-going-up(elevator)   : { action-fluent, bool, default = false };\n\t\topen-door-going-down(elevator) : { action-fluent, bool, default = false };\n\t\tclose-door(elevator)           : { action-fluent, bool, default = false };\n\t};\n  \n\tcpfs {\n\t\t\n\t\t// We might even allow people to get off the elevator if it switches\n\t\t// directions on them while they're in it, but we won't model this now.\n\t\t\n\t\t// A person is waiting unless they get on an elevator going in their\n\t\t// direction.\n\t\tperson-waiting-up'(?f) = \n\t\t\tif (person-waiting-up(?f) ^ \n\t\t\t\t~(exists_{?e: elevator} [elevator-at-floor(?e, ?f) ^ elevator-dir-up(?e) ^ ~elevator-closed(?e)]))\n\t\t\tthen KronDelta(true) \n\t\t\telse Bernoulli(ARRIVE-PARAM(?f));\n\t\t\t\n\t\tperson-waiting-down'(?f) = \n\t\t\tif (person-waiting-down(?f) ^ \n\t\t\t\t~(exists_{?e: elevator} [elevator-at-floor(?e, ?f) ^ ~elevator-dir-up(?e) ^ ~elevator-closed(?e)]))\n\t\t\tthen KronDelta(true) \n\t\t\telse Bernoulli(ARRIVE-PARAM(?f));\n  \t\t\n  \t\t// A person is in the elevator going in a direction if someone gets on \n  \t\t// in that direction or someone was already on in that direction and does\n  \t\t// not get off.", "mimetype": "text/plain", "start_char_idx": 1769, "end_char_idx": 2965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b9855d3-8ca2-4a48-972d-094a329a1dae": {"__data__": {"id_": "6b9855d3-8ca2-4a48-972d-094a329a1dae", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "915693c2-07c6-4bd3-bfb9-7821e5c996c4", "node_type": "1", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "607a868080d6fd5f7e6c1f9cbcb99ba4843bb521a439dd97727b8892a3dfc465", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d67da453-7b7b-4d95-8a03-6cd2bacbf83f", "node_type": "1", "metadata": {}, "hash": "7041a43e9042fbcf501c7ee4552d115e2a7f0dba723ffefcd5eead6ad852f53f", "class_name": "RelatedNodeInfo"}}, "text": "person-in-elevator-going-up'(?e) = \n  \t\t\tif (person-in-elevator-going-up(?e))\n  \t\t\t\t// If elevator not at top floor then stays true, otherwise set to false\n  \t\t\t\tthen KronDelta( ~(exists_{?f : floor} [elevator-at-floor(?e, ?f) ^ TOP-FLOOR(?f)]) )\n  \t\t\telse\n  \t\t\t\t// No one in elevator going up... can only be true if someone going up gets in\n  \t\t\t\tKronDelta( exists_{?f : floor} \n  \t\t\t\t\t[ elevator-at-floor(?e, ?f) ^ elevator-dir-up(?e) ^ \n  \t\t\t\t\t  ~elevator-closed(?e) ^ person-waiting-up(?f) ] );\n\n  \t\tperson-in-elevator-going-down'(?e) = \n  \t\t\tif (person-in-elevator-going-down(?e))\n  \t\t\t\t// If elevator not at bottom floor then stays true, otherwise set to false\n  \t\t\t\tthen KronDelta( ~(exists_{?f : floor} [elevator-at-floor(?e, ?f) ^ BOTTOM-FLOOR(?f)]) )\n  \t\t\telse\n  \t\t\t\t// No one in elevator going up... can only be true if someone going up gets in\n  \t\t\t\tKronDelta( exists_{?f : floor} \n  \t\t\t\t\t[ elevator-at-floor(?e, ?f) ^ ~elevator-dir-up(?e) ^ \n  \t\t\t\t\t  ~elevator-closed(?e) ^ person-waiting-down(?f) ] );\n  \t\t\n\t\t// Elevator needs to be explicitly closed\n\t\televator-closed'(?e) = \n\t\t\tKronDelta([elevator-closed(?e) ^ ~open-door-going-up(?e) ^ ~open-door-going-down(?e)] \n\t\t\t\t\t  | close-door(?e));\n\n\t\t// Elevator's destination is set when door is opened (to signal\n\t\t// to people which direction the elevator is going)\n\t\televator-dir-up'(?e) = \n\t\t\tif (open-door-going-up(?e))\n\t\t\t\tthen KronDelta(true)\n\t\t\telse if (open-door-going-down(?e))\n\t\t\t\tthen KronDelta(false)\n\t\t\telse \n\t\t\t\t// If not explicitly set then previous direction persists\n\t\t\t\tKronDelta( elevator-dir-up(?e) );\n\t\t\n\t\t// Elevator movement\n\t\t//\n\t\t// Note: if the elevator should pause at a floor, it can simply open\n\t\t//       do noops (all actions false).", "mimetype": "text/plain", "start_char_idx": 2970, "end_char_idx": 4695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d67da453-7b7b-4d95-8a03-6cd2bacbf83f": {"__data__": {"id_": "d67da453-7b7b-4d95-8a03-6cd2bacbf83f", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b9855d3-8ca2-4a48-972d-094a329a1dae", "node_type": "1", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "62020d275375bfacffa6ea4901e8cf1ce98d15b256ab4346400d81fd8a22b81c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9c804f4-9482-46e3-b3a7-e0b9a03e484a", "node_type": "1", "metadata": {}, "hash": "a6a2da0d9866382d868c3606890802a836641294c7d34c761d3d5758547e9583", "class_name": "RelatedNodeInfo"}}, "text": "elevator-at-floor'(?e, ?f) =\n\t\t\n\t\t\t\n\t\t\t// Elevator does not move if door is open or elevator does not move\n\t\t\t\n\t\t\tif (~elevator-closed(?e) | ~move-current-dir(?e))\n\t\t\t\tthen KronDelta( elevator-at-floor(?e, ?f) )\n\n\t\t\t\n\t\t\t// These handle the floor that is moved to\n\t\t\t\n\t\t\telse if (move-current-dir(?e) ^ elevator-dir-up(?e) ^ exists_{?cur : floor} \n\t\t\t\t\t [elevator-at-floor(?e, ?cur) ^ ADJACENT-UP(?cur,?f)])\n\t\t\t\tthen KronDelta(true)\n\t\t\telse if (move-current-dir(?e) ^ ~elevator-dir-up(?e) ^ exists_{?cur : floor} \n\t\t\t\t\t\t[elevator-at-floor(?e, ?cur) ^ ADJACENT-UP(?f,?cur)])\n\t\t\t\tthen KronDelta(true)\n\n\t\t\t\n\t\t\t// These handle failed actions -- stay at current floor\n\t\t\t\n\t\t\telse if (move-current-dir(?e) ^ elevator-dir-up(?e) ^ ~(exists_{?next : floor} \n\t\t\t\t\t [elevator-at-floor(?e, ?f) ^ ADJACENT-UP(?f,?next)]))\n\t\t\t\tthen KronDelta( elevator-at-floor(?e, ?f) )\n\t\t\telse if (move-current-dir(?e) ^ ~elevator-dir-up(?e) ^ ~(exists_{?next : floor} \n\t\t\t\t\t\t[elevator-at-floor(?e, ?f) ^ ADJACENT-UP(?next,?f)]))\n\t\t\t\tthen KronDelta( elevator-at-floor(?e, ?f) )\n\n\t\t\t\n\t\t\t// Otherwise elevator ?e does not move to floor ?f\n\t\t\t\n\t\t\telse\n\t\t\t\t// If here, state persists\n\t\t\t\tKronDelta( false ); \n\t};\n  \n  \t// Reward is a sum of waiting penalties for those in elevators and at floor\n\treward = \n\t\t[sum_{?e: elevator} [\n\t\t\t-ELEVATOR-PENALTY-RIGHT-DIR * (person-in-elevator-going-up(?e) ^ elevator-dir-up(?e))\n\t\t]] + \n\t\t[sum_{?e: elevator} [\n\t\t\t-ELEVATOR-PENALTY-RIGHT-DIR * (person-in-elevator-going-down(?e) ^ ~elevator-dir-up(?e))\n\t\t]] + \n\t\t[sum_{?e: elevator} [\n\t\t\t-ELEVATOR-PENALTY-WRONG-DIR * (person-in-elevator-going-up(?e) ^ ~elevator-dir-up(?e))\n\t\t]] + \n\t\t[sum_{?e: elevator} [\n\t\t\t-ELEVATOR-PENALTY-WRONG-DIR * (person-in-elevator-going-down(?e) ^ elevator-dir-up(?e))\n\t\t]] + \n\t\t[sum_{?f: floor} [\n\t\t\t- person-waiting-up(?f) - person-waiting-down(?f)\n\t\t]];\n\n\tstate-action-constraints {\n\t\t// Can check uniqueness constraint in many ways, but for simulator easiest \n\t\t// is just to count.\n//\t\tforall_{?e : elevator} ([sum_{?f: floor} elevator-at-floor(?e, ?f)] == 1);\n\t\t\n\t\t// Max of one action per elevator.", "mimetype": "text/plain", "start_char_idx": 4698, "end_char_idx": 6789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9c804f4-9482-46e3-b3a7-e0b9a03e484a": {"__data__": {"id_": "c9c804f4-9482-46e3-b3a7-e0b9a03e484a", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e8aef4d-f33b-4707-93d9-601d9cec0b34", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "d18a5b7519db3def0b8fce7c88e4ad496e9ba45023e6332a3d722be8fb6c5347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d67da453-7b7b-4d95-8a03-6cd2bacbf83f", "node_type": "1", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "231d6926048a4ab7897e46a3e82634c75fa3718f6e31ee37bc40e7b0b3446df6", "class_name": "RelatedNodeInfo"}}, "text": "//\t\tforall_{?e : elevator} ([sum_{?f: floor} elevator-at-floor(?e, ?f)] == 1);\n\t\t\n\t\t// Max of one action per elevator.\n\t\tforall_{?e : elevator} [(open-door-going-up(?e) + open-door-going-down(?e) + close-door(?e) + move-current-dir(?e)) <= 1];\n\t\t\n\t\t// All floors except top and bottom must have one adjacent floor above/below\n//\t\tforall_{?f : floor} [ TOP-FLOOR(?f) | (sum_{?fup : floor} ADJACENT-UP(?f,?fup)) == 1 ];\n//\t\tforall_{?f : floor} [ BOTTOM-FLOOR(?f) | (sum_{?fdown : floor} ADJACENT-UP(?fdown,?f)) == 1 ];\n\t};\n}", "mimetype": "text/plain", "start_char_idx": 6671, "end_char_idx": 7193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "827f7d6c-2e71-4c37-86a1-846d963fac74": {"__data__": {"id_": "827f7d6c-2e71-4c37-86a1-846d963fac74", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3454decf-3339-4011-8950-c757b494954f", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "82958aff0f45f7850c790b7ee183e553eda1055c6644e7c0c89216f713388881", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__1 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0};\n\t\tfloor : {f0,f1,f2 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.14635538;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tTOP-FLOOR(f2) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__1 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__1;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea85e861-a31a-49f1-bcea-cb88aa532bef": {"__data__": {"id_": "ea85e861-a31a-49f1-bcea-cb88aa532bef", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2b65dd1-be59-4e27-b3f3-ee7ea3b5ffed", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "c9da72056c979f26b679d8328cb62b02320bda26bf394a619dc1dfb70e96e12d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__10 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0};\n\t\tfloor : {f0,f1,f2,f3,f4,f5 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.019988691;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.03846489;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tARRIVE-PARAM(f3) = 0.024069317;\n\t\tADJACENT-UP(f3,f4) = true;\n\t\tARRIVE-PARAM(f4) = 0.025387743;\n\t\tADJACENT-UP(f4,f5) = true;\n\t\tTOP-FLOOR(f5) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__10 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__10;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01d3f905-6874-490a-8df0-ffb08bed369b": {"__data__": {"id_": "01d3f905-6874-490a-8df0-ffb08bed369b", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dabb47b5-df7c-4d6a-ae0a-3523a3f56a71", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "21aa88fadcc44b11a398e77f081c224881691ec3b34287c090ad493c412c5c65", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__2 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.07776069;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tTOP-FLOOR(f2) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__2 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__2;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02ddf15b-ed7b-47a2-b809-520014d427fd": {"__data__": {"id_": "02ddf15b-ed7b-47a2-b809-520014d427fd", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbc0e080-9642-4480-9dc7-3d8d3291e033", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "1a9f6810c633cfec66fa3749e896be6ebd45fa72817f4c0531c07b33f28328b8", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__3 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.23324005;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tTOP-FLOOR(f2) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__3 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__3;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b43f3e2-2428-4ab3-b96a-187a87b126db": {"__data__": {"id_": "1b43f3e2-2428-4ab3-b96a-187a87b126db", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "484c4621-a67c-4865-a4b5-c0304f8f5961", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "47a42ed5250f74daf74ea47e1a37a2007dbc557002a6c55037fa727a65637e06", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__4 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0};\n\t\tfloor : {f0,f1,f2,f3 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.051886387;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.070308864;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tTOP-FLOOR(f3) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__4 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__4;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58f1fe67-ee1d-4376-96b1-8b405ac2143f": {"__data__": {"id_": "58f1fe67-ee1d-4376-96b1-8b405ac2143f", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05aede4a-45f8-4c4c-950f-e347a9e65901", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "1a1356fd8060c549e85e018c38971a2b94f098bbb4a69a28a472f963d5075366", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__5 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2,f3 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.06492667;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.08997825;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tTOP-FLOOR(f3) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__5 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__5;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8bb3c28-c97f-4101-8d0b-4610277d25da": {"__data__": {"id_": "f8bb3c28-c97f-4101-8d0b-4610277d25da", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79ecb65a-68f9-4a0b-9e89-bca2721d0e70", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "3421fb09598cbcf6140549ee19fd418441524a887525c347d80c28fcded9973d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__6 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2,f3 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.089050114;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.1175044;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tTOP-FLOOR(f3) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__6 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__6;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73bdee10-6d14-4ee7-92e7-54b32b6c5f12": {"__data__": {"id_": "73bdee10-6d14-4ee7-92e7-54b32b6c5f12", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1d799b6-bdbc-4bd9-9ee3-09bf53fc77ad", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "1f24556d1cab06b7da983d74163db8a2909a7f1378957fee6acb103660d279c4", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__7 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0};\n\t\tfloor : {f0,f1,f2,f3,f4 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.057459753;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.04761868;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tARRIVE-PARAM(f3) = 0.041935332;\n\t\tADJACENT-UP(f3,f4) = true;\n\t\tTOP-FLOOR(f4) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__7 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__7;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "571ef8c0-d73e-4d49-8df0-4f828c8368ac": {"__data__": {"id_": "571ef8c0-d73e-4d49-8df0-4f828c8368ac", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44c8cc16-cd61-4092-a3cd-c91558f7253b", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "11bbf1d5053aeeace3ca21c1f821aa319f185b55933469a8ac72afd3f481c3bd", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__8 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2,f3,f4 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.04416793;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.07763073;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tARRIVE-PARAM(f3) = 0.057016928;\n\t\tADJACENT-UP(f3,f4) = true;\n\t\tTOP-FLOOR(f4) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__8 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__8;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f45c9a17-eb5f-4cde-a17e-c19a94884a92": {"__data__": {"id_": "f45c9a17-eb5f-4cde-a17e-c19a94884a92", "embedding": null, "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43e996c9-4077-4398-b7f2-cf1fb19d243d", "node_type": "4", "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}, "hash": "3871380761a6ffdab7a79578cc28a7f0ad4283cc34da7dd2f1fdfe6b1be7aac1", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_elevators_inst_mdp__9 {\n\tdomain = elevators_mdp; \n\tobjects { \n\t\televator : {e0,e1};\n\t\tfloor : {f0,f1,f2,f3,f4 }; \n\t}; \n\tnon-fluents {\n\t\tELEVATOR-PENALTY-RIGHT-DIR = 0.75;\n\t\tELEVATOR-PENALTY-WRONG-DIR = 3.0;\n\t\tADJACENT-UP(f0,f1) = true;\n\t\tARRIVE-PARAM(f1) = 0.06957142;\n\t\tADJACENT-UP(f1,f2) = true;\n\t\tARRIVE-PARAM(f2) = 0.06842119;\n\t\tADJACENT-UP(f2,f3) = true;\n\t\tARRIVE-PARAM(f3) = 0.080825426;\n\t\tADJACENT-UP(f3,f4) = true;\n\t\tTOP-FLOOR(f4) = true;\n\t\tBOTTOM-FLOOR(f0) = true;\n \t}; \n }\ninstance elevators_inst_mdp__9 { \n\tdomain = elevators_mdp; \n \tnon-fluents = nf_elevators_inst_mdp__9;\n\tinit-state { \n\t\televator-at-floor(e0,f0);\n\t\televator-at-floor(e1,f0);\n\t};\n\tmax-nondef-actions = 2;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbffa9d4-6e37-476f-bb51-313d7592b715": {"__data__": {"id_": "cbffa9d4-6e37-476f-bb51-313d7592b715", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67520dd1-cfec-49d6-9d40-a0ea907e520b", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "38b68b3147d10f87cc48fb4f41aee290981a9e55063aad6d2fc21fd87d7e7186", "class_name": "RelatedNodeInfo"}}, "text": "domain game_of_life_mdp {\n  \t\n\trequirements = { reward-deterministic };\n\n\ttypes { \n\t\tx_pos : object;\n\t\ty_pos : object; \n\t};\n      \t\n\tpvariables { \n\t\tNOISE-PROB(x_pos,y_pos) : { non-fluent, real, default = 0.1 };\n\t\tNEIGHBOR(x_pos,y_pos,x_pos,y_pos) : { non-fluent, bool, default = false };\n\t\talive(x_pos,y_pos) : { state-fluent,  bool, default = false };\n\t\tset(x_pos,y_pos)   : { action-fluent, bool, default = false };\n\t};\n  \n\tcpfs {\n\t\t// Conway's game of life rules (from Wikipedia):\n\t\t// 1. Any live cell with fewer than two live neighbors dies, as if caused by under-population.\n   \t\t// 2. Any live cell with more than three live neighbors dies, as if by overcrowding.\n   \t\t// 3. Any live cell with two or three live neighbors lives on to the next generation.\n   \t\t// 4. Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction.\n   \t\t//\n   \t\t// For interactivity: we allow an agent to explicitly set different cells.\n\t\t\n\t\talive'(?x,?y) = \n\t\t\tif ([alive(?x,?y) ^ ([sum_{?x2 : x_pos, ?y2 : y_pos} NEIGHBOR(?x,?y,?x2,?y2) ^ alive(?x2,?y2)] >= 2) \n\t\t\t\t\t\t\t^ ([sum_{?x2 : x_pos, ?y2 : y_pos} NEIGHBOR(?x,?y,?x2,?y2) ^ alive(?x2,?y2)] <= 3)]\n\t\t\t\t\t\t| [~alive(?x,?y) ^ ([sum_{?x2 : x_pos, ?y2 : y_pos} NEIGHBOR(?x,?y,?x2,?y2) ^ alive(?x2,?y2)] == 3)]\n\t\t\t\t\t\t| set(?x,?y))\n\t\t\tthen Bernoulli(1.0 - NOISE-PROB(?x,?y))\n\t\t\telse Bernoulli(NOISE-PROB(?x,?y));\n\t};\n    \t\n\treward = (sum_{?x : x_pos, ?y : y_pos} [alive(?x,?y) - set(?x,?y)]);\n    \t\n    state-action-constraints {\n    \tforall_{?x : x_pos, ?y : y_pos}\n    \t\t[(NOISE-PROB(?x,?y) >= 0.0) ^ (NOISE-PROB(?x,?y) <= 1.0)];\n    };\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c704674a-d39a-49a1-8725-ab3cb59db398": {"__data__": {"id_": "c704674a-d39a-49a1-8725-ab3cb59db398", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf8a2b13-b8db-4bf6-8ca3-23b817085865", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "9c42a7a81604c90e400dd0a0d89f6fba0a7d405a02c73bedfa63b38873a4d9e0", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__1 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3};\n\t\ty_pos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.020850267;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.031577107;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.02465339;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNOISE-PROB(x2,y1) = 0.017134635;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.014217583;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.037390165;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNOISE-PROB(x3,y1) = 0.017355671;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNOISE-PROB(x3,y2) = 0.044999346;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNOISE-PROB(x3,y3) = 0.049556054;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t};\n}\n\ninstance game_of_life_inst_mdp__1 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__1;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x1,y3);\n\t\talive(x2,y1);\n\t\talive(x2,y2);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29a3533c-7201-468f-b0bf-6371bdf6ff1c": {"__data__": {"id_": "29a3533c-7201-468f-b0bf-6371bdf6ff1c", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab1d5049-59c2-484c-ab5e-9c915156cdbd", "node_type": "1", "metadata": {}, "hash": "476cb562d365042da66b67a31c27e30dc612759d2c1125ea5315af3469614bcf", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__10 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4,x5,x6,x7,x8,x9,x10};\n\t\ty_pos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.012714098;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.024050696;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.016084328;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNOISE-PROB(x2,y1) = 0.010219863;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.02083448;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.016493639;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNOISE-PROB(x3,y1) = 0.028516276;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.018745653;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab1d5049-59c2-484c-ab5e-9c915156cdbd": {"__data__": {"id_": "ab1d5049-59c2-484c-ab5e-9c915156cdbd", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a3533c-7201-468f-b0bf-6371bdf6ff1c", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "62f6e6877036feb3552efce66d63c0a73821f82c11201331d808f58e0827bb09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30", "node_type": "1", "metadata": {}, "hash": "d85c306c59db1bd791036a09b2cbe7d66404b92b08b15307e62f656d84a67616", "class_name": "RelatedNodeInfo"}}, "text": "028516276;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.018745653;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.027854873;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNOISE-PROB(x4,y1) = 0.023402575;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNEIGHBOR(x4,y1,x5,y1);\n\t\tNEIGHBOR(x4,y1,x5,y2);\n\t\tNOISE-PROB(x4,y2) = 0.021979118;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNEIGHBOR(x4,y2,x5,y1);\n\t\tNEIGHBOR(x4,y2,x5,y2);\n\t\tNEIGHBOR(x4,y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.025344696;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNOISE-PROB(x5,y1) = 0.021083586;\n\t\tNEIGHBOR(x5,y1,x4,y1);\n\t\tNEIGHBOR(x5,y1,x4,y2);\n\t\tNEIGHBOR(x5,y1,x5,y2);\n\t\tNEIGHBOR(x5,y1,x6,y1);\n\t\tNEIGHBOR(x5,y1,x6,y2);\n\t\tNOISE-PROB(x5,y2) = 0.027321136;\n\t\tNEIGHBOR(x5,y2,x4,y1);\n\t\tNEIGHBOR(x5,y2,x4,y2);\n\t\tNEIGHBOR(x5,y2,x4,y3);", "mimetype": "text/plain", "start_char_idx": 1125, "end_char_idx": 2480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30": {"__data__": {"id_": "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab1d5049-59c2-484c-ab5e-9c915156cdbd", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "12d2927e9f9aff930fc98933c454131661665efba96dc3dcf1422a330d8dd605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc", "node_type": "1", "metadata": {}, "hash": "2a80b3eca9179aaca541ff65cc74abfa71a1dc5b42785de60b9ca659fad163f3", "class_name": "RelatedNodeInfo"}}, "text": "y3,x5,y3);\n\t\tNOISE-PROB(x5,y1) = 0.021083586;\n\t\tNEIGHBOR(x5,y1,x4,y1);\n\t\tNEIGHBOR(x5,y1,x4,y2);\n\t\tNEIGHBOR(x5,y1,x5,y2);\n\t\tNEIGHBOR(x5,y1,x6,y1);\n\t\tNEIGHBOR(x5,y1,x6,y2);\n\t\tNOISE-PROB(x5,y2) = 0.027321136;\n\t\tNEIGHBOR(x5,y2,x4,y1);\n\t\tNEIGHBOR(x5,y2,x4,y2);\n\t\tNEIGHBOR(x5,y2,x4,y3);\n\t\tNEIGHBOR(x5,y2,x5,y1);\n\t\tNEIGHBOR(x5,y2,x5,y3);\n\t\tNEIGHBOR(x5,y2,x6,y1);\n\t\tNEIGHBOR(x5,y2,x6,y2);\n\t\tNEIGHBOR(x5,y2,x6,y3);\n\t\tNOISE-PROB(x5,y3) = 0.01387684;\n\t\tNEIGHBOR(x5,y3,x4,y2);\n\t\tNEIGHBOR(x5,y3,x4,y3);\n\t\tNEIGHBOR(x5,y3,x5,y2);\n\t\tNEIGHBOR(x5,y3,x6,y2);\n\t\tNEIGHBOR(x5,y3,x6,y3);\n\t\tNOISE-PROB(x6,y1) = 0.024764735;\n\t\tNEIGHBOR(x6,y1,x5,y1);\n\t\tNEIGHBOR(x6,y1,x5,y2);\n\t\tNEIGHBOR(x6,y1,x6,y2);\n\t\tNEIGHBOR(x6,y1,x7,y1);\n\t\tNEIGHBOR(x6,y1,x7,y2);\n\t\tNOISE-PROB(x6,y2) = 0.010576258;\n\t\tNEIGHBOR(x6,y2,x5,y1);\n\t\tNEIGHBOR(x6,y2,x5,y2);\n\t\tNEIGHBOR(x6,y2,x5,y3);\n\t\tNEIGHBOR(x6,y2,x6,y1);\n\t\tNEIGHBOR(x6,y2,x6,y3);\n\t\tNEIGHBOR(x6,y2,x7,y1);\n\t\tNEIGHBOR(x6,y2,x7,y2);\n\t\tNEIGHBOR(x6,y2,x7,y3);\n\t\tNOISE-PROB(x6,y3) = 0.021612609;\n\t\tNEIGHBOR(x6,y3,x5,y2);\n\t\tNEIGHBOR(x6,y3,x5,y3);\n\t\tNEIGHBOR(x6,y3,x6,y2);\n\t\tNEIGHBOR(x6,y3,x7,y2);\n\t\tNEIGHBOR(x6,y3,x7,y3);\n\t\tNOISE-PROB(x7,y1) = 0.023609534;\n\t\tNEIGHBOR(x7,y1,x6,y1);\n\t\tNEIGHBOR(x7,y1,x6,y2);\n\t\tNEIGHBOR(x7,y1,x7,y2);\n\t\tNEIGHBOR(x7,y1,x8,y1);\n\t\tNEIGHBOR(x7,y1,x8,y2);\n\t\tNOISE-PROB(x7,y2) = 0.028647663;\n\t\tNEIGHBOR(x7,y2,x6,y1);\n\t\tNEIGHBOR(x7,y2,", "mimetype": "text/plain", "start_char_idx": 2200, "end_char_idx": 3557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc": {"__data__": {"id_": "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "3f776bc93bf27bf8196cc73211123325a0658b68efbe58f29eb7a78f739e796c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b5533f4-9c8c-4f90-834a-5234003f3ce1", "node_type": "1", "metadata": {}, "hash": "89d572a48d9ab1efaead4d493fa4db15699d6733150db3b60e1ac29531084380", "class_name": "RelatedNodeInfo"}}, "text": "NEIGHBOR(x6,y3,x7,y2);\n\t\tNEIGHBOR(x6,y3,x7,y3);\n\t\tNOISE-PROB(x7,y1) = 0.023609534;\n\t\tNEIGHBOR(x7,y1,x6,y1);\n\t\tNEIGHBOR(x7,y1,x6,y2);\n\t\tNEIGHBOR(x7,y1,x7,y2);\n\t\tNEIGHBOR(x7,y1,x8,y1);\n\t\tNEIGHBOR(x7,y1,x8,y2);\n\t\tNOISE-PROB(x7,y2) = 0.028647663;\n\t\tNEIGHBOR(x7,y2,x6,y1);\n\t\tNEIGHBOR(x7,y2,x6,y2);\n\t\tNEIGHBOR(x7,y2,x6,y3);\n\t\tNEIGHBOR(x7,y2,x7,y1);\n\t\tNEIGHBOR(x7,y2,x7,y3);\n\t\tNEIGHBOR(x7,y2,x8,y1);\n\t\tNEIGHBOR(x7,y2,x8,y2);\n\t\tNEIGHBOR(x7,y2,x8,y3);\n\t\tNOISE-PROB(x7,y3) = 0.01449094;\n\t\tNEIGHBOR(x7,y3,x6,y2);\n\t\tNEIGHBOR(x7,y3,x6,y3);\n\t\tNEIGHBOR(x7,y3,x7,y2);\n\t\tNEIGHBOR(x7,y3,x8,y2);\n\t\tNEIGHBOR(x7,y3,x8,y3);\n\t\tNOISE-PROB(x8,y1) = 0.018533165;\n\t\tNEIGHBOR(x8,y1,x7,y1);\n\t\tNEIGHBOR(x8,y1,x7,y2);\n\t\tNEIGHBOR(x8,y1,x8,y2);\n\t\tNEIGHBOR(x8,y1,x9,y1);\n\t\tNEIGHBOR(x8,y1,x9,y2);\n\t\tNOISE-PROB(x8,y2) = 0.013072096;\n\t\tNEIGHBOR(x8,y2,x7,y1);\n\t\tNEIGHBOR(x8,y2,x7,y2);\n\t\tNEIGHBOR(x8,y2,x7,y3);\n\t\tNEIGHBOR(x8,y2,x8,y1);\n\t\tNEIGHBOR(x8,y2,x8,y3);\n\t\tNEIGHBOR(x8,y2,x9,y1);\n\t\tNEIGHBOR(x8,y2,x9,y2);\n\t\tNEIGHBOR(x8,y2,x9,y3);\n\t\tNOISE-PROB(x8,y3) = 0.02124291;\n\t\tNEIGHBOR(x8,y3,x7,y2);\n\t\tNEIGHBOR(x8,y3,x7,y3);\n\t\tNEIGHBOR(x8,y3,x8,y2);\n\t\tNEIGHBOR(x8,y3,x9,y2);\n\t\tNEIGHBOR(x8,y3,x9,y3);\n\t\tNOISE-PROB(x9,y1) = 0.011981677;\n\t\tNEIGHBOR(x9,y1,x8,y1);\n\t\tNEIGHBOR(x9,y1,x8,y2);\n\t\tNEIGHBOR(x9,y1,x9,y2);\n\t\tNEIGHBOR(x9,y1,x10,y1);\n\t\tNEIGHBOR(x9,y1,x10,y2);\n\t\tNOISE-PROB(x9,y2) = 0.026951706;", "mimetype": "text/plain", "start_char_idx": 3272, "end_char_idx": 4624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b5533f4-9c8c-4f90-834a-5234003f3ce1": {"__data__": {"id_": "5b5533f4-9c8c-4f90-834a-5234003f3ce1", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b611e2-c0e3-41d6-87da-2068c38a9da0", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b7fd188b8bd6027b8cd68fd7d57c527498ca13514256d5865765301a12c72b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "2b255a890f93a3b74a109630100707c8cf30e1ad6c06447441777bc20a4dcf3d", "class_name": "RelatedNodeInfo"}}, "text": "y3,x7,y3);\n\t\tNEIGHBOR(x8,y3,x8,y2);\n\t\tNEIGHBOR(x8,y3,x9,y2);\n\t\tNEIGHBOR(x8,y3,x9,y3);\n\t\tNOISE-PROB(x9,y1) = 0.011981677;\n\t\tNEIGHBOR(x9,y1,x8,y1);\n\t\tNEIGHBOR(x9,y1,x8,y2);\n\t\tNEIGHBOR(x9,y1,x9,y2);\n\t\tNEIGHBOR(x9,y1,x10,y1);\n\t\tNEIGHBOR(x9,y1,x10,y2);\n\t\tNOISE-PROB(x9,y2) = 0.026951706;\n\t\tNEIGHBOR(x9,y2,x8,y1);\n\t\tNEIGHBOR(x9,y2,x8,y2);\n\t\tNEIGHBOR(x9,y2,x8,y3);\n\t\tNEIGHBOR(x9,y2,x9,y1);\n\t\tNEIGHBOR(x9,y2,x9,y3);\n\t\tNEIGHBOR(x9,y2,x10,y1);\n\t\tNEIGHBOR(x9,y2,x10,y2);\n\t\tNEIGHBOR(x9,y2,x10,y3);\n\t\tNOISE-PROB(x9,y3) = 0.014111699;\n\t\tNEIGHBOR(x9,y3,x8,y2);\n\t\tNEIGHBOR(x9,y3,x8,y3);\n\t\tNEIGHBOR(x9,y3,x9,y2);\n\t\tNEIGHBOR(x9,y3,x10,y2);\n\t\tNEIGHBOR(x9,y3,x10,y3);\n\t\tNOISE-PROB(x10,y1) = 0.02276952;\n\t\tNEIGHBOR(x10,y1,x9,y1);\n\t\tNEIGHBOR(x10,y1,x9,y2);\n\t\tNEIGHBOR(x10,y1,x10,y2);\n\t\tNOISE-PROB(x10,y2) = 0.01313008;\n\t\tNEIGHBOR(x10,y2,x9,y1);\n\t\tNEIGHBOR(x10,y2,x9,y2);\n\t\tNEIGHBOR(x10,y2,x9,y3);\n\t\tNEIGHBOR(x10,y2,x10,y1);\n\t\tNEIGHBOR(x10,y2,x10,y3);\n\t\tNOISE-PROB(x10,y3) = 0.02819415;\n\t\tNEIGHBOR(x10,y3,x9,y2);\n\t\tNEIGHBOR(x10,y3,x9,y3);\n\t\tNEIGHBOR(x10,y3,x10,y2);\n\t};\n}\n\ninstance game_of_life_inst_mdp__10 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__10;\n\tinit-state {\n\t\talive(x1,y3);\n\t\talive(x2,y2);\n\t\talive(x2,y3);\n\t\talive(x4,y1);\n\t\talive(x4,y3);\n\t\talive(x6,y2);\n\t\talive(x6,y3);\n\t\talive(x7,y1);\n\t\talive(x8,y1);\n\t\talive(x8,y2);\n\t\talive(x8,y3);\n\t\talive(x10,y1);\n\t\talive(x10,y2);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 4342, "end_char_idx": 5790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8166559f-689f-4923-be10-3464ac53348d": {"__data__": {"id_": "8166559f-689f-4923-be10-3464ac53348d", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c6913da-33d4-4e1f-b2ed-aa431e0e647b", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ea3332835be468ac60602a16727a3087d72b946ad8d15be776fe92e9d9ea1f80", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__2 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3};\n\t\ty_pos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.086708486;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.07763067;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.09053348;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNOISE-PROB(x2,y1) = 0.06538754;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.09307135;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.0581377;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNOISE-PROB(x3,y1) = 0.061724667;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNOISE-PROB(x3,y2) = 0.09901054;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNOISE-PROB(x3,y3) = 0.06759059;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t};\n}\n\ninstance game_of_life_inst_mdp__2 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__2;\n\tinit-state {\n\t\talive(x3,y1);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e923a96-3ea6-46de-b35d-098a3c6fdffb": {"__data__": {"id_": "5e923a96-3ea6-46de-b35d-098a3c6fdffb", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2a375c1-9a18-423c-bf9c-7eb83fe2e533", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e1f54a1389e2cb0f88799ab8d977741525105a6dcffc2c036bb5a45a8bc4d231", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__3 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3};\n\t\ty_pos : {y1,y2,y3};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.10457405;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.13319133;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.12977487;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNOISE-PROB(x2,y1) = 0.11337666;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.14685884;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.14385562;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNOISE-PROB(x3,y1) = 0.126071;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNOISE-PROB(x3,y2) = 0.1329396;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNOISE-PROB(x3,y3) = 0.13876444;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t};\n}\n\ninstance game_of_life_inst_mdp__3 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__3;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x3,y1);\n\t\talive(x3,y3);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fe86272-f8fe-44ea-8dea-f78f291817ae": {"__data__": {"id_": "5fe86272-f8fe-44ea-8dea-f78f291817ae", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0da4639d00644759d25ee246c208105e648eb4562cf15588ecd5af9a8551b045", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7245f55-14cd-4955-8336-6e510cbffbce", "node_type": "1", "metadata": {}, "hash": "407f7cbbede64912e260c1eef240ac155f1a332ed49e3e88ba2ca7ec62370b60", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__4 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4};\n\t\ty_pos : {y1,y2,y3,y4};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.03923399;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.0314785;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.045545667;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.01801408;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNOISE-PROB(x2,y1) = 0.033678092;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.026349675;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.04605113;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.017919231;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7245f55-14cd-4955-8336-6e510cbffbce": {"__data__": {"id_": "c7245f55-14cd-4955-8336-6e510cbffbce", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0da4639d00644759d25ee246c208105e648eb4562cf15588ecd5af9a8551b045", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fe86272-f8fe-44ea-8dea-f78f291817ae", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "87fcbca43027b1e46eec96be3b344a9a9218db68885c6e6c8a1323efa81e895f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27fee787-e2b8-4b3e-a3a0-62b6947902b5", "node_type": "1", "metadata": {}, "hash": "d2916ea799964c2fb5b2bd08fb37249e1f1251479951c9a124f0ee8b0e5d0bbd", "class_name": "RelatedNodeInfo"}}, "text": "y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.017919231;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNOISE-PROB(x3,y1) = 0.019390307;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.048203297;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.023413055;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.036412105;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.0243228;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.029662048;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);", "mimetype": "text/plain", "start_char_idx": 1129, "end_char_idx": 2482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27fee787-e2b8-4b3e-a3a0-62b6947902b5": {"__data__": {"id_": "27fee787-e2b8-4b3e-a3a0-62b6947902b5", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7b7c6c8-917c-4718-ad37-77d919bbca4a", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0da4639d00644759d25ee246c208105e648eb4562cf15588ecd5af9a8551b045", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7245f55-14cd-4955-8336-6e510cbffbce", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "885061b08c967bbb191db953ae5c1d6d983205c8295caf41931cee415b6f1087", "class_name": "RelatedNodeInfo"}}, "text": "y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.0243228;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.029662048;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNOISE-PROB(x4,y3) = 0.02418549;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNOISE-PROB(x4,y4) = 0.023468675;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t};\n}\n\ninstance game_of_life_inst_mdp__4 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__4;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x1,y3);\n\t\talive(x1,y4);\n\t\talive(x2,y2);\n\t\talive(x3,y3);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2204, "end_char_idx": 3075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5fd340d-ce41-4f6e-bb0a-c3a2ebcfae1a": {"__data__": {"id_": "d5fd340d-ce41-4f6e-bb0a-c3a2ebcfae1a", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ebe70135d0c2c4f5b9fef2d4820596aef913bfea710a8f04601ae119ce964ebf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "facc419e-78a7-419c-ac16-c23755051f8f", "node_type": "1", "metadata": {}, "hash": "a7234397f1614d0cc9a0f1e0e96f88a49eb076bb6bd52e0392fade40b4cc6eb6", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__5 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4};\n\t\ty_pos : {y1,y2,y3,y4};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.08170411;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.06457257;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.057646375;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.09161495;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNOISE-PROB(x2,y1) = 0.08360939;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.06658315;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.055408515;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.08932304;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "facc419e-78a7-419c-ac16-c23755051f8f": {"__data__": {"id_": "facc419e-78a7-419c-ac16-c23755051f8f", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ebe70135d0c2c4f5b9fef2d4820596aef913bfea710a8f04601ae119ce964ebf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5fd340d-ce41-4f6e-bb0a-c3a2ebcfae1a", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0f7f993212b35fe1580595dfac33957edad5678bcd68a59013a37627cdc787bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ddc8cbc-3165-46f8-a6d4-9a0d050275be", "node_type": "1", "metadata": {}, "hash": "ab5163482c213627d2856c2fa9f37cc67d3f372562116e51fc11b0ef40858977", "class_name": "RelatedNodeInfo"}}, "text": "y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.08932304;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNOISE-PROB(x3,y1) = 0.07570642;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.098435394;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.08612682;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.07202495;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.087329805;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.0678805;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);", "mimetype": "text/plain", "start_char_idx": 1129, "end_char_idx": 2478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ddc8cbc-3165-46f8-a6d4-9a0d050275be": {"__data__": {"id_": "6ddc8cbc-3165-46f8-a6d4-9a0d050275be", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cc23001-518b-4ab4-9853-e1d69e9ca2b2", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ebe70135d0c2c4f5b9fef2d4820596aef913bfea710a8f04601ae119ce964ebf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "facc419e-78a7-419c-ac16-c23755051f8f", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "fa807c39ffe279a18bdcde8d2738ed6c8f3ee72176e09d32bad2fae6001a6a6c", "class_name": "RelatedNodeInfo"}}, "text": "y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.087329805;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.0678805;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNOISE-PROB(x4,y3) = 0.070467114;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNOISE-PROB(x4,y4) = 0.054750577;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t};\n}\n\ninstance game_of_life_inst_mdp__5 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__5;\n\tinit-state {\n\t\talive(x1,y2);\n\t\talive(x1,y3);\n\t\talive(x2,y2);\n\t\talive(x2,y3);\n\t\talive(x3,y1);\n\t\talive(x3,y2);\n\t\talive(x3,y3);\n\t\talive(x4,y1);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2200, "end_char_idx": 3120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "792f14e3-361a-424f-a47a-7c2c434a373a": {"__data__": {"id_": "792f14e3-361a-424f-a47a-7c2c434a373a", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b373fb97-7e0f-4685-89a2-84d10a145106", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4c1ea93f3dfcf18e6858821cc13bea91ce74527111472030ce466ea4044fbd54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "029a94c3-83f9-432e-b31f-43363ed39bf8", "node_type": "1", "metadata": {}, "hash": "c73db78ecfb282d4642e30cd2894845063ccd9e006c9c43b7308c33146c7bba1", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__6 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4};\n\t\ty_pos : {y1,y2,y3,y4};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.108406425;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.11904223;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.11202998;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.10162268;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNOISE-PROB(x2,y1) = 0.10322304;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.12109037;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.119622916;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.14511083;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "029a94c3-83f9-432e-b31f-43363ed39bf8": {"__data__": {"id_": "029a94c3-83f9-432e-b31f-43363ed39bf8", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b373fb97-7e0f-4685-89a2-84d10a145106", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4c1ea93f3dfcf18e6858821cc13bea91ce74527111472030ce466ea4044fbd54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "792f14e3-361a-424f-a47a-7c2c434a373a", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e4a7d8aeb0ab278c1da16beb7c92385ec563e9a73f2f0f72b2b024e8e7ad7726", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c63a1ce9-6451-4e1f-96d0-c3a0a10ade8b", "node_type": "1", "metadata": {}, "hash": "1e5a7119fed35afb4bd5d04447d360ee3113707753b84fd50a440fdc4ef1b2c2", "class_name": "RelatedNodeInfo"}}, "text": "y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.14511083;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNOISE-PROB(x3,y1) = 0.12707518;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.13026968;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.11564466;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.13718301;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.14839673;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.14451447;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);", "mimetype": "text/plain", "start_char_idx": 1129, "end_char_idx": 2477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c63a1ce9-6451-4e1f-96d0-c3a0a10ade8b": {"__data__": {"id_": "c63a1ce9-6451-4e1f-96d0-c3a0a10ade8b", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b373fb97-7e0f-4685-89a2-84d10a145106", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4c1ea93f3dfcf18e6858821cc13bea91ce74527111472030ce466ea4044fbd54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "029a94c3-83f9-432e-b31f-43363ed39bf8", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "04c53f5941220b33787b3b01843f7958b9c68f421e3bafe71d333c058ff4421a", "class_name": "RelatedNodeInfo"}}, "text": "y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNOISE-PROB(x4,y1) = 0.14839673;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNOISE-PROB(x4,y2) = 0.14451447;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNOISE-PROB(x4,y3) = 0.14957437;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNOISE-PROB(x4,y4) = 0.13314088;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t};\n}\n\ninstance game_of_life_inst_mdp__6 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__6;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x1,y3);\n\t\talive(x2,y1);\n\t\talive(x2,y2);\n\t\talive(x2,y3);\n\t\talive(x3,y2);\n\t\talive(x3,y4);\n\t\talive(x4,y1);\n\t\talive(x4,y3);\n\t\talive(x4,y4);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2199, "end_char_idx": 3149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc697b9b-d9a7-47e3-922c-7c2bfaf342cc": {"__data__": {"id_": "dc697b9b-d9a7-47e3-922c-7c2bfaf342cc", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66a64a94-fe82-4f09-af53-4b2d23d05914", "node_type": "1", "metadata": {}, "hash": "de070d64c37ea849ddab1ac225113d979007c824f3721365e543d745a4fea5bb", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__7 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4,x5};\n\t\ty_pos : {y1,y2,y3,y4,y5};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.010703316;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.010708468;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.04702638;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.011425058;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x1,y5);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNEIGHBOR(x1,y4,x2,y5);\n\t\tNOISE-PROB(x1,y5) = 0.026255652;\n\t\tNEIGHBOR(x1,y5,x1,y4);\n\t\tNEIGHBOR(x1,y5,x2,y4);\n\t\tNEIGHBOR(x1,y5,x2,y5);\n\t\tNOISE-PROB(x2,y1) = 0.044376127;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.013822777;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.04265871;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66a64a94-fe82-4f09-af53-4b2d23d05914": {"__data__": {"id_": "66a64a94-fe82-4f09-af53-4b2d23d05914", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc697b9b-d9a7-47e3-922c-7c2bfaf342cc", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "3583a126e6bb7aceb7027654f28ce8f5a85ae28f631e3288611eedf10898c337", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18c5c911-ac59-4517-8e56-1edf2ca7558d", "node_type": "1", "metadata": {}, "hash": "688f00ef8162e92f95467f95119c1438237c32a6a19bf69951849e4dfe67f50b", "class_name": "RelatedNodeInfo"}}, "text": "y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.04265871;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.049125753;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x1,y5);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x2,y5);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNEIGHBOR(x2,y4,x3,y5);\n\t\tNOISE-PROB(x2,y5) = 0.015448101;\n\t\tNEIGHBOR(x2,y5,x1,y4);\n\t\tNEIGHBOR(x2,y5,x1,y5);\n\t\tNEIGHBOR(x2,y5,x2,y4);\n\t\tNEIGHBOR(x2,y5,x3,y4);\n\t\tNEIGHBOR(x2,y5,x3,y5);\n\t\tNOISE-PROB(x3,y1) = 0.012252467;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.044863924;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.0169857;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);", "mimetype": "text/plain", "start_char_idx": 1139, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18c5c911-ac59-4517-8e56-1edf2ca7558d": {"__data__": {"id_": "18c5c911-ac59-4517-8e56-1edf2ca7558d", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66a64a94-fe82-4f09-af53-4b2d23d05914", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "f62ba6b70a899d11564a23ae70ff63315f5b274d98a106b60f34b8423c400a11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9e2837e-078c-4189-8f23-48896e9f608e", "node_type": "1", "metadata": {}, "hash": "eb4f3de400b45a3787a6d40b019ce4f7fd1a660ee0e06bb5f943e5ce7202dada", "class_name": "RelatedNodeInfo"}}, "text": "y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.0169857;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.015599415;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x2,y5);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x3,y5);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNEIGHBOR(x3,y4,x4,y5);\n\t\tNOISE-PROB(x3,y5) = 0.01906131;\n\t\tNEIGHBOR(x3,y5,x2,y4);\n\t\tNEIGHBOR(x3,y5,x2,y5);\n\t\tNEIGHBOR(x3,y5,x3,y4);\n\t\tNEIGHBOR(x3,y5,x4,y4);\n\t\tNEIGHBOR(x3,y5,x4,y5);\n\t\tNOISE-PROB(x4,y1) = 0.038328312;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNEIGHBOR(x4,y1,x5,y1);\n\t\tNEIGHBOR(x4,y1,x5,y2);\n\t\tNOISE-PROB(x4,y2) = 0.033463318;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNEIGHBOR(x4,y2,x5,y1);\n\t\tNEIGHBOR(x4,y2,x5,y2);\n\t\tNEIGHBOR(x4,y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.010245487;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.03743592;", "mimetype": "text/plain", "start_char_idx": 2213, "end_char_idx": 3564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9e2837e-078c-4189-8f23-48896e9f608e": {"__data__": {"id_": "a9e2837e-078c-4189-8f23-48896e9f608e", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18c5c911-ac59-4517-8e56-1edf2ca7558d", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ec194972f1a5a3c21cf8865cc71321f694b48134a64f978f533217cddfd1febb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b6b8481-2fa7-4ad9-b0c0-f261e7dafea5", "node_type": "1", "metadata": {}, "hash": "0183ce069e4392e4a78c5fe6b1f3b321affeb7682c6e97f66591dbefb6405b48", "class_name": "RelatedNodeInfo"}}, "text": "y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.010245487;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.03743592;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x3,y5);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t\tNEIGHBOR(x4,y4,x4,y5);\n\t\tNEIGHBOR(x4,y4,x5,y3);\n\t\tNEIGHBOR(x4,y4,x5,y4);\n\t\tNEIGHBOR(x4,y4,x5,y5);\n\t\tNOISE-PROB(x4,y5) = 0.020325921;\n\t\tNEIGHBOR(x4,y5,x3,y4);\n\t\tNEIGHBOR(x4,y5,x3,y5);\n\t\tNEIGHBOR(x4,y5,x4,y4);\n\t\tNEIGHBOR(x4,y5,x5,y4);\n\t\tNEIGHBOR(x4,y5,x5,y5);\n\t\tNOISE-PROB(x5,y1) = 0.023148427;\n\t\tNEIGHBOR(x5,y1,x4,y1);\n\t\tNEIGHBOR(x5,y1,x4,y2);\n\t\tNEIGHBOR(x5,y1,x5,y2);\n\t\tNOISE-PROB(x5,y2) = 0.049670517;\n\t\tNEIGHBOR(x5,y2,x4,y1);\n\t\tNEIGHBOR(x5,y2,x4,y2);\n\t\tNEIGHBOR(x5,y2,x4,y3);\n\t\tNEIGHBOR(x5,y2,x5,y1);\n\t\tNEIGHBOR(x5,y2,x5,y3);\n\t\tNOISE-PROB(x5,y3) = 0.03982632;\n\t\tNEIGHBOR(x5,y3,x4,y2);\n\t\tNEIGHBOR(x5,y3,x4,y3);\n\t\tNEIGHBOR(x5,y3,x4,y4);\n\t\tNEIGHBOR(x5,y3,x5,y2);\n\t\tNEIGHBOR(x5,y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.020023348;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.0269987;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};", "mimetype": "text/plain", "start_char_idx": 3285, "end_char_idx": 4625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b6b8481-2fa7-4ad9-b0c0-f261e7dafea5": {"__data__": {"id_": "5b6b8481-2fa7-4ad9-b0c0-f261e7dafea5", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "88df41167e0847213a5ec65031e2c11a418a25bf9be893a731941201449aae49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9e2837e-078c-4189-8f23-48896e9f608e", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e3087a9fa3a914a17fb176b4ed7fda55185382524b018f7b53c5bf89bef6f3e1", "class_name": "RelatedNodeInfo"}}, "text": "y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.020023348;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.0269987;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};\n}\n\ninstance game_of_life_inst_mdp__7 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__7;\n\tinit-state {\n\t\talive(x1,y2);\n\t\talive(x1,y4);\n\t\talive(x2,y1);\n\t\talive(x2,y2);\n\t\talive(x2,y4);\n\t\talive(x2,y5);\n\t\talive(x3,y1);\n\t\talive(x3,y4);\n\t\talive(x3,y5);\n\t\talive(x4,y1);\n\t\talive(x4,y3);\n\t\talive(x5,y1);\n\t\talive(x5,y3);\n\t\talive(x5,y4);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 4343, "end_char_idx": 5039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc12134c-79e9-4c83-ade9-f22e23555424": {"__data__": {"id_": "cc12134c-79e9-4c83-ade9-f22e23555424", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0777f580-1526-4acb-94eb-331f78f1bde6", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29b47d7c-a151-489e-b940-2c1caaae1368", "node_type": "1", "metadata": {}, "hash": "08218fba780fb947ec0236462f8a7fdd9d2451edb21495c3a64af36876a2bba8", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__8 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4,x5};\n\t\ty_pos : {y1,y2,y3,y4,y5};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.06735329;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.045206487;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.054894388;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.044731613;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x1,y5);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNEIGHBOR(x1,y4,x2,y5);\n\t\tNOISE-PROB(x1,y5) = 0.06041733;\n\t\tNEIGHBOR(x1,y5,x1,y4);\n\t\tNEIGHBOR(x1,y5,x2,y4);\n\t\tNEIGHBOR(x1,y5,x2,y5);\n\t\tNOISE-PROB(x2,y1) = 0.044897996;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.063448586;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.037478086;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29b47d7c-a151-489e-b940-2c1caaae1368": {"__data__": {"id_": "29b47d7c-a151-489e-b940-2c1caaae1368", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0777f580-1526-4acb-94eb-331f78f1bde6", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc12134c-79e9-4c83-ade9-f22e23555424", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "387384d99f2ca4adc9736e6ad927587f8b0a5317fe9f7cb8b6a1bba9c12efcb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6fdf905-7832-4676-b275-54bc2a882bdf", "node_type": "1", "metadata": {}, "hash": "edf3b7d5fc66491275557507c15fccffa42af9a099ac304d97d4e95454c20f61", "class_name": "RelatedNodeInfo"}}, "text": "y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.037478086;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.039684244;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x1,y5);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x2,y5);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNEIGHBOR(x2,y4,x3,y5);\n\t\tNOISE-PROB(x2,y5) = 0.042371973;\n\t\tNEIGHBOR(x2,y5,x1,y4);\n\t\tNEIGHBOR(x2,y5,x1,y5);\n\t\tNEIGHBOR(x2,y5,x2,y4);\n\t\tNEIGHBOR(x2,y5,x3,y4);\n\t\tNEIGHBOR(x2,y5,x3,y5);\n\t\tNOISE-PROB(x3,y1) = 0.051712807;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.05565058;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.057950255;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);", "mimetype": "text/plain", "start_char_idx": 1138, "end_char_idx": 2482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6fdf905-7832-4676-b275-54bc2a882bdf": {"__data__": {"id_": "e6fdf905-7832-4676-b275-54bc2a882bdf", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0777f580-1526-4acb-94eb-331f78f1bde6", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29b47d7c-a151-489e-b940-2c1caaae1368", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "463e2c91f90f5a1f1e4c3cf6418896d600298c803fe8758c6b29188163558ffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ac3dae5-c9da-49d1-a729-1d0b3f19b541", "node_type": "1", "metadata": {}, "hash": "d48b637daea62c0da2acca196577ebdc5d9e0bca05de28f63bba4be36cf43840", "class_name": "RelatedNodeInfo"}}, "text": "y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.057950255;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.0643877;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x2,y5);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x3,y5);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNEIGHBOR(x3,y4,x4,y5);\n\t\tNOISE-PROB(x3,y5) = 0.05106218;\n\t\tNEIGHBOR(x3,y5,x2,y4);\n\t\tNEIGHBOR(x3,y5,x2,y5);\n\t\tNEIGHBOR(x3,y5,x3,y4);\n\t\tNEIGHBOR(x3,y5,x4,y4);\n\t\tNEIGHBOR(x3,y5,x4,y5);\n\t\tNOISE-PROB(x4,y1) = 0.04899808;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNEIGHBOR(x4,y1,x5,y1);\n\t\tNEIGHBOR(x4,y1,x5,y2);\n\t\tNOISE-PROB(x4,y2) = 0.039407507;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNEIGHBOR(x4,y2,x5,y1);\n\t\tNEIGHBOR(x4,y2,x5,y2);\n\t\tNEIGHBOR(x4,y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.048497245;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.060630508;", "mimetype": "text/plain", "start_char_idx": 2212, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ac3dae5-c9da-49d1-a729-1d0b3f19b541": {"__data__": {"id_": "3ac3dae5-c9da-49d1-a729-1d0b3f19b541", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0777f580-1526-4acb-94eb-331f78f1bde6", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6fdf905-7832-4676-b275-54bc2a882bdf", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4c3d231c5f36b13e7a63b0539f94161be4e73761755bc8eca027716897527a3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bf8b686-7209-4077-a1fd-3a46f2205907", "node_type": "1", "metadata": {}, "hash": "7a0401edab6d9251a51e47167f4b5b9524f237e623115489bf55d9d9bf380714", "class_name": "RelatedNodeInfo"}}, "text": "y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.048497245;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.060630508;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x3,y5);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t\tNEIGHBOR(x4,y4,x4,y5);\n\t\tNEIGHBOR(x4,y4,x5,y3);\n\t\tNEIGHBOR(x4,y4,x5,y4);\n\t\tNEIGHBOR(x4,y4,x5,y5);\n\t\tNOISE-PROB(x4,y5) = 0.03444973;\n\t\tNEIGHBOR(x4,y5,x3,y4);\n\t\tNEIGHBOR(x4,y5,x3,y5);\n\t\tNEIGHBOR(x4,y5,x4,y4);\n\t\tNEIGHBOR(x4,y5,x5,y4);\n\t\tNEIGHBOR(x4,y5,x5,y5);\n\t\tNOISE-PROB(x5,y1) = 0.05395849;\n\t\tNEIGHBOR(x5,y1,x4,y1);\n\t\tNEIGHBOR(x5,y1,x4,y2);\n\t\tNEIGHBOR(x5,y1,x5,y2);\n\t\tNOISE-PROB(x5,y2) = 0.053761184;\n\t\tNEIGHBOR(x5,y2,x4,y1);\n\t\tNEIGHBOR(x5,y2,x4,y2);\n\t\tNEIGHBOR(x5,y2,x4,y3);\n\t\tNEIGHBOR(x5,y2,x5,y1);\n\t\tNEIGHBOR(x5,y2,x5,y3);\n\t\tNOISE-PROB(x5,y3) = 0.06362108;\n\t\tNEIGHBOR(x5,y3,x4,y2);\n\t\tNEIGHBOR(x5,y3,x4,y3);\n\t\tNEIGHBOR(x5,y3,x4,y4);\n\t\tNEIGHBOR(x5,y3,x5,y2);\n\t\tNEIGHBOR(x5,y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.044641457;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.06326831;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};", "mimetype": "text/plain", "start_char_idx": 3283, "end_char_idx": 4623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bf8b686-7209-4077-a1fd-3a46f2205907": {"__data__": {"id_": "9bf8b686-7209-4077-a1fd-3a46f2205907", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0777f580-1526-4acb-94eb-331f78f1bde6", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "917bf2f6167803fdbc8e331cdc22faffe3bf58ac342800de37bc1b53451af4b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ac3dae5-c9da-49d1-a729-1d0b3f19b541", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "72980040ba35f5802c8d2b5c5d12ebcaa5c1f7de22e6f7b40debf7ceddb7b768", "class_name": "RelatedNodeInfo"}}, "text": "y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.044641457;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.06326831;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};\n}\n\ninstance game_of_life_inst_mdp__8 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__8;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x1,y4);\n\t\talive(x1,y5);\n\t\talive(x2,y1);\n\t\talive(x2,y5);\n\t\talive(x3,y1);\n\t\talive(x3,y2);\n\t\talive(x3,y4);\n\t\talive(x4,y2);\n\t\talive(x4,y4);\n\t\talive(x5,y3);\n\t\talive(x5,y4);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 4340, "end_char_idx": 5005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42da3e91-6ad8-4ddf-9860-ebdd3d442590": {"__data__": {"id_": "42da3e91-6ad8-4ddf-9860-ebdd3d442590", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e769163-1759-442c-a118-beb1c7a10663", "node_type": "1", "metadata": {}, "hash": "7e443446294ff03d752f0636a2b6e3b974745204d0c9dbff555269d13d6e3cda", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_game_of_life_inst_mdp__9 {\n\tdomain = game_of_life_mdp;\n\tobjects {\n\t\tx_pos : {x1,x2,x3,x4,x5};\n\t\ty_pos : {y1,y2,y3,y4,y5};\n\t};\n\tnon-fluents {\n\t\tNOISE-PROB(x1,y1) = 0.05305214;\n\t\tNEIGHBOR(x1,y1,x1,y2);\n\t\tNEIGHBOR(x1,y1,x2,y1);\n\t\tNEIGHBOR(x1,y1,x2,y2);\n\t\tNOISE-PROB(x1,y2) = 0.08104587;\n\t\tNEIGHBOR(x1,y2,x1,y1);\n\t\tNEIGHBOR(x1,y2,x1,y3);\n\t\tNEIGHBOR(x1,y2,x2,y1);\n\t\tNEIGHBOR(x1,y2,x2,y2);\n\t\tNEIGHBOR(x1,y2,x2,y3);\n\t\tNOISE-PROB(x1,y3) = 0.08625997;\n\t\tNEIGHBOR(x1,y3,x1,y2);\n\t\tNEIGHBOR(x1,y3,x1,y4);\n\t\tNEIGHBOR(x1,y3,x2,y2);\n\t\tNEIGHBOR(x1,y3,x2,y3);\n\t\tNEIGHBOR(x1,y3,x2,y4);\n\t\tNOISE-PROB(x1,y4) = 0.05275586;\n\t\tNEIGHBOR(x1,y4,x1,y3);\n\t\tNEIGHBOR(x1,y4,x1,y5);\n\t\tNEIGHBOR(x1,y4,x2,y3);\n\t\tNEIGHBOR(x1,y4,x2,y4);\n\t\tNEIGHBOR(x1,y4,x2,y5);\n\t\tNOISE-PROB(x1,y5) = 0.054442063;\n\t\tNEIGHBOR(x1,y5,x1,y4);\n\t\tNEIGHBOR(x1,y5,x2,y4);\n\t\tNEIGHBOR(x1,y5,x2,y5);\n\t\tNOISE-PROB(x2,y1) = 0.05921717;\n\t\tNEIGHBOR(x2,y1,x1,y1);\n\t\tNEIGHBOR(x2,y1,x1,y2);\n\t\tNEIGHBOR(x2,y1,x2,y2);\n\t\tNEIGHBOR(x2,y1,x3,y1);\n\t\tNEIGHBOR(x2,y1,x3,y2);\n\t\tNOISE-PROB(x2,y2) = 0.08715388;\n\t\tNEIGHBOR(x2,y2,x1,y1);\n\t\tNEIGHBOR(x2,y2,x1,y2);\n\t\tNEIGHBOR(x2,y2,x1,y3);\n\t\tNEIGHBOR(x2,y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.080226846;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e769163-1759-442c-a118-beb1c7a10663": {"__data__": {"id_": "9e769163-1759-442c-a118-beb1c7a10663", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42da3e91-6ad8-4ddf-9860-ebdd3d442590", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "87f9215ae94ca75c2b1431a5c77c663786141b62573020cec8e1544081b82894", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72b124c1-6962-42fc-b351-43e43cba60c0", "node_type": "1", "metadata": {}, "hash": "bf4e45eecdeb955ab6b0582d5d572fd80498b6861f57e3e06873749f4c3ec100", "class_name": "RelatedNodeInfo"}}, "text": "y2,x2,y1);\n\t\tNEIGHBOR(x2,y2,x2,y3);\n\t\tNEIGHBOR(x2,y2,x3,y1);\n\t\tNEIGHBOR(x2,y2,x3,y2);\n\t\tNEIGHBOR(x2,y2,x3,y3);\n\t\tNOISE-PROB(x2,y3) = 0.080226846;\n\t\tNEIGHBOR(x2,y3,x1,y2);\n\t\tNEIGHBOR(x2,y3,x1,y3);\n\t\tNEIGHBOR(x2,y3,x1,y4);\n\t\tNEIGHBOR(x2,y3,x2,y2);\n\t\tNEIGHBOR(x2,y3,x2,y4);\n\t\tNEIGHBOR(x2,y3,x3,y2);\n\t\tNEIGHBOR(x2,y3,x3,y3);\n\t\tNEIGHBOR(x2,y3,x3,y4);\n\t\tNOISE-PROB(x2,y4) = 0.060447462;\n\t\tNEIGHBOR(x2,y4,x1,y3);\n\t\tNEIGHBOR(x2,y4,x1,y4);\n\t\tNEIGHBOR(x2,y4,x1,y5);\n\t\tNEIGHBOR(x2,y4,x2,y3);\n\t\tNEIGHBOR(x2,y4,x2,y5);\n\t\tNEIGHBOR(x2,y4,x3,y3);\n\t\tNEIGHBOR(x2,y4,x3,y4);\n\t\tNEIGHBOR(x2,y4,x3,y5);\n\t\tNOISE-PROB(x2,y5) = 0.06585156;\n\t\tNEIGHBOR(x2,y5,x1,y4);\n\t\tNEIGHBOR(x2,y5,x1,y5);\n\t\tNEIGHBOR(x2,y5,x2,y4);\n\t\tNEIGHBOR(x2,y5,x3,y4);\n\t\tNEIGHBOR(x2,y5,x3,y5);\n\t\tNOISE-PROB(x3,y1) = 0.054248307;\n\t\tNEIGHBOR(x3,y1,x2,y1);\n\t\tNEIGHBOR(x3,y1,x2,y2);\n\t\tNEIGHBOR(x3,y1,x3,y2);\n\t\tNEIGHBOR(x3,y1,x4,y1);\n\t\tNEIGHBOR(x3,y1,x4,y2);\n\t\tNOISE-PROB(x3,y2) = 0.0615227;\n\t\tNEIGHBOR(x3,y2,x2,y1);\n\t\tNEIGHBOR(x3,y2,x2,y2);\n\t\tNEIGHBOR(x3,y2,x2,y3);\n\t\tNEIGHBOR(x3,y2,x3,y1);\n\t\tNEIGHBOR(x3,y2,x3,y3);\n\t\tNEIGHBOR(x3,y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.08972506;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);", "mimetype": "text/plain", "start_char_idx": 1134, "end_char_idx": 2475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72b124c1-6962-42fc-b351-43e43cba60c0": {"__data__": {"id_": "72b124c1-6962-42fc-b351-43e43cba60c0", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e769163-1759-442c-a118-beb1c7a10663", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e8de213e45d53d7017e74e0cf06359af6c960fb18215bb313384f987cfdf361e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbf3f779-5b03-43fe-b409-44041797e372", "node_type": "1", "metadata": {}, "hash": "dfdfd5afdb7a60151ea99b76670591c385c9893e3e5c571c16a42eaa163a292d", "class_name": "RelatedNodeInfo"}}, "text": "y2,x4,y1);\n\t\tNEIGHBOR(x3,y2,x4,y2);\n\t\tNEIGHBOR(x3,y2,x4,y3);\n\t\tNOISE-PROB(x3,y3) = 0.08972506;\n\t\tNEIGHBOR(x3,y3,x2,y2);\n\t\tNEIGHBOR(x3,y3,x2,y3);\n\t\tNEIGHBOR(x3,y3,x2,y4);\n\t\tNEIGHBOR(x3,y3,x3,y2);\n\t\tNEIGHBOR(x3,y3,x3,y4);\n\t\tNEIGHBOR(x3,y3,x4,y2);\n\t\tNEIGHBOR(x3,y3,x4,y3);\n\t\tNEIGHBOR(x3,y3,x4,y4);\n\t\tNOISE-PROB(x3,y4) = 0.07582614;\n\t\tNEIGHBOR(x3,y4,x2,y3);\n\t\tNEIGHBOR(x3,y4,x2,y4);\n\t\tNEIGHBOR(x3,y4,x2,y5);\n\t\tNEIGHBOR(x3,y4,x3,y3);\n\t\tNEIGHBOR(x3,y4,x3,y5);\n\t\tNEIGHBOR(x3,y4,x4,y3);\n\t\tNEIGHBOR(x3,y4,x4,y4);\n\t\tNEIGHBOR(x3,y4,x4,y5);\n\t\tNOISE-PROB(x3,y5) = 0.07414019;\n\t\tNEIGHBOR(x3,y5,x2,y4);\n\t\tNEIGHBOR(x3,y5,x2,y5);\n\t\tNEIGHBOR(x3,y5,x3,y4);\n\t\tNEIGHBOR(x3,y5,x4,y4);\n\t\tNEIGHBOR(x3,y5,x4,y5);\n\t\tNOISE-PROB(x4,y1) = 0.08445734;\n\t\tNEIGHBOR(x4,y1,x3,y1);\n\t\tNEIGHBOR(x4,y1,x3,y2);\n\t\tNEIGHBOR(x4,y1,x4,y2);\n\t\tNEIGHBOR(x4,y1,x5,y1);\n\t\tNEIGHBOR(x4,y1,x5,y2);\n\t\tNOISE-PROB(x4,y2) = 0.051212516;\n\t\tNEIGHBOR(x4,y2,x3,y1);\n\t\tNEIGHBOR(x4,y2,x3,y2);\n\t\tNEIGHBOR(x4,y2,x3,y3);\n\t\tNEIGHBOR(x4,y2,x4,y1);\n\t\tNEIGHBOR(x4,y2,x4,y3);\n\t\tNEIGHBOR(x4,y2,x5,y1);\n\t\tNEIGHBOR(x4,y2,x5,y2);\n\t\tNEIGHBOR(x4,y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.08477443;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.075674266;", "mimetype": "text/plain", "start_char_idx": 2206, "end_char_idx": 3556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbf3f779-5b03-43fe-b409-44041797e372": {"__data__": {"id_": "dbf3f779-5b03-43fe-b409-44041797e372", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72b124c1-6962-42fc-b351-43e43cba60c0", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ff28b23f7132ca8c7e32eacc34b1450a6d5fb462471ed69579fb21d9367e37e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "236ab30c-c795-4e00-824a-eba599684cd9", "node_type": "1", "metadata": {}, "hash": "5b66e3648b90decb7e158be622adbe520f3ae43852e1b9e3afbc0207159536fa", "class_name": "RelatedNodeInfo"}}, "text": "y2,x5,y3);\n\t\tNOISE-PROB(x4,y3) = 0.08477443;\n\t\tNEIGHBOR(x4,y3,x3,y2);\n\t\tNEIGHBOR(x4,y3,x3,y3);\n\t\tNEIGHBOR(x4,y3,x3,y4);\n\t\tNEIGHBOR(x4,y3,x4,y2);\n\t\tNEIGHBOR(x4,y3,x4,y4);\n\t\tNEIGHBOR(x4,y3,x5,y2);\n\t\tNEIGHBOR(x4,y3,x5,y3);\n\t\tNEIGHBOR(x4,y3,x5,y4);\n\t\tNOISE-PROB(x4,y4) = 0.075674266;\n\t\tNEIGHBOR(x4,y4,x3,y3);\n\t\tNEIGHBOR(x4,y4,x3,y4);\n\t\tNEIGHBOR(x4,y4,x3,y5);\n\t\tNEIGHBOR(x4,y4,x4,y3);\n\t\tNEIGHBOR(x4,y4,x4,y5);\n\t\tNEIGHBOR(x4,y4,x5,y3);\n\t\tNEIGHBOR(x4,y4,x5,y4);\n\t\tNEIGHBOR(x4,y4,x5,y5);\n\t\tNOISE-PROB(x4,y5) = 0.08846702;\n\t\tNEIGHBOR(x4,y5,x3,y4);\n\t\tNEIGHBOR(x4,y5,x3,y5);\n\t\tNEIGHBOR(x4,y5,x4,y4);\n\t\tNEIGHBOR(x4,y5,x5,y4);\n\t\tNEIGHBOR(x4,y5,x5,y5);\n\t\tNOISE-PROB(x5,y1) = 0.060647354;\n\t\tNEIGHBOR(x5,y1,x4,y1);\n\t\tNEIGHBOR(x5,y1,x4,y2);\n\t\tNEIGHBOR(x5,y1,x5,y2);\n\t\tNOISE-PROB(x5,y2) = 0.08992037;\n\t\tNEIGHBOR(x5,y2,x4,y1);\n\t\tNEIGHBOR(x5,y2,x4,y2);\n\t\tNEIGHBOR(x5,y2,x4,y3);\n\t\tNEIGHBOR(x5,y2,x5,y1);\n\t\tNEIGHBOR(x5,y2,x5,y3);\n\t\tNOISE-PROB(x5,y3) = 0.086986154;\n\t\tNEIGHBOR(x5,y3,x4,y2);\n\t\tNEIGHBOR(x5,y3,x4,y3);\n\t\tNEIGHBOR(x5,y3,x4,y4);\n\t\tNEIGHBOR(x5,y3,x5,y2);\n\t\tNEIGHBOR(x5,y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.07164741;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.088661686;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};", "mimetype": "text/plain", "start_char_idx": 3277, "end_char_idx": 4617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "236ab30c-c795-4e00-824a-eba599684cd9": {"__data__": {"id_": "236ab30c-c795-4e00-824a-eba599684cd9", "embedding": null, "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ace246-d7c5-4b5d-b750-3e4795e2d6af", "node_type": "4", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32244181c51935b3605b94069da72dd317bce4e2cdb93dc87a9f9a04e88d436", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbf3f779-5b03-43fe-b409-44041797e372", "node_type": "1", "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e22f244388a98358a532d126988f0db2495a58b40140f8ad5b36ff86dc193e1a", "class_name": "RelatedNodeInfo"}}, "text": "y3,x5,y4);\n\t\tNOISE-PROB(x5,y4) = 0.07164741;\n\t\tNEIGHBOR(x5,y4,x4,y3);\n\t\tNEIGHBOR(x5,y4,x4,y4);\n\t\tNEIGHBOR(x5,y4,x4,y5);\n\t\tNEIGHBOR(x5,y4,x5,y3);\n\t\tNEIGHBOR(x5,y4,x5,y5);\n\t\tNOISE-PROB(x5,y5) = 0.088661686;\n\t\tNEIGHBOR(x5,y5,x4,y4);\n\t\tNEIGHBOR(x5,y5,x4,y5);\n\t\tNEIGHBOR(x5,y5,x5,y4);\n\t};\n}\n\ninstance game_of_life_inst_mdp__9 {\n\tdomain = game_of_life_mdp;\n\tnon-fluents = nf_game_of_life_inst_mdp__9;\n\tinit-state {\n\t\talive(x1,y1);\n\t\talive(x1,y2);\n\t\talive(x1,y5);\n\t\talive(x2,y4);\n\t\talive(x2,y5);\n\t\talive(x3,y2);\n\t\talive(x3,y3);\n\t\talive(x3,y4);\n\t\talive(x4,y1);\n\t\talive(x5,y1);\n\t\talive(x5,y3);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 4334, "end_char_idx": 4983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f67a25b4-6a7f-4eb7-bfac-b0fcfde581b4": {"__data__": {"id_": "f67a25b4-6a7f-4eb7-bfac-b0fcfde581b4", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18d71101-b08d-4f4b-b097-ec770f400f1b", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "1e0bdb713d09c6d6164efe01ac03f1291bc8ce681104a9cf8e51ecef7723ad6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5093d403-fa1b-4c55-a734-309cd7e9c39f", "node_type": "1", "metadata": {}, "hash": "24c45291e35107500d7f1ac02b97e9168603a555498440382ead80b3e89a63fc", "class_name": "RelatedNodeInfo"}}, "text": "domain navigation_mdp {\n\trequirements = {\n//\t\tconstrained-state,\n\t\treward-deterministic\n\t};\n\t\n\ttypes {\n\t\txpos : object;\n\t\typos : object;\n\t};\n\t\n\tpvariables {\n\n\t\tNORTH(ypos, ypos) : {non-fluent, bool, default = false};\n\t\tSOUTH(ypos, ypos) : {non-fluent, bool, default = false};\n\t\tEAST(xpos, xpos)  : {non-fluent, bool, default = false};\n\t\tWEST(xpos, xpos)  : {non-fluent, bool, default = false};\n\n\t\tMIN-XPOS(xpos) : {non-fluent, bool, default = false};\n\t\tMAX-XPOS(xpos) : {non-fluent, bool, default = false};\n\t\tMIN-YPOS(ypos) : {non-fluent, bool, default = false};\n\t\tMAX-YPOS(ypos) : {non-fluent, bool, default = false};\n\t\n\t\tP(xpos, ypos) : {non-fluent, real, default = 0.0};\n\t\t\n\t\tGOAL(xpos,ypos) : {non-fluent, bool, default = false};\n\t\t\n\t\t// Fluents\n\t\trobot-at(xpos, ypos) : {state-fluent, bool, default = false};\n\t\t\n\t\t// Actions\n\t\tmove-north : {action-fluent, bool, default = false};\n\t\tmove-south : {action-fluent, bool, default = false};\n\t\tmove-east  : {action-fluent, bool, default = false};\n\t\tmove-west  : {action-fluent, bool, default = false};\n\t};\n\t\n\tcpfs {\n\t\n\t\trobot-at'(?x,?y) =\n\t\t\n\t\t\tif ( GOAL(?x,?y) ^ robot-at(?x,?y)  )\n\t\t\tthen \n\t\t\t\tKronDelta(true)\n\t\t\telse if (( exists_{?x2 : xpos, ?y2 : ypos} [ GOAL(?x2,?y2) ^ robot-at(?x2,?y2)  ] )\n\t\t\t\t\t | ( move-north ^ exists_{?y2 : ypos} [ NORTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-east ^ exists_{?x2 : xpos} [ EAST(?x,?x2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-west ^ exists_{?x2 : xpos} [ WEST(?x,?x2) ^ robot-at(?x,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5093d403-fa1b-4c55-a734-309cd7e9c39f": {"__data__": {"id_": "5093d403-fa1b-4c55-a734-309cd7e9c39f", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18d71101-b08d-4f4b-b097-ec770f400f1b", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "1e0bdb713d09c6d6164efe01ac03f1291bc8ce681104a9cf8e51ecef7723ad6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f67a25b4-6a7f-4eb7-bfac-b0fcfde581b4", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "b2a1cbcda96a29c9c2f45d4b347f8a52a6ffb09f946bcb75355f0b4a8625d437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b74a45a1-a6d7-4c2e-a63d-ff362cca0d0e", "node_type": "1", "metadata": {}, "hash": "e834fbe99145a7ccdcb439e42128caa4016822c7ddd52383a3510674a2ea1fd5", "class_name": "RelatedNodeInfo"}}, "text": "?y) ^ robot-at(?x,?y)  )\n\t\t\tthen \n\t\t\t\tKronDelta(true)\n\t\t\telse if (( exists_{?x2 : xpos, ?y2 : ypos} [ GOAL(?x2,?y2) ^ robot-at(?x2,?y2)  ] )\n\t\t\t\t\t | ( move-north ^ exists_{?y2 : ypos} [ NORTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y,?y2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-east ^ exists_{?x2 : xpos} [ EAST(?x,?x2) ^ robot-at(?x,?y) ] )\n\t\t\t\t\t | ( move-west ^ exists_{?x2 : xpos} [ WEST(?x,?x2) ^ robot-at(?x,?y) ] ))\n\t\t\tthen \n\t\t\t\tKronDelta(false) \n\t\t\telse if (( move-north ^ exists_{?y2 : ypos} [ NORTH(?y2,?y) ^ robot-at(?x,?y2) ] )\n\t\t\t\t\t | ( move-south ^ exists_{?y2 : ypos} [ SOUTH(?y2,?y) ^ robot-at(?x,?y2) ] )\n\t\t\t\t\t | ( move-east ^ exists_{?x2 : xpos} [ EAST(?x2,?x) ^ robot-at(?x2,?y) ] )\n\t\t\t\t\t | ( move-west ^ exists_{?x2 : xpos} [ WEST(?x2,?x) ^ robot-at(?x2,?y) ] ))\n\t\t\tthen \n\t\t\t\tBernoulli( 1.0 - P(?x, ?y) ) \n\t\t\telse \n\t\t\t\tKronDelta( robot-at(?x,?y) );\n\t\t\t\t\n\t};\n\t\n\t// 0 reward for reaching goal, -1 in all other cases\n\treward = [sum_{?x : xpos, ?y : ypos} -(GOAL(?x,?y) ^ ~robot-at(?x,?y))]; \n\t\n//\tstate-action-constraints {\n//\t\n//\t\t// Robot at exactly one position\n//\t\t[sum_{?x : xpos, ?y : ypos} robot-at(?x,?y)] <= 1;\n//\t\t\n//\t\t// EAST, WEST, NORTH, SOUTH defined properly (unique and symmetric)\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} WEST(?x1,?x2)) <= 1];\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} EAST(?x1,?x2)) <= 1];\n//\t\tforall_{?y1 : ypos} [(sum_{?y2 : ypos} NORTH(?y1,?y2)) <= 1];", "mimetype": "text/plain", "start_char_idx": 1106, "end_char_idx": 2554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b74a45a1-a6d7-4c2e-a63d-ff362cca0d0e": {"__data__": {"id_": "b74a45a1-a6d7-4c2e-a63d-ff362cca0d0e", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18d71101-b08d-4f4b-b097-ec770f400f1b", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "1e0bdb713d09c6d6164efe01ac03f1291bc8ce681104a9cf8e51ecef7723ad6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5093d403-fa1b-4c55-a734-309cd7e9c39f", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "df379729b860d0e1beea6cf2816d015b55956caaa61ec2d6f7a6b4f7bf41ea62", "class_name": "RelatedNodeInfo"}}, "text": "?y : ypos} -(GOAL(?x,?y) ^ ~robot-at(?x,?y))]; \n\t\n//\tstate-action-constraints {\n//\t\n//\t\t// Robot at exactly one position\n//\t\t[sum_{?x : xpos, ?y : ypos} robot-at(?x,?y)] <= 1;\n//\t\t\n//\t\t// EAST, WEST, NORTH, SOUTH defined properly (unique and symmetric)\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} WEST(?x1,?x2)) <= 1];\n//\t\tforall_{?x1 : xpos} [(sum_{?x2 : xpos} EAST(?x1,?x2)) <= 1];\n//\t\tforall_{?y1 : ypos} [(sum_{?y2 : ypos} NORTH(?y1,?y2)) <= 1];\n//\t\tforall_{?y1 : ypos} [(sum_{?y2 : ypos} SOUTH(?y1,?y2)) <= 1];\n//\t\tforall_{?x1 : xpos, ?x2 : xpos} [ EAST(?x1,?x2) <=> WEST(?x2,?x1) ];\n//\t\tforall_{?y1 : ypos, ?y2 : ypos} [ SOUTH(?y1,?y2) <=> NORTH(?y2,?y1) ];\n//\n//\t\t// Definition verification\n//\t\t[ sum_{?x : xpos} MIN-XPOS(?x) ] == 1;\n//\t\t[ sum_{?x : xpos} MAX-XPOS(?x) ] == 1;\n//\t\t[ sum_{?y : ypos} MIN-YPOS(?y) ] == 1;\n//\t\t[ sum_{?y : ypos} MAX-YPOS(?y) ] == 1;\n//\t\t[ sum_{?x : xpos, ?y : ypos} GOAL(?x,?y) ] == 1;\n//\t\t\n//\t};\n\t\n}", "mimetype": "text/plain", "start_char_idx": 2106, "end_char_idx": 3042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e5a3fc0-dd9d-45fd-9b78-d3ab674b6fd1": {"__data__": {"id_": "1e5a3fc0-dd9d-45fd-9b78-d3ab674b6fd1", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a6ba5cc-7170-41ee-bea9-ea051ed6394a", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "ddf7906a430712c4faa74aff4ed3b7c9fa6f1a344184ccb7be6982d11fc45c59", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__1 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x6,x14,x21,x9};\n\t\typos : {y12,y20,y15};\n\t};\n\tnon-fluents {\n\t\tSOUTH(y15,y12);\n\t\tGOAL(x21,y20);\n\t\tWEST(x14,x9);\n\t\tNORTH(y12,y15);\n\t\tMAX-YPOS(y20);\n\t\tP(x9,y15) = 0.34543713989357155;\n\t\tSOUTH(y20,y15);\n\t\tMIN-YPOS(y12);\n\t\tEAST(x14,x21);\n\t\tEAST(x9,x14);\n\t\tMAX-XPOS(x21);\n\t\tWEST(x9,x6);\n\t\tP(x6,y15) = 0.04896671138703823;\n\t\tP(x21,y15) = 0.928158446525534;\n\t\tEAST(x6,x9);\n\t\tP(x14,y15) = 0.6369951789577802;\n\t\tWEST(x21,x14);\n\t\tNORTH(y15,y20);\n\t\tMIN-XPOS(x6);\n\t};\n}\n\ninstance navigation_inst_mdp__1 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__1;\n\tinit-state {\n\t\trobot-at(x21,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f19d0c72-290e-4331-b25c-b281a83c7358": {"__data__": {"id_": "f19d0c72-290e-4331-b25c-b281a83c7358", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d277908f-9e75-4db2-ac71-1610cd8017f5", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6", "node_type": "1", "metadata": {}, "hash": "9284d4e22352858e7abbc3c20795816236adb5251186b4053d061f206fef59da", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__10 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x86,x9,x201,x69,x329,x126,x261,x54,x230,x30,x6,x174,x149,x366,x21,x405,x14,x294,x41,x105};\n\t\typos : {y20,y36,y12,y27,y15};\n\t};\n\tnon-fluents {\n\t\tP(x14,y15) = 0.15249802260414552;\n\t\tSOUTH(y15,y12);\n\t\tWEST(x69,x54);\n\t\tP(x294,y27) = 0.7871350846988591;\n\t\tEAST(x54,x69);\n\t\tP(x261,y20) = 0.7218886631920836;\n\t\tP(x201,y20) = 0.6473250686142006;\n\t\tEAST(x21,x30);\n\t\tEAST(x41,x54);\n\t\tP(x69,y20) = 0.3765070266825588;\n\t\tMIN-YPOS(y12);\n\t\tNORTH(y12,y15);\n\t\tWEST(x41,x30);\n\t\tEAST(x6,x9);\n\t\tP(x201,y27) = 0.6546238641205587;\n\t\tWEST(x230,x201);\n\t\tEAST(x261,x294);\n\t\tP(x14,y27) = 0.1155198830621023;\n\t\tMIN-XPOS(x6);\n\t\tP(x30,y20) = 0.20109588794385722;\n\t\tP(x405,y27) = 0.9236670966799322;\n\t\tP(x21,y15) = 0.17413296334837614;\n\t\tNORTH(y20,y27);\n\t\tP(x30,y27) = 0.2127137481185951;\n\t\tP(x366,y20) = 0.9017004185405216;\n\t\tSOUTH(y27,y20);\n\t\tP(x41,y15) = 0.2896668278661213;\n\t\tP(x126,y15) = 0.5218342786752863;\n\t\tP(x105,y20) = 0.47504829154594946;\n\t\tP(x105,y27) = 0.46703882240935374;\n\t\tP(x329,y20) = 0.8276717586834964;\n\t\tSOUTH(y36,y27);\n\t\tEAST(x126,x149);\n\t\tP(x329,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6": {"__data__": {"id_": "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d277908f-9e75-4db2-ac71-1610cd8017f5", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f19d0c72-290e-4331-b25c-b281a83c7358", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "c58072be15551e3c119c9538c517ded98b34db3eb9c42bb6e269c2f194d7e24f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a", "node_type": "1", "metadata": {}, "hash": "17517d3bfc8d2827ce04f34284dda82b2d372cbe03d34f865284299cbc6d4b4d", "class_name": "RelatedNodeInfo"}}, "text": "y15) = 0.17413296334837614;\n\t\tNORTH(y20,y27);\n\t\tP(x30,y27) = 0.2127137481185951;\n\t\tP(x366,y20) = 0.9017004185405216;\n\t\tSOUTH(y27,y20);\n\t\tP(x41,y15) = 0.2896668278661213;\n\t\tP(x126,y15) = 0.5218342786752863;\n\t\tP(x105,y20) = 0.47504829154594946;\n\t\tP(x105,y27) = 0.46703882240935374;\n\t\tP(x329,y20) = 0.8276717586834964;\n\t\tSOUTH(y36,y27);\n\t\tEAST(x126,x149);\n\t\tP(x329,y15) = 0.8344797723387417;\n\t\tP(x261,y15) = 0.743265128057254;\n\t\tP(x230,y27) = 0.721857582365996;\n\t\tWEST(x366,x329);\n\t\tWEST(x86,x69);\n\t\tGOAL(x405,y36);\n\t\tP(x329,y27) = 0.8417070795242724;\n\t\tP(x174,y20) = 0.57908649514368;\n\t\tNORTH(y27,y36);\n\t\tP(x9,y20) = 0.06489074041478729;\n\t\tP(x30,y15) = 0.24802985405059239;\n\t\tWEST(x14,x9);\n\t\tP(x86,y15) = 0.4203020499921159;\n\t\tP(x126,y27) = 0.5217572396719141;\n\t\tP(x230,y15) = 0.6967564664388958;\n\t\tP(x405,y20) = 0.9195293152312699;\n\t\tMAX-XPOS(x405);\n\t\tWEST(x405,x366);\n\t\tEAST(x149,x174);\n\t\tWEST(x201,x174);\n\t\tWEST(x294,x261);\n\t\tP(x126,y20) = 0.502970067980258;\n\t\tEAST(x86,x105);\n\t\tWEST(x30,x21);\n\t\tP(x174,y27) = 0.6242552704520916;\n\t\tP(x6,y27) = 0.05393376015126705;\n\t\tEAST(x366,x405);\n\t\tP(x149,y20) = 0.5566534208820054;", "mimetype": "text/plain", "start_char_idx": 767, "end_char_idx": 1887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a": {"__data__": {"id_": "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d277908f-9e75-4db2-ac71-1610cd8017f5", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "0bcd596210269c7b747e3ad09b587947af1a484dcf14b24efcaa1e74b2a9477a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6361a14-10d0-48ce-9edf-28ba37f8d8df", "node_type": "1", "metadata": {}, "hash": "c8376b9b065d43a2de2c7d9abcef7267178bef4d5b45760fcfc8622dfd07f907", "class_name": "RelatedNodeInfo"}}, "text": "P(x230,y15) = 0.6967564664388958;\n\t\tP(x405,y20) = 0.9195293152312699;\n\t\tMAX-XPOS(x405);\n\t\tWEST(x405,x366);\n\t\tEAST(x149,x174);\n\t\tWEST(x201,x174);\n\t\tWEST(x294,x261);\n\t\tP(x126,y20) = 0.502970067980258;\n\t\tEAST(x86,x105);\n\t\tWEST(x30,x21);\n\t\tP(x174,y27) = 0.6242552704520916;\n\t\tP(x6,y27) = 0.05393376015126705;\n\t\tEAST(x366,x405);\n\t\tP(x149,y20) = 0.5566534208820054;\n\t\tP(x105,y15) = 0.45070689548983384;\n\t\tSOUTH(y20,y15);\n\t\tP(x9,y27) = 0.09803082372405028;\n\t\tP(x6,y20) = 0.05381382070481777;\n\t\tP(x21,y20) = 0.1901519668141478;\n\t\tP(x54,y27) = 0.30268661153355714;\n\t\tEAST(x294,x329);\n\t\tEAST(x69,x86);\n\t\tP(x149,y15) = 0.5480713560000846;\n\t\tNORTH(y15,y20);\n\t\tWEST(x174,x149);\n\t\tP(x261,y27) = 0.7239234615730024;\n\t\tEAST(x174,x201);\n\t\tP(x69,y27) = 0.3881094917458923;\n\t\tP(x405,y15) = 0.9321193240190807;\n\t\tWEST(x149,x126);\n\t\tP(x294,y15) = 0.7941499553424748;\n\t\tP(x6,y15) = 0.049380214884877205;\n\t\tP(x230,y20) = 0.6796873716245356;\n\t\tWEST(x261,x230);\n\t\tWEST(x105,x86);\n\t\tP(x366,y15) = 0.8813777306166134;\n\t\tP(x9,y15) = 0.09127656693913436;\n\t\tP(x366,y27) = 0.8738803072881541;\n\t\tP(x201,y15) = 0.6594119965049782;\n\t\tP(x149,y27) = 0.", "mimetype": "text/plain", "start_char_idx": 1528, "end_char_idx": 2644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6361a14-10d0-48ce-9edf-28ba37f8d8df": {"__data__": {"id_": "a6361a14-10d0-48ce-9edf-28ba37f8d8df", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d277908f-9e75-4db2-ac71-1610cd8017f5", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "767c3825073b258e94a4e974a9b8c876c373043035ebc489a3aea26305d0d522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28266af3-571a-4be1-a0c8-b109a69299f8", "node_type": "1", "metadata": {}, "hash": "a80223564e7ac0db911c491aa2d4bfb6e6c9ad6cd4cea9e64c409fedac7fdbaf", "class_name": "RelatedNodeInfo"}}, "text": "y15) = 0.9321193240190807;\n\t\tWEST(x149,x126);\n\t\tP(x294,y15) = 0.7941499553424748;\n\t\tP(x6,y15) = 0.049380214884877205;\n\t\tP(x230,y20) = 0.6796873716245356;\n\t\tWEST(x261,x230);\n\t\tWEST(x105,x86);\n\t\tP(x366,y15) = 0.8813777306166134;\n\t\tP(x9,y15) = 0.09127656693913436;\n\t\tP(x366,y27) = 0.8738803072881541;\n\t\tP(x201,y15) = 0.6594119965049782;\n\t\tP(x149,y27) = 0.5589389553979823;\n\t\tWEST(x21,x14);\n\t\tP(x54,y20) = 0.33979411550650473;\n\t\tP(x86,y27) = 0.40163065473500054;\n\t\tP(x14,y20) = 0.14583939402119112;\n\t\tP(x174,y15) = 0.6133839792915081;\n\t\tP(x54,y15) = 0.33987255149373885;\n\t\tP(x294,y20) = 0.7689352319146948;\n\t\tWEST(x329,x294);\n\t\tEAST(x201,x230);\n\t\tEAST(x230,x261);\n\t\tWEST(x54,x41);\n\t\tEAST(x30,x41);\n\t\tEAST(x9,x14);\n\t\tP(x21,y27) = 0.17168686323260007;\n\t\tEAST(x105,x126);\n\t\tMAX-YPOS(y36);\n\t\tP(x69,y15) = 0.3854513592821987;\n\t\tWEST(x126,x105);\n\t\tP(x41,y27) = 0.2793932707097969;\n\t\tEAST(x14,x21);\n\t\tEAST(x329,x366);\n\t\tP(x41,y20) = 0.25024425783684767;\n\t\tP(x86,y20) = 0.43350079449775974;\n\t\tWEST(x9,x6);\n\t};\n}\n\ninstance navigation_inst_mdp__10 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__10;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;", "mimetype": "text/plain", "start_char_idx": 2292, "end_char_idx": 3476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28266af3-571a-4be1-a0c8-b109a69299f8": {"__data__": {"id_": "28266af3-571a-4be1-a0c8-b109a69299f8", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d277908f-9e75-4db2-ac71-1610cd8017f5", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "baaab567ccdd466691c589135e7112ec5565829e31abd3014932e3735488d79b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6361a14-10d0-48ce-9edf-28ba37f8d8df", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "ca0b90ac7bc53c590de738b5dcbbe47bfb2ce157b57a9ef6a828165813e12ad7", "class_name": "RelatedNodeInfo"}}, "text": "MAX-YPOS(y36);\n\t\tP(x69,y15) = 0.3854513592821987;\n\t\tWEST(x126,x105);\n\t\tP(x41,y27) = 0.2793932707097969;\n\t\tEAST(x14,x21);\n\t\tEAST(x329,x366);\n\t\tP(x41,y20) = 0.25024425783684767;\n\t\tP(x86,y20) = 0.43350079449775974;\n\t\tWEST(x9,x6);\n\t};\n}\n\ninstance navigation_inst_mdp__10 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__10;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 3059, "end_char_idx": 3495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0f888e1-5c95-41bf-baca-deac6f6d5557": {"__data__": {"id_": "c0f888e1-5c95-41bf-baca-deac6f6d5557", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "154b557d-9b90-41c7-9626-4b0adc9e788b", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "95924f4a82b0c1d6d07d3819f91161b2a05ecbc5cd0a93a203aa54cd98b08822", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__2 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x14,x9,x21,x6,x30};\n\t\typos : {y15,y12,y20};\n\t};\n\tnon-fluents {\n\t\tSOUTH(y15,y12);\n\t\tMAX-XPOS(x30);\n\t\tEAST(x21,x30);\n\t\tP(x6,y15) = 0.0360226184129715;\n\t\tMAX-YPOS(y20);\n\t\tMIN-YPOS(y12);\n\t\tEAST(x9,x14);\n\t\tGOAL(x30,y20);\n\t\tWEST(x30,x21);\n\t\tP(x21,y15) = 0.6909389975480735;\n\t\tWEST(x9,x6);\n\t\tP(x30,y15) = 0.916325646918267;\n\t\tEAST(x6,x9);\n\t\tNORTH(y12,y15);\n\t\tWEST(x14,x9);\n\t\tWEST(x21,x14);\n\t\tP(x9,y15) = 0.23629253543913364;\n\t\tSOUTH(y20,y15);\n\t\tMIN-XPOS(x6);\n\t\tP(x14,y15) = 0.48970670998096466;\n\t\tNORTH(y15,y20);\n\t\tEAST(x14,x21);\n\t};\n}\n\ninstance navigation_inst_mdp__2 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__2;\n\tinit-state {\n\t\trobot-at(x30,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1188255-3d7d-4aba-a56d-47c677f7b037": {"__data__": {"id_": "d1188255-3d7d-4aba-a56d-47c677f7b037", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47553964-3793-41d2-a284-6165612d1a74", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "f56e75bfd665276405cc0f2450f37c59a46ace8cb5a642a73c2fac190c00ea73", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__3 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x14,x30,x9,x21,x6};\n\t\typos : {y20,y12,y27,y15};\n\t};\n\tnon-fluents {\n\t\tP(x9,y20) = 0.2450880827382207;\n\t\tSOUTH(y20,y15);\n\t\tWEST(x21,x14);\n\t\tP(x6,y20) = 0.05156800337135792;\n\t\tGOAL(x30,y27);\n\t\tP(x30,y15) = 0.9280268289148808;\n\t\tSOUTH(y27,y20);\n\t\tWEST(x14,x9);\n\t\tWEST(x9,x6);\n\t\tP(x21,y15) = 0.7021599113941193;\n\t\tMIN-XPOS(x6);\n\t\tEAST(x21,x30);\n\t\tSOUTH(y15,y12);\n\t\tP(x14,y15) = 0.5013892482966185;\n\t\tMIN-YPOS(y12);\n\t\tP(x30,y20) = 0.9452640172094107;\n\t\tP(x21,y20) = 0.6855187271139584;\n\t\tNORTH(y20,y27);\n\t\tP(x9,y15) = 0.250524521805346;\n\t\tEAST(x14,x21);\n\t\tNORTH(y15,y20);\n\t\tMAX-YPOS(y27);\n\t\tP(x14,y20) = 0.48334174789488316;\n\t\tEAST(x6,x9);\n\t\tNORTH(y12,y15);\n\t\tP(x6,y15) = 0.03749256581068039;\n\t\tEAST(x9,x14);\n\t\tMAX-XPOS(x30);\n\t\tWEST(x30,x21);\n\t};\n}\n\ninstance navigation_inst_mdp__3 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__3;\n\tinit-state {\n\t\trobot-at(x30,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7de2885f-c2f1-4538-9dd1-bf9ef5bb9f64": {"__data__": {"id_": "7de2885f-c2f1-4538-9dd1-bf9ef5bb9f64", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "695bb35a-8a71-4d1d-82da-2bc699f404ac", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "e100811cadd7f5183bed52089fb2154dbb9b16cd22dd3406bc88a8d509212d04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ade827da-f08c-4898-b1ba-8de933f3be08", "node_type": "1", "metadata": {}, "hash": "2ac3370b4b9688608898c8ea31e642e13039a6add37b47e507259033225fc863", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__4 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x21,x30,x9,x6,x14};\n\t\typos : {y47,y12,y36,y15,y27,y20};\n\t};\n\tnon-fluents {\n\t\tSOUTH(y20,y15);\n\t\tSOUTH(y36,y27);\n\t\tP(x14,y27) = 0.4755020113661885;\n\t\tP(x6,y20) = 0.024376023560762405;\n\t\tP(x30,y36) = 0.9497081767767668;\n\t\tMIN-YPOS(y12);\n\t\tP(x21,y15) = 0.7021266371011734;\n\t\tSOUTH(y27,y20);\n\t\tNORTH(y15,y20);\n\t\tMAX-YPOS(y47);\n\t\tSOUTH(y15,y12);\n\t\tNORTH(y20,y27);\n\t\tP(x30,y27) = 0.9107285801437683;\n\t\tMAX-XPOS(x30);\n\t\tP(x14,y36) = 0.4749935809522867;\n\t\tSOUTH(y47,y36);\n\t\tWEST(x9,x6);\n\t\tP(x30,y20) = 0.9197213770821691;\n\t\tP(x9,y15) = 0.23887041257694364;\n\t\tMIN-XPOS(x6);\n\t\tNORTH(y36,y47);\n\t\tNORTH(y12,y15);\n\t\tEAST(x9,x14);\n\t\tP(x14,y15) = 0.5034075286239386;\n\t\tWEST(x21,x14);\n\t\tP(x30,y15) = 0.9199540922418237;\n\t\tWEST(x30,x21);\n\t\tP(x6,y36) = 0.05538930557668209;\n\t\tEAST(x14,x21);\n\t\tP(x6,y27) = 0.044584812596440315;\n\t\tEAST(x6,x9);\n\t\tP(x21,y27) = 0.7054883688688278;\n\t\tP(x9,y20) = 0.27436082251369953;\n\t\tP(x21,y36) = 0.7212282549589872;\n\t\tGOAL(x30,y47);\n\t\tP(x21,y20) = 0.7279216069728136;\n\t\tP(x14,y20) = 0.49078163877129555;\n\t\tP(x6,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ade827da-f08c-4898-b1ba-8de933f3be08": {"__data__": {"id_": "ade827da-f08c-4898-b1ba-8de933f3be08", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "695bb35a-8a71-4d1d-82da-2bc699f404ac", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "e100811cadd7f5183bed52089fb2154dbb9b16cd22dd3406bc88a8d509212d04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7de2885f-c2f1-4538-9dd1-bf9ef5bb9f64", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "7131bcc71afef32d425994208a8ae505580ad5022a2903ae82f8bcc66d1fbc89", "class_name": "RelatedNodeInfo"}}, "text": "9199540922418237;\n\t\tWEST(x30,x21);\n\t\tP(x6,y36) = 0.05538930557668209;\n\t\tEAST(x14,x21);\n\t\tP(x6,y27) = 0.044584812596440315;\n\t\tEAST(x6,x9);\n\t\tP(x21,y27) = 0.7054883688688278;\n\t\tP(x9,y20) = 0.27436082251369953;\n\t\tP(x21,y36) = 0.7212282549589872;\n\t\tGOAL(x30,y47);\n\t\tP(x21,y20) = 0.7279216069728136;\n\t\tP(x14,y20) = 0.49078163877129555;\n\t\tP(x6,y15) = 0.013172781793400645;\n\t\tNORTH(y27,y36);\n\t\tP(x9,y27) = 0.24275896279141307;\n\t\tWEST(x14,x9);\n\t\tP(x9,y36) = 0.26499055325984955;\n\t\tEAST(x21,x30);\n\t};\n}\n\ninstance navigation_inst_mdp__4 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__4;\n\tinit-state {\n\t\trobot-at(x30,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 772, "end_char_idx": 1466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbfa6d89-e710-4a1a-bf56-43958cf0191e": {"__data__": {"id_": "fbfa6d89-e710-4a1a-bf56-43958cf0191e", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17fcb1f3-98fa-4246-9ddb-972116280b92", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "0f817ee69d87bf7800d3d885b4dad6f356c0f55e55734abc84d19a306e66fa4b", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__5 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x14,x54,x6,x86,x41,x9,x21,x30,x69,x105};\n\t\typos : {y12,y15,y20};\n\t};\n\tnon-fluents {\n\t\tMIN-YPOS(y12);\n\t\tSOUTH(y15,y12);\n\t\tP(x6,y15) = 0.024014816619455814;\n\t\tP(x30,y15) = 0.42676472498310936;\n\t\tEAST(x9,x14);\n\t\tWEST(x41,x30);\n\t\tWEST(x21,x14);\n\t\tEAST(x69,x86);\n\t\tP(x105,y15) = 0.9336750203122696;\n\t\tWEST(x14,x9);\n\t\tEAST(x21,x30);\n\t\tWEST(x69,x54);\n\t\tGOAL(x105,y20);\n\t\tMAX-XPOS(x105);\n\t\tP(x54,y15) = 0.6266834967666202;\n\t\tEAST(x54,x69);\n\t\tP(x86,y15) = 0.8539166100737121;\n\t\tEAST(x30,x41);\n\t\tP(x69,y15) = 0.7261174401889244;\n\t\tWEST(x30,x21);\n\t\tP(x9,y15) = 0.1536506913188431;\n\t\tMAX-YPOS(y20);\n\t\tWEST(x105,x86);\n\t\tNORTH(y15,y20);\n\t\tMIN-XPOS(x6);\n\t\tNORTH(y12,y15);\n\t\tWEST(x9,x6);\n\t\tWEST(x86,x69);\n\t\tEAST(x86,x105);\n\t\tWEST(x54,x41);\n\t\tP(x41,y15) = 0.525113389827311;\n\t\tP(x21,y15) = 0.34208946157660747;\n\t\tP(x14,y15) = 0.21357332878849572;\n\t\tEAST(x14,x21);\n\t\tEAST(x6,x9);\n\t\tEAST(x41,x54);\n\t\tSOUTH(y20,y15);\n\t};\n}\n\ninstance navigation_inst_mdp__5 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__5;\n\tinit-state {\n\t\trobot-at(x105,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d012bfa-95e0-4682-b9af-b5adf95fc1a6": {"__data__": {"id_": "4d012bfa-95e0-4682-b9af-b5adf95fc1a6", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "557e1cdd-dc48-4cff-912a-420b02eb869e", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "b93730dc9738e19a05b008e9988e5bad9ef0bcfb7b67d4a983d56f9b3f145030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fddfdf19-0c97-4792-b0b7-cf6a784a4c88", "node_type": "1", "metadata": {}, "hash": "7c0c3f4041ccb9e095b0a29a7e9e5adba96899919b767f723610ccb7db20379d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__6 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x54,x9,x86,x30,x105,x14,x69,x6,x41,x21};\n\t\typos : {y12,y15,y27,y20};\n\t};\n\tnon-fluents {\n\t\tP(x6,y20) = 0.032087743282318115;\n\t\tP(x69,y20) = 0.7137660816467056;\n\t\tP(x41,y15) = 0.5501891952008009;\n\t\tP(x41,y20) = 0.5153869516216218;\n\t\tEAST(x30,x41);\n\t\tEAST(x21,x30);\n\t\tMAX-XPOS(x105);\n\t\tWEST(x86,x69);\n\t\tWEST(x30,x21);\n\t\tWEST(x14,x9);\n\t\tP(x86,y20) = 0.8570238709863689;\n\t\tNORTH(y20,y27);\n\t\tP(x69,y15) = 0.7465580261001984;\n\t\tSOUTH(y27,y20);\n\t\tNORTH(y15,y20);\n\t\tP(x6,y15) = 0.03272361308336258;\n\t\tP(x30,y20) = 0.43298558166457546;\n\t\tP(x30,y15) = 0.4296299742741717;\n\t\tP(x105,y20) = 0.9116935366376614;\n\t\tWEST(x54,x41);\n\t\tWEST(x69,x54);\n\t\tSOUTH(y20,y15);\n\t\tWEST(x21,x14);\n\t\tEAST(x14,x21);\n\t\tWEST(x9,x6);\n\t\tMAX-YPOS(y27);\n\t\tEAST(x86,x105);\n\t\tP(x86,y15) = 0.8315923180845048;\n\t\tSOUTH(y15,y12);\n\t\tMIN-XPOS(x6);\n\t\tEAST(x6,x9);\n\t\tEAST(x41,x54);\n\t\tP(x105,y15) = 0.9274347008516391;\n\t\tEAST(x9,x14);\n\t\tGOAL(x105,y27);\n\t\tNORTH(y12,y15);\n\t\tP(x54,y20) = 0.6428951051914029;\n\t\tP(x14,y15) = 0.21247654371998376;\n\t\tP(x21,y15) = 0.33662864607241416;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fddfdf19-0c97-4792-b0b7-cf6a784a4c88": {"__data__": {"id_": "fddfdf19-0c97-4792-b0b7-cf6a784a4c88", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "557e1cdd-dc48-4cff-912a-420b02eb869e", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "b93730dc9738e19a05b008e9988e5bad9ef0bcfb7b67d4a983d56f9b3f145030", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d012bfa-95e0-4682-b9af-b5adf95fc1a6", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "eda62da4c5ba8dbd30051d205ba6adae9a10e2222c6b473d2490940f646c1a2b", "class_name": "RelatedNodeInfo"}}, "text": "WEST(x9,x6);\n\t\tMAX-YPOS(y27);\n\t\tEAST(x86,x105);\n\t\tP(x86,y15) = 0.8315923180845048;\n\t\tSOUTH(y15,y12);\n\t\tMIN-XPOS(x6);\n\t\tEAST(x6,x9);\n\t\tEAST(x41,x54);\n\t\tP(x105,y15) = 0.9274347008516391;\n\t\tEAST(x9,x14);\n\t\tGOAL(x105,y27);\n\t\tNORTH(y12,y15);\n\t\tP(x54,y20) = 0.6428951051914029;\n\t\tP(x14,y15) = 0.21247654371998376;\n\t\tP(x21,y15) = 0.33662864607241416;\n\t\tP(x14,y20) = 0.21680060737869805;\n\t\tMIN-YPOS(y12);\n\t\tEAST(x69,x86);\n\t\tP(x9,y20) = 0.15484551112684938;\n\t\tWEST(x105,x86);\n\t\tP(x54,y15) = 0.659609689273768;\n\t\tP(x9,y15) = 0.12321555666211578;\n\t\tP(x21,y20) = 0.3461974025186565;\n\t\tEAST(x54,x69);\n\t\tWEST(x41,x30);\n\t};\n}\n\ninstance navigation_inst_mdp__6 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__6;\n\tinit-state {\n\t\trobot-at(x105,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 772, "end_char_idx": 1584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "521d154e-458a-4840-a373-cd6ac9839c2c": {"__data__": {"id_": "521d154e-458a-4840-a373-cd6ac9839c2c", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "7bb0cc0a2e51dd466e92dfb9e5d3155d5eb59a045a34db30afde95177b0136db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e", "node_type": "1", "metadata": {}, "hash": "2293b34a8cb170d707a8716ccbddfe1a8ba696bb955e35570daad2af7f582fff", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__7 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x9,x21,x54,x69,x30,x6,x14,x41,x86,x105};\n\t\typos : {y36,y27,y15,y12,y20};\n\t};\n\tnon-fluents {\n\t\tP(x41,y15) = 0.5383095685392618;\n\t\tP(x54,y15) = 0.6193011131965451;\n\t\tEAST(x30,x41);\n\t\tSOUTH(y36,y27);\n\t\tGOAL(x105,y36);\n\t\tSOUTH(y20,y15);\n\t\tP(x30,y20) = 0.44101769311560524;\n\t\tP(x6,y15) = 0.023994946852326393;\n\t\tP(x14,y20) = 0.21105071109357393;\n\t\tWEST(x30,x21);\n\t\tEAST(x21,x30);\n\t\tNORTH(y27,y36);\n\t\tP(x54,y20) = 0.6595114645444684;\n\t\tWEST(x69,x54);\n\t\tSOUTH(y15,y12);\n\t\tMIN-XPOS(x6);\n\t\tMAX-XPOS(x105);\n\t\tP(x21,y20) = 0.33060312312510276;\n\t\tWEST(x21,x14);\n\t\tEAST(x9,x14);\n\t\tP(x86,y20) = 0.8134994268831279;\n\t\tNORTH(y15,y20);\n\t\tP(x41,y20) = 0.5119783256668597;\n\t\tWEST(x105,x86);\n\t\tP(x30,y27) = 0.4580075146837367;\n\t\tWEST(x41,x30);\n\t\tP(x6,y20) = 0.021341380663216114;\n\t\tP(x54,y27) = 0.642808957853251;\n\t\tP(x69,y20) = 0.7245696174601713;\n\t\tEAST(x86,x105);\n\t\tP(x14,y15) = 0.22739516860908932;\n\t\tP(x9,y15) = 0.15196049358281824;\n\t\tP(x86,y15) = 0.859581144940522;\n\t\tWEST(x9,x6);\n\t\tEAST(x41,x54);\n\t\tP(x69,y15) = 0.7486556774626175;\n\t\tP(x105,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e": {"__data__": {"id_": "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "7bb0cc0a2e51dd466e92dfb9e5d3155d5eb59a045a34db30afde95177b0136db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "521d154e-458a-4840-a373-cd6ac9839c2c", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "f20e8b158f51fe94cd00e04ff0f2489a39bb2655eef16be22626c74139b584dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aed0be1a-b71d-4406-9e50-5986d5e5b682", "node_type": "1", "metadata": {}, "hash": "1cf90ac3f3dcf8ed687561997a3e906855506d39df3c464c65d668e17e310e6a", "class_name": "RelatedNodeInfo"}}, "text": "y27) = 0.4580075146837367;\n\t\tWEST(x41,x30);\n\t\tP(x6,y20) = 0.021341380663216114;\n\t\tP(x54,y27) = 0.642808957853251;\n\t\tP(x69,y20) = 0.7245696174601713;\n\t\tEAST(x86,x105);\n\t\tP(x14,y15) = 0.22739516860908932;\n\t\tP(x9,y15) = 0.15196049358281824;\n\t\tP(x86,y15) = 0.859581144940522;\n\t\tWEST(x9,x6);\n\t\tEAST(x41,x54);\n\t\tP(x69,y15) = 0.7486556774626175;\n\t\tP(x105,y20) = 0.9399694142242273;\n\t\tWEST(x54,x41);\n\t\tMAX-YPOS(y36);\n\t\tP(x21,y15) = 0.31251017162058914;\n\t\tP(x30,y15) = 0.43196028677953613;\n\t\tP(x14,y27) = 0.24023324913448757;\n\t\tP(x6,y27) = 0.011126482859253883;\n\t\tEAST(x54,x69);\n\t\tEAST(x14,x21);\n\t\tP(x105,y27) = 0.9386047304918369;\n\t\tP(x69,y27) = 0.7594633890936772;\n\t\tP(x9,y20) = 0.1492169178608391;\n\t\tSOUTH(y27,y20);\n\t\tEAST(x6,x9);\n\t\tP(x86,y27) = 0.833064128127363;\n\t\tEAST(x69,x86);\n\t\tP(x41,y27) = 0.5182037660852075;\n\t\tP(x21,y27) = 0.31010614638135947;\n\t\tWEST(x14,x9);\n\t\tWEST(x86,x69);\n\t\tNORTH(y20,y27);\n\t\tNORTH(y12,y15);\n\t\tP(x9,y27) = 0.14091320667001936;\n\t\tP(x105,y15) = 0.926005428036054;\n\t\tMIN-YPOS(y12);\n\t};\n}\n\ninstance navigation_inst_mdp__7 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__7;\n\tinit-state {\n\t\trobot-at(x105,y12);\n\t};", "mimetype": "text/plain", "start_char_idx": 767, "end_char_idx": 1918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aed0be1a-b71d-4406-9e50-5986d5e5b682": {"__data__": {"id_": "aed0be1a-b71d-4406-9e50-5986d5e5b682", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63b8f18b-e3f6-45a7-8cbb-f46400494271", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "7bb0cc0a2e51dd466e92dfb9e5d3155d5eb59a045a34db30afde95177b0136db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "1375f305986bf24d63e2096cb262455420ae9beb3dff768475e6769a32379abf", "class_name": "RelatedNodeInfo"}}, "text": "833064128127363;\n\t\tEAST(x69,x86);\n\t\tP(x41,y27) = 0.5182037660852075;\n\t\tP(x21,y27) = 0.31010614638135947;\n\t\tWEST(x14,x9);\n\t\tWEST(x86,x69);\n\t\tNORTH(y20,y27);\n\t\tNORTH(y12,y15);\n\t\tP(x9,y27) = 0.14091320667001936;\n\t\tP(x105,y15) = 0.926005428036054;\n\t\tMIN-YPOS(y12);\n\t};\n}\n\ninstance navigation_inst_mdp__7 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__7;\n\tinit-state {\n\t\trobot-at(x105,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1509, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d173dbb9-852b-4462-9366-adb9df3ae483": {"__data__": {"id_": "d173dbb9-852b-4462-9366-adb9df3ae483", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "dd09aa4b32d81d4e4a31a153d7664e4f812160b0856d1a1c2c0e621eaf26c4e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee", "node_type": "1", "metadata": {}, "hash": "37a8d36eaa081e6b1b62be83bbbd33ba5266bb9d34c42c453226eb46633dcd38", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__8 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x261,x149,x21,x69,x9,x174,x41,x86,x105,x329,x14,x366,x30,x405,x201,x294,x126,x6,x230,x54};\n\t\typos : {y12,y20,y15};\n\t};\n\tnon-fluents {\n\t\tWEST(x14,x9);\n\t\tP(x30,y15) = 0.2347686960312881;\n\t\tEAST(x30,x41);\n\t\tWEST(x149,x126);\n\t\tWEST(x41,x30);\n\t\tEAST(x69,x86);\n\t\tGOAL(x405,y20);\n\t\tEAST(x14,x21);\n\t\tSOUTH(y15,y12);\n\t\tMAX-YPOS(y20);\n\t\tP(x126,y15) = 0.522427676441638;\n\t\tMIN-XPOS(x6);\n\t\tEAST(x9,x14);\n\t\tEAST(x21,x30);\n\t\tWEST(x366,x329);\n\t\tEAST(x329,x366);\n\t\tP(x69,y15) = 0.3703241362971695;\n\t\tEAST(x261,x294);\n\t\tP(x86,y15) = 0.4001688238135294;\n\t\tWEST(x230,x201);\n\t\tP(x329,y15) = 0.8186304341928151;\n\t\tWEST(x30,x21);\n\t\tNORTH(y15,y20);\n\t\tP(x201,y15) = 0.6305554033208051;\n\t\tP(x149,y15) = 0.5512959579692075;\n\t\tP(x105,y15) = 0.47063784213050414;\n\t\tP(x14,y15) = 0.12598508870915365;\n\t\tWEST(x54,x41);\n\t\tEAST(x174,x201);\n\t\tP(x21,y15) = 0.18618261039649187;\n\t\tEAST(x201,x230);\n\t\tMAX-XPOS(x405);\n\t\tWEST(x174,x149);\n\t\tEAST(x294,x329);\n\t\tP(x6,y15) = 0.020123825408518314;\n\t\tP(x41,y15) = 0.2575469744440756;\n\t\tP(x261,y15) = 0.7651083509584791;\n\t\tEAST(x6,x9);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee": {"__data__": {"id_": "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "dd09aa4b32d81d4e4a31a153d7664e4f812160b0856d1a1c2c0e621eaf26c4e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d173dbb9-852b-4462-9366-adb9df3ae483", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "a86e34c6b72720ac30d3e02a964039a5b81c7fb2d772ac87e8cad8d46c096cc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6109f391-6fe8-4425-b959-e2698431710d", "node_type": "1", "metadata": {}, "hash": "07d2f2d1281d6c02800cd70e61321e104ca2a422b8f55a77b09ea503853ceba7", "class_name": "RelatedNodeInfo"}}, "text": "5512959579692075;\n\t\tP(x105,y15) = 0.47063784213050414;\n\t\tP(x14,y15) = 0.12598508870915365;\n\t\tWEST(x54,x41);\n\t\tEAST(x174,x201);\n\t\tP(x21,y15) = 0.18618261039649187;\n\t\tEAST(x201,x230);\n\t\tMAX-XPOS(x405);\n\t\tWEST(x174,x149);\n\t\tEAST(x294,x329);\n\t\tP(x6,y15) = 0.020123825408518314;\n\t\tP(x41,y15) = 0.2575469744440756;\n\t\tP(x261,y15) = 0.7651083509584791;\n\t\tEAST(x6,x9);\n\t\tWEST(x126,x105);\n\t\tEAST(x230,x261);\n\t\tP(x294,y15) = 0.7724588914578291;\n\t\tEAST(x86,x105);\n\t\tWEST(x294,x261);\n\t\tWEST(x69,x54);\n\t\tP(x366,y15) = 0.8710702612113795;\n\t\tEAST(x41,x54);\n\t\tEAST(x149,x174);\n\t\tEAST(x126,x149);\n\t\tEAST(x54,x69);\n\t\tWEST(x201,x174);\n\t\tP(x9,y15) = 0.08958914081909156;\n\t\tWEST(x21,x14);\n\t\tWEST(x105,x86);\n\t\tWEST(x329,x294);\n\t\tSOUTH(y20,y15);\n\t\tWEST(x261,x230);\n\t\tP(x230,y15) = 0.6802079464848104;\n\t\tP(x54,y15) = 0.3001739060212123;\n\t\tP(x174,y15) = 0.5920608441688513;\n\t\tWEST(x86,x69);\n\t\tNORTH(y12,y15);\n\t\tEAST(x366,x405);\n\t\tWEST(x9,x6);\n\t\tWEST(x405,x366);\n\t\tP(x405,y15) = 0.94463482979489;\n\t\tMIN-YPOS(y12);\n\t\tEAST(x105,x126);\n\t};\n}\n\ninstance navigation_inst_mdp__8 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__8;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;", "mimetype": "text/plain", "start_char_idx": 767, "end_char_idx": 1961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6109f391-6fe8-4425-b959-e2698431710d": {"__data__": {"id_": "6109f391-6fe8-4425-b959-e2698431710d", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "813c12ae-09fd-41fa-86ab-0cfe7f758fa0", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "dd09aa4b32d81d4e4a31a153d7664e4f812160b0856d1a1c2c0e621eaf26c4e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "6899b90eacf362b7f30288d8477443015d31a183c219058f3532e89d2111f38f", "class_name": "RelatedNodeInfo"}}, "text": "P(x54,y15) = 0.3001739060212123;\n\t\tP(x174,y15) = 0.5920608441688513;\n\t\tWEST(x86,x69);\n\t\tNORTH(y12,y15);\n\t\tEAST(x366,x405);\n\t\tWEST(x9,x6);\n\t\tWEST(x405,x366);\n\t\tP(x405,y15) = 0.94463482979489;\n\t\tMIN-YPOS(y12);\n\t\tEAST(x105,x126);\n\t};\n}\n\ninstance navigation_inst_mdp__8 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__8;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1546, "end_char_idx": 1980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4701020-2d1c-42bb-a342-abd2aae3b734": {"__data__": {"id_": "d4701020-2d1c-42bb-a342-abd2aae3b734", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8342d416-b313-49ab-9337-880afb976b83", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "532acafbd0685fd01f62e2793748300a87e2a07e0f8b1e8944101bf5a57ca65b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12", "node_type": "1", "metadata": {}, "hash": "fb71a495bc301a248e35120e6346827ef44f2cc3061c244f73f8f91c5fdc30f3", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_navigation_inst_mdp__9 {\n\tdomain = navigation_mdp;\n\tobjects {\n\t\txpos : {x41,x174,x30,x294,x366,x54,x230,x105,x329,x14,x21,x126,x201,x149,x9,x6,x261,x69,x405,x86};\n\t\typos : {y15,y20,y12,y27};\n\t};\n\tnon-fluents {\n\t\tP(x30,y20) = 0.22066297931106468;\n\t\tWEST(x261,x230);\n\t\tWEST(x21,x14);\n\t\tP(x230,y15) = 0.6826771881039205;\n\t\tWEST(x366,x329);\n\t\tSOUTH(y27,y20);\n\t\tP(x405,y15) = 0.9465039047951761;\n\t\tP(x405,y20) = 0.9567379925007883;\n\t\tMAX-YPOS(y27);\n\t\tNORTH(y12,y15);\n\t\tSOUTH(y20,y15);\n\t\tP(x329,y20) = 0.845591881165379;\n\t\tWEST(x86,x69);\n\t\tEAST(x6,x9);\n\t\tP(x126,y20) = 0.5255273675644083;\n\t\tEAST(x41,x54);\n\t\tP(x366,y15) = 0.9121467062321148;\n\t\tP(x86,y15) = 0.39646985175970356;\n\t\tP(x9,y20) = 0.07717571731068587;\n\t\tEAST(x30,x41);\n\t\tP(x174,y20) = 0.5823425541043674;\n\t\tEAST(x86,x105);\n\t\tP(x261,y20) = 0.7237653274236149;\n\t\tEAST(x149,x174);\n\t\tP(x6,y15) = 0.03873947076499462;\n\t\tWEST(x201,x174);\n\t\tP(x174,y15) = 0.6239512122579312;\n\t\tP(x201,y20) = 0.6425731274250307;\n\t\tP(x41,y15) = 0.2852499784019432;\n\t\tMAX-XPOS(x405);\n\t\tP(x329,y15) = 0.8237989943866667;\n\t\tEAST(x329,x366);\n\t\tWEST(x230,x201);\n\t\tWEST(x149,x126);\n\t\tP(x41,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12": {"__data__": {"id_": "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8342d416-b313-49ab-9337-880afb976b83", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "532acafbd0685fd01f62e2793748300a87e2a07e0f8b1e8944101bf5a57ca65b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4701020-2d1c-42bb-a342-abd2aae3b734", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "d13a958ca93343356aad5d9cefe1952bb76dbdb8270f7e9037c5a10a2b5676ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4810f6a0-da22-439f-a356-b8a8c9499178", "node_type": "1", "metadata": {}, "hash": "ab74d3283c88aa63f608bbf533cb1d41f6d15a9d7292aaba836e2a07babdca9f", "class_name": "RelatedNodeInfo"}}, "text": "EAST(x86,x105);\n\t\tP(x261,y20) = 0.7237653274236149;\n\t\tEAST(x149,x174);\n\t\tP(x6,y15) = 0.03873947076499462;\n\t\tWEST(x201,x174);\n\t\tP(x174,y15) = 0.6239512122579312;\n\t\tP(x201,y20) = 0.6425731274250307;\n\t\tP(x41,y15) = 0.2852499784019432;\n\t\tMAX-XPOS(x405);\n\t\tP(x329,y15) = 0.8237989943866667;\n\t\tEAST(x329,x366);\n\t\tWEST(x230,x201);\n\t\tWEST(x149,x126);\n\t\tP(x41,y20) = 0.24861652041344265;\n\t\tP(x149,y20) = 0.552126179890413;\n\t\tMIN-XPOS(x6);\n\t\tWEST(x54,x41);\n\t\tEAST(x126,x149);\n\t\tP(x149,y15) = 0.5461224238143155;\n\t\tP(x14,y15) = 0.10511640175000618;\n\t\tEAST(x261,x294);\n\t\tWEST(x174,x149);\n\t\tP(x261,y15) = 0.7466745006998903;\n\t\tWEST(x14,x9);\n\t\tP(x86,y20) = 0.4120094569301919;\n\t\tMIN-YPOS(y12);\n\t\tP(x105,y15) = 0.44468523473723937;\n\t\tSOUTH(y15,y12);\n\t\tP(x366,y20) = 0.8901059870657168;\n\t\tWEST(x294,x261);\n\t\tEAST(x54,x69);\n\t\tWEST(x126,x105);\n\t\tEAST(x201,x230);\n\t\tP(x201,y15) = 0.668071996322588;\n\t\tEAST(x69,x86);\n\t\tP(x14,y20) = 0.11832347693607996;\n\t\tWEST(x9,x6);\n\t\tP(x21,y15) = 0.1869948428908461;\n\t\tEAST(x230,x261);\n\t\tEAST(x174,x201);\n\t\tP(x69,y15) = 0.3620691891563566;\n\t\tGOAL(x405,y27);\n\t\tP(x30,y15) = 0.20788565547646662;\n\t\tEAST(x294,", "mimetype": "text/plain", "start_char_idx": 777, "end_char_idx": 1899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4810f6a0-da22-439f-a356-b8a8c9499178": {"__data__": {"id_": "4810f6a0-da22-439f-a356-b8a8c9499178", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8342d416-b313-49ab-9337-880afb976b83", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "532acafbd0685fd01f62e2793748300a87e2a07e0f8b1e8944101bf5a57ca65b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "433b761b77b36d9c217f574b20bf07500fcdce0eccab6fc350557654bed1c794", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8706dd23-37b9-44c7-b557-1a014ac1a5dd", "node_type": "1", "metadata": {}, "hash": "ebe4c34236dc8fdc118534cc3bb2a2b61e57141246137a1b6341f827ed0282b5", "class_name": "RelatedNodeInfo"}}, "text": "WEST(x294,x261);\n\t\tEAST(x54,x69);\n\t\tWEST(x126,x105);\n\t\tEAST(x201,x230);\n\t\tP(x201,y15) = 0.668071996322588;\n\t\tEAST(x69,x86);\n\t\tP(x14,y20) = 0.11832347693607996;\n\t\tWEST(x9,x6);\n\t\tP(x21,y15) = 0.1869948428908461;\n\t\tEAST(x230,x261);\n\t\tEAST(x174,x201);\n\t\tP(x69,y15) = 0.3620691891563566;\n\t\tGOAL(x405,y27);\n\t\tP(x30,y15) = 0.20788565547646662;\n\t\tEAST(x294,x329);\n\t\tEAST(x105,x126);\n\t\tP(x126,y15) = 0.5183540826761408;\n\t\tP(x54,y15) = 0.30929778271207686;\n\t\tP(x21,y20) = 0.15904580260087786;\n\t\tP(x294,y15) = 0.793770940582219;\n\t\tWEST(x30,x21);\n\t\tEAST(x14,x21);\n\t\tP(x9,y15) = 0.0694843544379661;\n\t\tNORTH(y20,y27);\n\t\tP(x54,y20) = 0.29557425737699594;\n\t\tP(x230,y20) = 0.6812154801170293;\n\t\tWEST(x105,x86);\n\t\tWEST(x329,x294);\n\t\tWEST(x405,x366);\n\t\tP(x105,y20) = 0.48526905761345435;\n\t\tNORTH(y15,y20);\n\t\tWEST(x69,x54);\n\t\tEAST(x366,x405);\n\t\tP(x6,y20) = 0.05842727981507778;\n\t\tP(x294,y20) = 0.8174746013981732;\n\t\tP(x69,y20) = 0.3763092361194523;\n\t\tWEST(x41,x30);\n\t\tEAST(x21,x30);\n\t\tEAST(x9,x14);\n\t};\n}\n\ninstance navigation_inst_mdp__9 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__9;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;", "mimetype": "text/plain", "start_char_idx": 1550, "end_char_idx": 2717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8706dd23-37b9-44c7-b557-1a014ac1a5dd": {"__data__": {"id_": "8706dd23-37b9-44c7-b557-1a014ac1a5dd", "embedding": null, "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8342d416-b313-49ab-9337-880afb976b83", "node_type": "4", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "532acafbd0685fd01f62e2793748300a87e2a07e0f8b1e8944101bf5a57ca65b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4810f6a0-da22-439f-a356-b8a8c9499178", "node_type": "1", "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}, "hash": "8d096aa7146e971ea99782775a23acc18ceb814123aaa63d09079db591c9a01b", "class_name": "RelatedNodeInfo"}}, "text": "48526905761345435;\n\t\tNORTH(y15,y20);\n\t\tWEST(x69,x54);\n\t\tEAST(x366,x405);\n\t\tP(x6,y20) = 0.05842727981507778;\n\t\tP(x294,y20) = 0.8174746013981732;\n\t\tP(x69,y20) = 0.3763092361194523;\n\t\tWEST(x41,x30);\n\t\tEAST(x21,x30);\n\t\tEAST(x9,x14);\n\t};\n}\n\ninstance navigation_inst_mdp__9 {\n\tdomain = navigation_mdp;\n\tnon-fluents = nf_navigation_inst_mdp__9;\n\tinit-state {\n\t\trobot-at(x405,y12);\n\t};\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2300, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c5946b-a3a0-4fc7-8eeb-b4dc83b2c934": {"__data__": {"id_": "e6c5946b-a3a0-4fc7-8eeb-b4dc83b2c934", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af595f59-96f0-4ad5-b2fe-595af188c282", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "0b2db8e4a234a0d89a08035ca4b24488b9139aeb76dfa78f5266db1ebba773e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa", "node_type": "1", "metadata": {}, "hash": "3a88f2a99f77c37366b21e957ae01dd6506e5ff80094ebf59c598edbb59e7de9", "class_name": "RelatedNodeInfo"}}, "text": "domain skill_teaching_mdp {\n  \t\n\trequirements = { \n\t\treward-deterministic \n\t};\n\n\ttypes { \n\t\tskill : object;\n\t};\n      \t\n\tpvariables { \n\t\t\n\t\t//how valuable is this skill?\t\t\n\t\tSKILL_WEIGHT(skill) : { non-fluent, real, default = 1.0 };\n\t\t\n\t\t//some skills are pre-reqs for others.  Your ability to achiev a higher level skill is dependent on how \n\t\t//many of the pre-reqs you have mastered\n\t\tPRE_REQ(skill, skill) : { non-fluent, bool, default = false };\n\n\t\t//probability of getting a question right if you have all the pre-reqs\n\t\tPROB_ALL_PRE(skill) : { non-fluent, real, default = 0.8 };\n\t\t//if you don't have all the pre-cons, probaility mass is summed using these individual pieces\n\t\tPROB_PER_PRE(skill) : { non-fluent, real, default = 0.1 };\n\n\t\tPROB_ALL_PRE_MED(skill) : { non-fluent, real, default = 1.0 };\n\t\t//if you don't have all the pre-cons, probaility mass is summed using these individual pieces\n\t\tPROB_PER_PRE_MED(skill) : { non-fluent, real, default = 0.3 };\n\t\t\n\t\tPROB_HIGH(skill) : { non-fluent, real, default = 0.9 };\n\n\t\tLOSE_PROB(skill) : { non-fluent, real, default = 0.02 };\n\n\t\t//proficiency values, they accumulate so low and med can be on at the same time and only high will turn off\n\t\tproficiencyMed(skill) : { state-fluent, bool, default = false };\n\t\tproficiencyHigh(skill) : { state-fluent, bool, default = false };\n\n\t\tupdateTurn(skill) : {state-fluent, bool, default = false};\n\n\t\tansweredRight(skill): {state-fluent, bool, default = false};\n\t\thintedRight(skill): {state-fluent, bool, default = false};\n\t\thintDelayVar(skill) : {state-fluent, bool, default = false};\n\t\t\n\t\t//two actions.  Hint can get you directly to proficiencyMed, but only if all the pre_reqs are on\n\t\taskProb(skill) : {action-fluent, bool, default = false};\n\t\tgiveHint(skill) :  {action-fluent, bool, default = false};\n\t};\n  \n\tcpfs {\n\n\t\tupdateTurn'(?s) = \n\t\t\tKronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ (askProb(?s) | giveHint(?s)) ); \n\n\t\t//without intermediate nodes,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa": {"__data__": {"id_": "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af595f59-96f0-4ad5-b2fe-595af188c282", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "0b2db8e4a234a0d89a08035ca4b24488b9139aeb76dfa78f5266db1ebba773e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c5946b-a3a0-4fc7-8eeb-b4dc83b2c934", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "a8d7763c5aa9150f736278a71c50faea1066c4380d03f33dbc310929a04c4956", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e778b564-a77d-40a9-a6be-06d9b6f686e1", "node_type": "1", "metadata": {}, "hash": "1d87f0f6da66b53aa5d99a4de479b61bf00dc2d3b3a1e6d0116ea513f2b352fd", "class_name": "RelatedNodeInfo"}}, "text": "Hint can get you directly to proficiencyMed, but only if all the pre_reqs are on\n\t\taskProb(skill) : {action-fluent, bool, default = false};\n\t\tgiveHint(skill) :  {action-fluent, bool, default = false};\n\t};\n  \n\tcpfs {\n\n\t\tupdateTurn'(?s) = \n\t\t\tKronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ (askProb(?s) | giveHint(?s)) ); \n\n\t\t//without intermediate nodes, we need to keep ``on'' all proficiency levels that have been attained\t\t\n\n\t\tansweredRight'(?s) = \n\t\t\tif ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyHigh(?s)) \n\t\t\t\tthen Bernoulli(PROB_HIGH(?s))\n\t\t\telse if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyMed(?s) ^forall_{?s3: skill}[PRE_REQ(?s3, ?s) => proficiencyHigh(?s3)]) \n\t\t\t\tthen Bernoulli(PROB_ALL_PRE_MED(?s))\n\t\t    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^proficiencyMed(?s) ^ askProb(?s)) \n\t\t    \tthen Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE_MED(?s)])\n\t\t\telse if ([forall_{?s3: skill} ~updateTurn(?s3)] ^ askProb(?s) ^forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)]) \n\t\t\t\tthen Bernoulli(PROB_ALL_PRE(?s))\n\t\t    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s)  ^ askProb(?s)) \n\t\t    \tthen Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE(?s)])\n\t\t\telse\n\t\t\t\tKronDelta( false );\n\n\t\thintedRight'(?s) = \n\t\t\tKronDelta( [forall_{?s3: skill} ~updateTurn(?s3)] ^ giveHint(?s) ^ forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)] );\n\t\t\t\n\t\thintDelayVar'(?s) = \n\t\t\tKronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ giveHint(?s) );", "mimetype": "text/plain", "start_char_idx": 1608, "end_char_idx": 3175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e778b564-a77d-40a9-a6be-06d9b6f686e1": {"__data__": {"id_": "e778b564-a77d-40a9-a6be-06d9b6f686e1", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af595f59-96f0-4ad5-b2fe-595af188c282", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "0b2db8e4a234a0d89a08035ca4b24488b9139aeb76dfa78f5266db1ebba773e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "bd932f2f75c64b1ec24fbf1995d1c48789ebfe80c7e7912bbb67f747d4f67cbd", "class_name": "RelatedNodeInfo"}}, "text": "?s) * PROB_PER_PRE(?s)])\n\t\t\telse\n\t\t\t\tKronDelta( false );\n\n\t\thintedRight'(?s) = \n\t\t\tKronDelta( [forall_{?s3: skill} ~updateTurn(?s3)] ^ giveHint(?s) ^ forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)] );\n\t\t\t\n\t\thintDelayVar'(?s) = \n\t\t\tKronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ giveHint(?s) );\n\n\t\t//proficiencyMed can be reached through a hint if all preconditions are known or by a problem answered correctly\n\t\tproficiencyMed'(?s) =\n\t\t    if (~updateTurn(?s) ^ proficiencyMed(?s)) \n\t\t    \tthen KronDelta( true )\n\t\t    else if (updateTurn(?s) ^ hintedRight(?s)) \n\t\t    \tthen KronDelta( true )\n\t\t    else if (updateTurn(?s) ^ answeredRight(?s)) \n\t\t    \tthen KronDelta( true )\n\t\t    else if (proficiencyHigh(?s)) //may come down\n\t\t    \tthen KronDelta( true )\n\t\t    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ hintDelayVar(?s)) \n\t\t    \tthen KronDelta( true ) //can't lose it on a hint\n\t        else \n\t        \tKronDelta( false );\n\n\t\t//high proficiency is reached by getting a question and having proficiencyMed\n\t\t//but you can lose it too if you get questions wrong  \n\t\tproficiencyHigh'(?s) =\n\t\t    if (forall_{?s2: skill}[~updateTurn(?s2)])  //student turn\n\t\t    \tthen KronDelta( proficiencyHigh(?s) )\n\t\t    else if (~updateTurn(?s) ^ proficiencyHigh(?s)) \n\t\t    \tthen Bernoulli(1.0 - LOSE_PROB(?s))\n\t\t    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ answeredRight(?s)) \n\t\t    \tthen KronDelta( true )\n\t\t    else if (proficiencyHigh(?s) ^ updateTurn(?s) ^ (hintDelayVar(?s) | answeredRight(?s))) //can't lose it on a hint\n\t\t    \tthen KronDelta( true )\n\t\t    else KronDelta( false );\n\n\t};\n    \n\t reward = [sum_{?s : skill} [SKILL_WEIGHT(?s) * proficiencyHigh(?s)]] + [sum_{?s : skill} -[SKILL_WEIGHT(?s) * ~proficiencyMed(?s)]];\n\n}", "mimetype": "text/plain", "start_char_idx": 2863, "end_char_idx": 4614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf6572da-1693-4bcd-83e9-94c4d74420ff": {"__data__": {"id_": "cf6572da-1693-4bcd-83e9-94c4d74420ff", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f63904d-b61d-4390-a64a-3d17ec05955c", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "bdce76e236cc9569ba8b71197de2b5fefbe62846e586aebf59494d12f2579dc2", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__1 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.56987906;\n\t\tPROB_ALL_PRE_MED(s0) = 0.71801746;\n\t\tPROB_HIGH(s0) = 0.9066789;\n\t\tSKILL_WEIGHT(s0) = 1.1778302;\n\t\tLOSE_PROB(s0) = 0.04352919459342957;\n\t\tPROB_ALL_PRE(s1) = 0.7414986;\n\t\tPROB_ALL_PRE_MED(s1) = 0.7900833;\n\t\tPROB_HIGH(s1) = 0.9543038;\n\t\tSKILL_WEIGHT(s1) = 1.2346091;\n\t\tLOSE_PROB(s1) = 0.018769168853759767;\n\t};\n}\ninstance skill_teaching_inst_mdp__1 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__1;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfd847e6-f636-45e3-ab68-c5cc60d6c43d": {"__data__": {"id_": "dfd847e6-f636-45e3-ab68-c5cc60d6c43d", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "b1cd265339e8c459ed9efbfdd93e2a3c56605ba5a1b7627e88cd9ed4f92ebf06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93b64e36-3259-4e10-97e3-1eba300af2d6", "node_type": "1", "metadata": {}, "hash": "f40488dc6843f8186c9394ece0f506eba2b9ab6e97ef6c7c62ef5333a1fb5c8a", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__10 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5,s6,s7};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.5379598;\n\t\tPROB_ALL_PRE_MED(s0) = 0.7966592;\n\t\tPROB_HIGH(s0) = 0.86250937;\n\t\tSKILL_WEIGHT(s0) = 1.3225485;\n\t\tLOSE_PROB(s0) = 0.04723575413227082;\n\t\tPROB_ALL_PRE(s1) = 0.5750398;\n\t\tPROB_ALL_PRE_MED(s1) = 0.75465524;\n\t\tPROB_HIGH(s1) = 0.9738445;\n\t\tSKILL_WEIGHT(s1) = 1.3837464;\n\t\tLOSE_PROB(s1) = 0.031081205606460573;\n\t\tPRE_REQ(s0, s2);\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.66695064;\n\t\tPROB_PER_PRE(s2) = 0.29437217116355896;\n\t\tPROB_ALL_PRE_MED(s2) = 0.7140531;\n\t\tPROB_PER_PRE_MED(s2) = 0.3180202007293701;\n\t\tPROB_HIGH(s2) = 0.91557336;\n\t\tSKILL_WEIGHT(s2) = 2.206152;\n\t\tLOSE_PROB(s2) = 0.012146511673927309;\n\t\tPRE_REQ(s2, s3);\n\t\tPRE_REQ(s0, s3);\n\t\tPRE_REQ(s1, s3);\n\t\tPROB_ALL_PRE(s3) = 0.62654245;\n\t\tPROB_PER_PRE(s3) = 0.19705301225185395;\n\t\tPROB_ALL_PRE_MED(s3) = 0.78798556;\n\t\tPROB_PER_PRE_MED(s3) = 0.24863892197608947;\n\t\tPROB_HIGH(s3) = 0.86368424;\n\t\tSKILL_WEIGHT(s3) = 3.1999547;\n\t\tLOSE_PROB(s3) = 0.01;\n\t\tPRE_REQ(s3, s4);\n\t\tPRE_REQ(s1, s4);\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.73045695;\n\t\tPROB_PER_PRE(s4) = 0.14810273945331573;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93b64e36-3259-4e10-97e3-1eba300af2d6": {"__data__": {"id_": "93b64e36-3259-4e10-97e3-1eba300af2d6", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "b1cd265339e8c459ed9efbfdd93e2a3c56605ba5a1b7627e88cd9ed4f92ebf06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfd847e6-f636-45e3-ab68-c5cc60d6c43d", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "9195832cdd1da5dd4dfc0f13a3426116f4232f3ac35eb1f734dc0527c6a242fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5614ab58-bc2e-47c2-98cb-160db9a0d561", "node_type": "1", "metadata": {}, "hash": "6c460f90b75ceede62134db7d9698a0a6da44d70f623613adee96858353e0408", "class_name": "RelatedNodeInfo"}}, "text": "62654245;\n\t\tPROB_PER_PRE(s3) = 0.19705301225185395;\n\t\tPROB_ALL_PRE_MED(s3) = 0.78798556;\n\t\tPROB_PER_PRE_MED(s3) = 0.24863892197608947;\n\t\tPROB_HIGH(s3) = 0.86368424;\n\t\tSKILL_WEIGHT(s3) = 3.1999547;\n\t\tLOSE_PROB(s3) = 0.01;\n\t\tPRE_REQ(s3, s4);\n\t\tPRE_REQ(s1, s4);\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.73045695;\n\t\tPROB_PER_PRE(s4) = 0.14810273945331573;\n\t\tPROB_ALL_PRE_MED(s4) = 0.73045695;\n\t\tPROB_PER_PRE_MED(s4) = 0.14849880039691926;\n\t\tPROB_HIGH(s4) = 0.850513;\n\t\tSKILL_WEIGHT(s4) = 4.0810323;\n\t\tLOSE_PROB(s4) = 0.020675605535507204;\n\t\tPRE_REQ(s0, s5);\n\t\tPRE_REQ(s3, s5);\n\t\tPRE_REQ(s1, s5);\n\t\tPROB_ALL_PRE(s5) = 0.5595663;\n\t\tPROB_PER_PRE(s5) = 0.15677647292613983;\n\t\tPROB_ALL_PRE_MED(s5) = 0.74796265;\n\t\tPROB_PER_PRE_MED(s5) = 0.20301721394062042;\n\t\tPROB_HIGH(s5) = 0.946712;\n\t\tSKILL_WEIGHT(s5) = 4.3929977;\n\t\tLOSE_PROB(s5) = 0.01;\n\t\tPRE_REQ(s1, s6);\n\t\tPROB_ALL_PRE(s6) = 0.58252585;\n\t\tPROB_PER_PRE(s6) = 0.5016196370124817;\n\t\tPROB_ALL_PRE_MED(s6) = 0.68700767;\n\t\tPROB_PER_PRE_MED(s6) = 0.6655245840549469;\n\t\tPROB_HIGH(s6) = 0.94299585;\n\t\tSKILL_WEIGHT(s6) = 2.0293026;\n\t\tLOSE_PROB(s6) = 0.01;\n\t\tPRE_REQ(s5, s7);\n\t\tPROB_ALL_PRE(s7) = 0.56091326;\n\t\tPROB_PER_PRE(s7) = 0.5188458919525146;\n\t\tPROB_ALL_PRE_MED(s7) = 0.", "mimetype": "text/plain", "start_char_idx": 860, "end_char_idx": 2075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5614ab58-bc2e-47c2-98cb-160db9a0d561": {"__data__": {"id_": "5614ab58-bc2e-47c2-98cb-160db9a0d561", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bd8f822-b31f-4222-9459-aa9c63ad01f3", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "b1cd265339e8c459ed9efbfdd93e2a3c56605ba5a1b7627e88cd9ed4f92ebf06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93b64e36-3259-4e10-97e3-1eba300af2d6", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "52bb5bbebcb162ca46388e15d0a7e371225893adfedce33b72a65c02de0ea1ac", "class_name": "RelatedNodeInfo"}}, "text": "s6);\n\t\tPROB_ALL_PRE(s6) = 0.58252585;\n\t\tPROB_PER_PRE(s6) = 0.5016196370124817;\n\t\tPROB_ALL_PRE_MED(s6) = 0.68700767;\n\t\tPROB_PER_PRE_MED(s6) = 0.6655245840549469;\n\t\tPROB_HIGH(s6) = 0.94299585;\n\t\tSKILL_WEIGHT(s6) = 2.0293026;\n\t\tLOSE_PROB(s6) = 0.01;\n\t\tPRE_REQ(s5, s7);\n\t\tPROB_ALL_PRE(s7) = 0.56091326;\n\t\tPROB_PER_PRE(s7) = 0.5188458919525146;\n\t\tPROB_ALL_PRE_MED(s7) = 0.7564045;\n\t\tPROB_PER_PRE_MED(s7) = 0.6960887908935547;\n\t\tPROB_HIGH(s7) = 0.9716682;\n\t\tSKILL_WEIGHT(s7) = 5.129872;\n\t\tLOSE_PROB(s7) = 0.031601646542549135;\n\t};\n}\ninstance skill_teaching_inst_mdp__10 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__10;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1708, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388df85a-7c6f-40dc-b614-495177d33923": {"__data__": {"id_": "388df85a-7c6f-40dc-b614-495177d33923", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6c8e69e-c2d2-4984-9d58-18ed3f69a0ec", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "4ed33576e6ba9ca6b571aafb2031e492540190ca9812dc69b66996e062628f30", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__2 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.6266419;\n\t\tPROB_ALL_PRE_MED(s0) = 0.78803456;\n\t\tPROB_HIGH(s0) = 0.8867099;\n\t\tSKILL_WEIGHT(s0) = 1.4431845;\n\t\tLOSE_PROB(s0) = 0.034901031851768495;\n\t\tPROB_ALL_PRE(s1) = 0.692982;\n\t\tPROB_ALL_PRE_MED(s1) = 0.6979286;\n\t\tPROB_HIGH(s1) = 0.882593;\n\t\tSKILL_WEIGHT(s1) = 1.4221066;\n\t\tLOSE_PROB(s1) = 0.028824603557586672;\n\t};\n}\ninstance skill_teaching_inst_mdp__2 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__2;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad897de4-9733-491b-898a-f5941f6095e7": {"__data__": {"id_": "ad897de4-9733-491b-898a-f5941f6095e7", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b636796-19bc-48ab-982d-4afb8f11f79f", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "1fd4d19ef8b7bb42b7e4a09335d4de2b0190f36f79fdc7ab6d87af9246ea3f5a", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__3 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.66335756;\n\t\tPROB_ALL_PRE_MED(s0) = 0.7459964;\n\t\tPROB_HIGH(s0) = 0.99047893;\n\t\tSKILL_WEIGHT(s0) = 1.0374055;\n\t\tLOSE_PROB(s0) = 0.012337374687194825;\n\t\tPROB_ALL_PRE(s1) = 0.55798024;\n\t\tPROB_ALL_PRE_MED(s1) = 0.7089525;\n\t\tPROB_HIGH(s1) = 0.8791513;\n\t\tSKILL_WEIGHT(s1) = 1.1605124;\n\t\tLOSE_PROB(s1) = 0.04907787442207337;\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.708089;\n\t\tPROB_PER_PRE(s2) = 0.6819602966308593;\n\t\tPROB_ALL_PRE_MED(s2) = 0.7432575;\n\t\tPROB_PER_PRE_MED(s2) = 0.6840869665145874;\n\t\tPROB_HIGH(s2) = 0.9442033;\n\t\tSKILL_WEIGHT(s2) = 2.058421;\n\t\tLOSE_PROB(s2) = 0.0229320228099823;\n\t\tPRE_REQ(s2, s3);\n\t\tPRE_REQ(s1, s3);\n\t\tPROB_ALL_PRE(s3) = 0.6968088;\n\t\tPROB_PER_PRE(s3) = 0.27056136131286623;\n\t\tPROB_ALL_PRE_MED(s3) = 0.6968088;\n\t\tPROB_PER_PRE_MED(s3) = 0.29863872528076174;\n\t\tPROB_HIGH(s3) = 0.9625534;\n\t\tSKILL_WEIGHT(s3) = 3.2540152;\n\t\tLOSE_PROB(s3) = 0.018247979879379272;\n\t};\n}\ninstance skill_teaching_inst_mdp__3 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__3;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b3f046e-ad9c-432a-82cf-b1a8b736215b": {"__data__": {"id_": "8b3f046e-ad9c-432a-82cf-b1a8b736215b", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "accdef0a-adb2-46ee-9e97-c1478718c699", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "f18694540a41319234b7973f92e038abf26ca9553c8fe4812395f238ed1f5074", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__4 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.5896874;\n\t\tPROB_ALL_PRE_MED(s0) = 0.7270725;\n\t\tPROB_HIGH(s0) = 0.8716181;\n\t\tSKILL_WEIGHT(s0) = 1.3676419;\n\t\tLOSE_PROB(s0) = 0.03601190745830536;\n\t\tPROB_ALL_PRE(s1) = 0.7384989;\n\t\tPROB_ALL_PRE_MED(s1) = 0.7719059;\n\t\tPROB_HIGH(s1) = 0.9575662;\n\t\tSKILL_WEIGHT(s1) = 1.4597329;\n\t\tLOSE_PROB(s1) = 0.014676687121391297;\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.55139333;\n\t\tPROB_PER_PRE(s2) = 0.5025920987129211;\n\t\tPROB_ALL_PRE_MED(s2) = 0.71999276;\n\t\tPROB_PER_PRE_MED(s2) = 0.6569385051727294;\n\t\tPROB_HIGH(s2) = 0.8832095;\n\t\tSKILL_WEIGHT(s2) = 2.210034;\n\t\tLOSE_PROB(s2) = 0.01724103093147278;\n\t\tPRE_REQ(s2, s3);\n\t\tPRE_REQ(s0, s3);\n\t\tPROB_ALL_PRE(s3) = 0.59788907;\n\t\tPROB_PER_PRE(s3) = 0.25873849987983705;\n\t\tPROB_ALL_PRE_MED(s3) = 0.68164736;\n\t\tPROB_PER_PRE_MED(s3) = 0.2871675968170166;\n\t\tPROB_HIGH(s3) = 0.95536256;\n\t\tSKILL_WEIGHT(s3) = 3.3674781;\n\t\tLOSE_PROB(s3) = 0.01;\n\t};\n}\ninstance skill_teaching_inst_mdp__4 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__4;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96bd7eda-0367-4214-b279-61ae73bf4ae6": {"__data__": {"id_": "96bd7eda-0367-4214-b279-61ae73bf4ae6", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac3c5d25-9808-4aa2-9bbb-d11790a2927d", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "97e0884dd3c6a41edbf3bcdfe1b5514890b83e27a32e48ae62a77b03884bc35e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0d3f4b1-d799-43e2-9c42-4cadbc01c333", "node_type": "1", "metadata": {}, "hash": "a04b391e05270e35931d29a32f73d1c23b4af353c95a6c748abf48d74660ea9f", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__5 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.5110731;\n\t\tPROB_ALL_PRE_MED(s0) = 0.7110124;\n\t\tPROB_HIGH(s0) = 0.861709;\n\t\tSKILL_WEIGHT(s0) = 1.2198048;\n\t\tLOSE_PROB(s0) = 0.01;\n\t\tPROB_ALL_PRE(s1) = 0.51994395;\n\t\tPROB_ALL_PRE_MED(s1) = 0.7370734;\n\t\tPROB_HIGH(s1) = 0.90181434;\n\t\tSKILL_WEIGHT(s1) = 1.3627454;\n\t\tLOSE_PROB(s1) = 0.04683011770248413;\n\t\tPRE_REQ(s0, s2);\n\t\tPROB_ALL_PRE(s2) = 0.5263212;\n\t\tPROB_PER_PRE(s2) = 0.4785213768482208;\n\t\tPROB_ALL_PRE_MED(s2) = 0.7659159;\n\t\tPROB_PER_PRE_MED(s2) = 0.7393631219863892;\n\t\tPROB_HIGH(s2) = 0.90920776;\n\t\tSKILL_WEIGHT(s2) = 2.2734613;\n\t\tLOSE_PROB(s2) = 0.04062457978725434;\n\t\tPRE_REQ(s0, s3);\n\t\tPRE_REQ(s1, s3);\n\t\tPROB_ALL_PRE(s3) = 0.71097314;\n\t\tPROB_PER_PRE(s3) = 0.3035835802555084;\n\t\tPROB_ALL_PRE_MED(s3) = 0.74594194;\n\t\tPROB_PER_PRE_MED(s3) = 0.31977035403251647;\n\t\tPROB_HIGH(s3) = 0.94182616;\n\t\tSKILL_WEIGHT(s3) = 2.4778152;\n\t\tLOSE_PROB(s3) = 0.023865684866905212;\n\t\tPRE_REQ(s1, s4);\n\t\tPROB_ALL_PRE(s4) = 0.5160548;\n\t\tPROB_PER_PRE(s4) = 0.4667068779468536;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7894275;\n\t\tPROB_PER_PRE_MED(s4) = 0.7206694662570954;\n\t\tPROB_HIGH(s4) = 0.90769273;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0d3f4b1-d799-43e2-9c42-4cadbc01c333": {"__data__": {"id_": "b0d3f4b1-d799-43e2-9c42-4cadbc01c333", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac3c5d25-9808-4aa2-9bbb-d11790a2927d", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "97e0884dd3c6a41edbf3bcdfe1b5514890b83e27a32e48ae62a77b03884bc35e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96bd7eda-0367-4214-b279-61ae73bf4ae6", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "3e7f4c6035d3f85136206ee14f670f1e6f398ec1ef9c91b4f18a3c46b213a927", "class_name": "RelatedNodeInfo"}}, "text": "74594194;\n\t\tPROB_PER_PRE_MED(s3) = 0.31977035403251647;\n\t\tPROB_HIGH(s3) = 0.94182616;\n\t\tSKILL_WEIGHT(s3) = 2.4778152;\n\t\tLOSE_PROB(s3) = 0.023865684866905212;\n\t\tPRE_REQ(s1, s4);\n\t\tPROB_ALL_PRE(s4) = 0.5160548;\n\t\tPROB_PER_PRE(s4) = 0.4667068779468536;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7894275;\n\t\tPROB_PER_PRE_MED(s4) = 0.7206694662570954;\n\t\tPROB_HIGH(s4) = 0.90769273;\n\t\tSKILL_WEIGHT(s4) = 2.1164362;\n\t\tLOSE_PROB(s4) = 0.04388810992240906;\n\t\tPRE_REQ(s4, s5);\n\t\tPROB_ALL_PRE(s5) = 0.7196633;\n\t\tPROB_PER_PRE(s5) = 0.6648123443126679;\n\t\tPROB_ALL_PRE_MED(s5) = 0.7276146;\n\t\tPROB_PER_PRE_MED(s5) = 0.6454042017459869;\n\t\tPROB_HIGH(s5) = 0.91400516;\n\t\tSKILL_WEIGHT(s5) = 3.1053238;\n\t\tLOSE_PROB(s5) = 0.01;\n\t};\n}\ninstance skill_teaching_inst_mdp__5 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__5;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 874, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "207a472e-dcbd-421c-8941-1e922dc01d2d": {"__data__": {"id_": "207a472e-dcbd-421c-8941-1e922dc01d2d", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "6b77265eecf682c1232138abd321af2c2c4e8090ded7787d21be2839815d690e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d474c66-4e1d-4bbf-8f94-9a7c5c3bb5ea", "node_type": "1", "metadata": {}, "hash": "78d110745ef154899e7187220bf0806abf20eea49fcdf30c952bc2674c2a24ca", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__6 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.51911426;\n\t\tPROB_ALL_PRE_MED(s0) = 0.7568351;\n\t\tPROB_HIGH(s0) = 0.9770715;\n\t\tSKILL_WEIGHT(s0) = 1.0499556;\n\t\tLOSE_PROB(s0) = 0.01;\n\t\tPROB_ALL_PRE(s1) = 0.5730685;\n\t\tPROB_ALL_PRE_MED(s1) = 0.78583014;\n\t\tPROB_HIGH(s1) = 0.924819;\n\t\tSKILL_WEIGHT(s1) = 1.1698302;\n\t\tLOSE_PROB(s1) = 0.01;\n\t\tPRE_REQ(s0, s2);\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.50426114;\n\t\tPROB_PER_PRE(s2) = 0.22255530953407288;\n\t\tPROB_ALL_PRE_MED(s2) = 0.70021623;\n\t\tPROB_PER_PRE_MED(s2) = 0.3218156695365906;\n\t\tPROB_HIGH(s2) = 0.97022885;\n\t\tSKILL_WEIGHT(s2) = 2.2892184;\n\t\tLOSE_PROB(s2) = 0.04107441306114197;\n\t\tPRE_REQ(s2, s3);\n\t\tPRE_REQ(s1, s3);\n\t\tPROB_ALL_PRE(s3) = 0.62887514;\n\t\tPROB_PER_PRE(s3) = 0.2262321412563324;\n\t\tPROB_ALL_PRE_MED(s3) = 0.7496369;\n\t\tPROB_PER_PRE_MED(s3) = 0.34937326312065126;\n\t\tPROB_HIGH(s3) = 0.95192325;\n\t\tSKILL_WEIGHT(s3) = 3.2861533;\n\t\tLOSE_PROB(s3) = 0.04200699329376221;\n\t\tPRE_REQ(s1, s4);\n\t\tPROB_ALL_PRE(s4) = 0.7391402;\n\t\tPROB_PER_PRE(s4) = 0.6482304692268371;\n\t\tPROB_ALL_PRE_MED(s4) = 0.76726276;\n\t\tPROB_PER_PRE_MED(s4) = 0.7316245138645172;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d474c66-4e1d-4bbf-8f94-9a7c5c3bb5ea": {"__data__": {"id_": "8d474c66-4e1d-4bbf-8f94-9a7c5c3bb5ea", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "6b77265eecf682c1232138abd321af2c2c4e8090ded7787d21be2839815d690e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "207a472e-dcbd-421c-8941-1e922dc01d2d", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "dfd995542f1ba8c7ead3691a17f6ae8a8c226d52d40a3983658e3c43b1240492", "class_name": "RelatedNodeInfo"}}, "text": "2262321412563324;\n\t\tPROB_ALL_PRE_MED(s3) = 0.7496369;\n\t\tPROB_PER_PRE_MED(s3) = 0.34937326312065126;\n\t\tPROB_HIGH(s3) = 0.95192325;\n\t\tSKILL_WEIGHT(s3) = 3.2861533;\n\t\tLOSE_PROB(s3) = 0.04200699329376221;\n\t\tPRE_REQ(s1, s4);\n\t\tPROB_ALL_PRE(s4) = 0.7391402;\n\t\tPROB_PER_PRE(s4) = 0.6482304692268371;\n\t\tPROB_ALL_PRE_MED(s4) = 0.76726276;\n\t\tPROB_PER_PRE_MED(s4) = 0.7316245138645172;\n\t\tPROB_HIGH(s4) = 0.8891609;\n\t\tSKILL_WEIGHT(s4) = 2.0785954;\n\t\tLOSE_PROB(s4) = 0.040353044867515564;\n\t\tPRE_REQ(s3, s5);\n\t\tPROB_ALL_PRE(s5) = 0.6509724;\n\t\tPROB_PER_PRE(s5) = 0.5587133944034577;\n\t\tPROB_ALL_PRE_MED(s5) = 0.7869552;\n\t\tPROB_PER_PRE_MED(s5) = 0.7214606761932373;\n\t\tPROB_HIGH(s5) = 0.9555468;\n\t\tSKILL_WEIGHT(s5) = 4.4450536;\n\t\tLOSE_PROB(s5) = 0.030699294805526734;\n\t};\n}\ninstance skill_teaching_inst_mdp__6 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__6;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 836, "end_char_idx": 1767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab650d6f-7533-4afe-9813-3f41d37a3542": {"__data__": {"id_": "ab650d6f-7533-4afe-9813-3f41d37a3542", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e44ad02-ee97-4c02-9ae1-8930d56a3c02", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "48bd42ce99cc84d4843101bfddbdba8ef5d2cb70b95e45420cec6e6f0c1fc107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35aff9dd-35c9-4525-8c3c-03105d4edcc7", "node_type": "1", "metadata": {}, "hash": "96fe1a12f8d0a601c90ec8a96c7efbd9a392751131568f45df1f36177dad24a0", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__7 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5,s6};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.6289614;\n\t\tPROB_ALL_PRE_MED(s0) = 0.77823144;\n\t\tPROB_HIGH(s0) = 0.98427343;\n\t\tSKILL_WEIGHT(s0) = 1.2882646;\n\t\tLOSE_PROB(s0) = 0.01;\n\t\tPROB_ALL_PRE(s1) = 0.73079157;\n\t\tPROB_ALL_PRE_MED(s1) = 0.73079157;\n\t\tPROB_HIGH(s1) = 0.9924673;\n\t\tSKILL_WEIGHT(s1) = 1.2146819;\n\t\tLOSE_PROB(s1) = 0.01;\n\t\tPRE_REQ(s0, s2);\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.6776028;\n\t\tPROB_PER_PRE(s2) = 0.31203954219818114;\n\t\tPROB_ALL_PRE_MED(s2) = 0.7547205;\n\t\tPROB_PER_PRE_MED(s2) = 0.290868866443634;\n\t\tPROB_HIGH(s2) = 0.85142183;\n\t\tSKILL_WEIGHT(s2) = 2.210087;\n\t\tLOSE_PROB(s2) = 0.01;\n\t\tPRE_REQ(s2, s3);\n\t\tPROB_ALL_PRE(s3) = 0.6201674;\n\t\tPROB_PER_PRE(s3) = 0.5488474607467652;\n\t\tPROB_ALL_PRE_MED(s3) = 0.76314354;\n\t\tPROB_PER_PRE_MED(s3) = 0.6977060377597809;\n\t\tPROB_HIGH(s3) = 0.8708903;\n\t\tSKILL_WEIGHT(s3) = 3.0924094;\n\t\tLOSE_PROB(s3) = 0.03600496053695679;\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.52846634;\n\t\tPROB_PER_PRE(s4) = 0.47665039300918577;\n\t\tPROB_ALL_PRE_MED(s4) = 0.6872452;\n\t\tPROB_PER_PRE_MED(s4) = 0.6772943496704101;\n\t\tPROB_HIGH(s4) = 0.89169645;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35aff9dd-35c9-4525-8c3c-03105d4edcc7": {"__data__": {"id_": "35aff9dd-35c9-4525-8c3c-03105d4edcc7", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e44ad02-ee97-4c02-9ae1-8930d56a3c02", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "48bd42ce99cc84d4843101bfddbdba8ef5d2cb70b95e45420cec6e6f0c1fc107", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab650d6f-7533-4afe-9813-3f41d37a3542", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "6b400ee6e5e30e46c43fbb5a1a274e3d05c29656f5a19758ab0546d2b8201b42", "class_name": "RelatedNodeInfo"}}, "text": "76314354;\n\t\tPROB_PER_PRE_MED(s3) = 0.6977060377597809;\n\t\tPROB_HIGH(s3) = 0.8708903;\n\t\tSKILL_WEIGHT(s3) = 3.0924094;\n\t\tLOSE_PROB(s3) = 0.03600496053695679;\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.52846634;\n\t\tPROB_PER_PRE(s4) = 0.47665039300918577;\n\t\tPROB_ALL_PRE_MED(s4) = 0.6872452;\n\t\tPROB_PER_PRE_MED(s4) = 0.6772943496704101;\n\t\tPROB_HIGH(s4) = 0.89169645;\n\t\tSKILL_WEIGHT(s4) = 2.3115351;\n\t\tLOSE_PROB(s4) = 0.01;\n\t\tPRE_REQ(s3, s5);\n\t\tPROB_ALL_PRE(s5) = 0.7326869;\n\t\tPROB_PER_PRE(s5) = 0.7151333153247833;\n\t\tPROB_ALL_PRE_MED(s5) = 0.7326869;\n\t\tPROB_PER_PRE_MED(s5) = 0.6484197080135345;\n\t\tPROB_HIGH(s5) = 0.9606661;\n\t\tSKILL_WEIGHT(s5) = 4.0512495;\n\t\tLOSE_PROB(s5) = 0.027902701497077943;\n\t\tPRE_REQ(s2, s6);\n\t\tPRE_REQ(s1, s6);\n\t\tPROB_ALL_PRE(s6) = 0.7161131;\n\t\tPROB_PER_PRE(s6) = 0.30167375802993773;\n\t\tPROB_ALL_PRE_MED(s6) = 0.7832805;\n\t\tPROB_PER_PRE_MED(s6) = 0.30948561429977417;\n\t\tPROB_HIGH(s6) = 0.92446464;\n\t\tSKILL_WEIGHT(s6) = 3.3795197;\n\t\tLOSE_PROB(s6) = 0.021241167187690736;\n\t};\n}\ninstance skill_teaching_inst_mdp__7 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__7;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 848, "end_char_idx": 2015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7d0ba13-a46e-40eb-87b3-ca533b8ad8e7": {"__data__": {"id_": "d7d0ba13-a46e-40eb-87b3-ca533b8ad8e7", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0560772-764c-463e-b831-a8178937b7d4", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "376bdf465ace8efb636ceda2cfd49301cec23459830c295da410155d8d2584a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31", "node_type": "1", "metadata": {}, "hash": "af98eeed2b631b8a9f9950304c2f0c70641be39f773eadada51fe14bb29668e8", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__8 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5,s6};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.6240561;\n\t\tPROB_ALL_PRE_MED(s0) = 0.6767317;\n\t\tPROB_HIGH(s0) = 0.99838704;\n\t\tSKILL_WEIGHT(s0) = 1.41502;\n\t\tLOSE_PROB(s0) = 0.013960319757461549;\n\t\tPROB_ALL_PRE(s1) = 0.7315788;\n\t\tPROB_ALL_PRE_MED(s1) = 0.7984441;\n\t\tPROB_HIGH(s1) = 0.9399337;\n\t\tSKILL_WEIGHT(s1) = 1.2734425;\n\t\tLOSE_PROB(s1) = 0.01895264983177185;\n\t\tPRE_REQ(s0, s2);\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.60257506;\n\t\tPROB_PER_PRE(s2) = 0.24754523038864135;\n\t\tPROB_ALL_PRE_MED(s2) = 0.771655;\n\t\tPROB_PER_PRE_MED(s2) = 0.29233548045158386;\n\t\tPROB_HIGH(s2) = 0.8795395;\n\t\tSKILL_WEIGHT(s2) = 2.160461;\n\t\tLOSE_PROB(s2) = 0.018922489881515504;\n\t\tPRE_REQ(s2, s3);\n\t\tPRE_REQ(s1, s3);\n\t\tPROB_ALL_PRE(s3) = 0.5341582;\n\t\tPROB_PER_PRE(s3) = 0.23814212083816527;\n\t\tPROB_ALL_PRE_MED(s3) = 0.78818285;\n\t\tPROB_PER_PRE_MED(s3) = 0.3877822756767273;\n\t\tPROB_HIGH(s3) = 0.91199195;\n\t\tSKILL_WEIGHT(s3) = 3.3209429;\n\t\tLOSE_PROB(s3) = 0.03833227455615998;\n\t\tPRE_REQ(s1, s4);\n\t\tPRE_REQ(s2, s4);\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.6759284;\n\t\tPROB_PER_PRE(s4) = 0.17441368997097015;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7965332;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31": {"__data__": {"id_": "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0560772-764c-463e-b831-a8178937b7d4", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "376bdf465ace8efb636ceda2cfd49301cec23459830c295da410155d8d2584a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7d0ba13-a46e-40eb-87b3-ca533b8ad8e7", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "908b1d522b742dfb004a7f7431b9eb3d0ac54444cf9ce9851c12e7a7450085a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28c8e780-953d-40d7-a465-a71710f1e7db", "node_type": "1", "metadata": {}, "hash": "843800209238912a963c050e4ef0c100f605fd1adffa3a006b3d25e73b2f0191", "class_name": "RelatedNodeInfo"}}, "text": "23814212083816527;\n\t\tPROB_ALL_PRE_MED(s3) = 0.78818285;\n\t\tPROB_PER_PRE_MED(s3) = 0.3877822756767273;\n\t\tPROB_HIGH(s3) = 0.91199195;\n\t\tSKILL_WEIGHT(s3) = 3.3209429;\n\t\tLOSE_PROB(s3) = 0.03833227455615998;\n\t\tPRE_REQ(s1, s4);\n\t\tPRE_REQ(s2, s4);\n\t\tPRE_REQ(s0, s4);\n\t\tPROB_ALL_PRE(s4) = 0.6759284;\n\t\tPROB_PER_PRE(s4) = 0.17441368997097015;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7965332;\n\t\tPROB_PER_PRE_MED(s4) = 0.23855116367340087;\n\t\tPROB_HIGH(s4) = 0.9278462;\n\t\tSKILL_WEIGHT(s4) = 3.1534872;\n\t\tLOSE_PROB(s4) = 0.036063560843467714;\n\t\tPRE_REQ(s3, s5);\n\t\tPRE_REQ(s4, s5);\n\t\tPRE_REQ(s0, s5);\n\t\tPROB_ALL_PRE(s5) = 0.50205016;\n\t\tPROB_PER_PRE(s5) = 0.09400476813316345;\n\t\tPROB_ALL_PRE_MED(s5) = 0.6836033;\n\t\tPROB_PER_PRE_MED(s5) = 0.13496833145618437;\n\t\tPROB_HIGH(s5) = 0.9482524;\n\t\tSKILL_WEIGHT(s5) = 4.2396054;\n\t\tLOSE_PROB(s5) = 0.01031070649623871;\n\t\tPRE_REQ(s5, s6);\n\t\tPROB_ALL_PRE(s6) = 0.5515639;\n\t\tPROB_PER_PRE(s6) = 0.529619014263153;\n\t\tPROB_ALL_PRE_MED(s6) = 0.6806831;\n\t\tPROB_PER_PRE_MED(s6) = 0.6520019173622131;\n\t\tPROB_HIGH(s6) = 0.9999735;\n\t\tSKILL_WEIGHT(s6) = 5.047902;\n\t\tLOSE_PROB(s6) = 0.02086797058582306;\n\t};\n}\ninstance skill_teaching_inst_mdp__8 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__8;\n\tmax-nondef-actions = 1;\n\thorizon = 40;", "mimetype": "text/plain", "start_char_idx": 865, "end_char_idx": 2128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28c8e780-953d-40d7-a465-a71710f1e7db": {"__data__": {"id_": "28c8e780-953d-40d7-a465-a71710f1e7db", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0560772-764c-463e-b831-a8178937b7d4", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "376bdf465ace8efb636ceda2cfd49301cec23459830c295da410155d8d2584a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "0a8464ba739c99940895e7bd4c3c02e101f479bacd8e0770ac44c0b21c76e5a2", "class_name": "RelatedNodeInfo"}}, "text": "s6);\n\t\tPROB_ALL_PRE(s6) = 0.5515639;\n\t\tPROB_PER_PRE(s6) = 0.529619014263153;\n\t\tPROB_ALL_PRE_MED(s6) = 0.6806831;\n\t\tPROB_PER_PRE_MED(s6) = 0.6520019173622131;\n\t\tPROB_HIGH(s6) = 0.9999735;\n\t\tSKILL_WEIGHT(s6) = 5.047902;\n\t\tLOSE_PROB(s6) = 0.02086797058582306;\n\t};\n}\ninstance skill_teaching_inst_mdp__8 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__8;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1709, "end_char_idx": 2147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fb46b48-1cd3-43de-8af5-f9b96e548faa": {"__data__": {"id_": "4fb46b48-1cd3-43de-8af5-f9b96e548faa", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77f8abb2-f02a-439c-a585-319ef7020a2b", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "693c52155242aba63a2967fc8b2ada1a1cfdbf2ff520e4e903baf403d0277d3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f822e542-7938-4bf1-9a1c-42fe9a5232b7", "node_type": "1", "metadata": {}, "hash": "0e7f33ecccedcff79f7a63c326a63219fd9d315d3c04034ec96c19377b5ca146", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_skill_teaching_inst_mdp__9 {\n\tdomain = skill_teaching_mdp; \n\tobjects { \n\t\tskill : {s0,s1,s2,s3,s4,s5,s6,s7};\n\n\t}; \n\tnon-fluents {\n\t\tPROB_ALL_PRE(s0) = 0.7468753;\n\t\tPROB_ALL_PRE_MED(s0) = 0.75113136;\n\t\tPROB_HIGH(s0) = 0.9234121;\n\t\tSKILL_WEIGHT(s0) = 1.3582392;\n\t\tLOSE_PROB(s0) = 0.014913406968116761;\n\t\tPROB_ALL_PRE(s1) = 0.65697837;\n\t\tPROB_ALL_PRE_MED(s1) = 0.70846575;\n\t\tPROB_HIGH(s1) = 0.9780681;\n\t\tSKILL_WEIGHT(s1) = 1.0981493;\n\t\tLOSE_PROB(s1) = 0.045139196515083316;\n\t\tPRE_REQ(s1, s2);\n\t\tPROB_ALL_PRE(s2) = 0.53933734;\n\t\tPROB_PER_PRE(s2) = 0.49018006920814516;\n\t\tPROB_ALL_PRE_MED(s2) = 0.79871774;\n\t\tPROB_PER_PRE_MED(s2) = 0.7786486804485321;\n\t\tPROB_HIGH(s2) = 0.9696077;\n\t\tSKILL_WEIGHT(s2) = 2.189137;\n\t\tLOSE_PROB(s2) = 0.01;\n\t\tPRE_REQ(s0, s3);\n\t\tPROB_ALL_PRE(s3) = 0.53588957;\n\t\tPROB_PER_PRE(s3) = 0.44219784140586854;\n\t\tPROB_ALL_PRE_MED(s3) = 0.7625461;\n\t\tPROB_PER_PRE_MED(s3) = 0.7428156793117523;\n\t\tPROB_HIGH(s3) = 0.90157753;\n\t\tSKILL_WEIGHT(s3) = 2.1772897;\n\t\tLOSE_PROB(s3) = 0.01718604564666748;\n\t\tPRE_REQ(s2, s4);\n\t\tPROB_ALL_PRE(s4) = 0.58775973;\n\t\tPROB_PER_PRE(s4) = 0.4956972599029541;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7435703;\n\t\tPROB_PER_PRE_MED(s4) = 0.7072389900684357;\n\t\tPROB_HIGH(s4) = 0.8567616;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f822e542-7938-4bf1-9a1c-42fe9a5232b7": {"__data__": {"id_": "f822e542-7938-4bf1-9a1c-42fe9a5232b7", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77f8abb2-f02a-439c-a585-319ef7020a2b", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "693c52155242aba63a2967fc8b2ada1a1cfdbf2ff520e4e903baf403d0277d3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fb46b48-1cd3-43de-8af5-f9b96e548faa", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "7133444ca395e7098ee1a30d70d5ef148ee3b8a97a26675abeaa5a75115656fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d757296-6ed4-451b-bd32-a5c52d2318cd", "node_type": "1", "metadata": {}, "hash": "05bc903695c395c98dd7f7d64da6c801e37bd191f8e7fd1b28dab4d6507d7803", "class_name": "RelatedNodeInfo"}}, "text": "7625461;\n\t\tPROB_PER_PRE_MED(s3) = 0.7428156793117523;\n\t\tPROB_HIGH(s3) = 0.90157753;\n\t\tSKILL_WEIGHT(s3) = 2.1772897;\n\t\tLOSE_PROB(s3) = 0.01718604564666748;\n\t\tPRE_REQ(s2, s4);\n\t\tPROB_ALL_PRE(s4) = 0.58775973;\n\t\tPROB_PER_PRE(s4) = 0.4956972599029541;\n\t\tPROB_ALL_PRE_MED(s4) = 0.7435703;\n\t\tPROB_PER_PRE_MED(s4) = 0.7072389900684357;\n\t\tPROB_HIGH(s4) = 0.8567616;\n\t\tSKILL_WEIGHT(s4) = 3.0918918;\n\t\tLOSE_PROB(s4) = 0.03257570564746857;\n\t\tPRE_REQ(s1, s5);\n\t\tPROB_ALL_PRE(s5) = 0.73840797;\n\t\tPROB_PER_PRE(s5) = 0.6889003455638886;\n\t\tPROB_ALL_PRE_MED(s5) = 0.73840797;\n\t\tPROB_PER_PRE_MED(s5) = 0.6990386247634888;\n\t\tPROB_HIGH(s5) = 0.95278406;\n\t\tSKILL_WEIGHT(s5) = 2.380104;\n\t\tLOSE_PROB(s5) = 0.01;\n\t\tPRE_REQ(s5, s6);\n\t\tPRE_REQ(s2, s6);\n\t\tPROB_ALL_PRE(s6) = 0.66732323;\n\t\tPROB_PER_PRE(s6) = 0.324363112449646;\n\t\tPROB_ALL_PRE_MED(s6) = 0.79848707;\n\t\tPROB_PER_PRE_MED(s6) = 0.34581115245819094;\n\t\tPROB_HIGH(s6) = 0.9003494;\n\t\tSKILL_WEIGHT(s6) = 3.323818;\n\t\tLOSE_PROB(s6) = 0.01;\n\t\tPRE_REQ(s4, s7);\n\t\tPROB_ALL_PRE(s7) = 0.542452;\n\t\tPROB_PER_PRE(s7) = 0.5253962814807892;\n\t\tPROB_ALL_PRE_MED(s7) = 0.7846911;\n\t\tPROB_PER_PRE_MED(s7) = 0.7298588871955871;\n\t\tPROB_HIGH(s7) = 0.9185347;\n\t\tSKILL_WEIGHT(s7) = 4.0537095;", "mimetype": "text/plain", "start_char_idx": 867, "end_char_idx": 2066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d757296-6ed4-451b-bd32-a5c52d2318cd": {"__data__": {"id_": "6d757296-6ed4-451b-bd32-a5c52d2318cd", "embedding": null, "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77f8abb2-f02a-439c-a585-319ef7020a2b", "node_type": "4", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "693c52155242aba63a2967fc8b2ada1a1cfdbf2ff520e4e903baf403d0277d3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f822e542-7938-4bf1-9a1c-42fe9a5232b7", "node_type": "1", "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}, "hash": "dfb1d899a7810e4318e0347fc430649976f40509b92f7916f3a4021bbc8772fc", "class_name": "RelatedNodeInfo"}}, "text": "79848707;\n\t\tPROB_PER_PRE_MED(s6) = 0.34581115245819094;\n\t\tPROB_HIGH(s6) = 0.9003494;\n\t\tSKILL_WEIGHT(s6) = 3.323818;\n\t\tLOSE_PROB(s6) = 0.01;\n\t\tPRE_REQ(s4, s7);\n\t\tPROB_ALL_PRE(s7) = 0.542452;\n\t\tPROB_PER_PRE(s7) = 0.5253962814807892;\n\t\tPROB_ALL_PRE_MED(s7) = 0.7846911;\n\t\tPROB_PER_PRE_MED(s7) = 0.7298588871955871;\n\t\tPROB_HIGH(s7) = 0.9185347;\n\t\tSKILL_WEIGHT(s7) = 4.0537095;\n\t\tLOSE_PROB(s7) = 0.03780151009559632;\n\t};\n}\ninstance skill_teaching_inst_mdp__9 { \n\tdomain = skill_teaching_mdp; \n \tnon-fluents = nf_skill_teaching_inst_mdp__9;\n\tmax-nondef-actions = 1;\n\thorizon = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1694, "end_char_idx": 2287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7ac9ba5-d01e-4d89-9589-6be215a991dc": {"__data__": {"id_": "f7ac9ba5-d01e-4d89-9589-6be215a991dc", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10ad1a3b-5890-4bbe-86a4-fa5804de378a", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "897e8fac7a55ac98995c0dbcdb7c503a94800198f102940227b76f69bee91a77", "class_name": "RelatedNodeInfo"}}, "text": "domain sysadmin_mdp {\n  \n\trequirements = { \n\t\treward-deterministic // this domain does not use a stochastic reward\n\t};\n\t\n\ttypes {\n  \t\tcomputer : object;\n\t};\n      \n\tpvariables { \n    \t\t  \t\t    \t\t  \t\t\n\t\tREBOOT-PROB : { non-fluent, real, default = 0.1 };\n\t\tREBOOT-PENALTY : { non-fluent, real, default = 0.75 };\n\n\t\tCONNECTED(computer, computer) : { non-fluent, bool, default = false };\n\n\t\trunning(computer) : { state-fluent, bool, default = false };\n      \n\t\treboot(computer) : { action-fluent, bool, default = false }; \n\t};\n\t\n\tcpfs {\n  \n\t\trunning'(?x) = if (reboot(?x))\n\t\t\t\t\t\tthen KronDelta(true)  // if computer is rebooted then must be running \n\t\t\t\t\t\telse if (running(?x)) // otherwise outcome depends on network properties\n\t\t\t\t\t\t\tthen Bernoulli(.45 + .5*[1 + sum_{?y : computer} (CONNECTED(?y,?x) ^ running(?y))] \n\t\t\t\t\t\t\t\t\t\t\t\t\t/ [1 + sum_{?y : computer} CONNECTED(?y,?x)])\n\t\t\t\t\t\t\telse Bernoulli(REBOOT-PROB); \n\t};\n  \n\treward = [sum_{?c : computer} [running(?c) - (REBOOT-PENALTY * reboot(?c))]];\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfd7e271-868e-4a7d-93e4-f20ef7da4686": {"__data__": {"id_": "dfd7e271-868e-4a7d-93e4-f20ef7da4686", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3ba77db-2be1-4d05-af44-1c8934130a30", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "3aa9247a6ac2d777a39b39c4b7ee65171c45011ca2a5bfce3fa73e3d1a173f7b", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__1 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.05;\n\t\tCONNECTED(c1,c4);\n\t\tCONNECTED(c1,c9);\n\t\tCONNECTED(c2,c8);\n\t\tCONNECTED(c3,c4);\n\t\tCONNECTED(c3,c9);\n\t\tCONNECTED(c4,c5);\n\t\tCONNECTED(c5,c7);\n\t\tCONNECTED(c6,c4);\n\t\tCONNECTED(c6,c8);\n\t\tCONNECTED(c7,c9);\n\t\tCONNECTED(c8,c6);\n\t\tCONNECTED(c8,c10);\n\t\tCONNECTED(c9,c6);\n\t\tCONNECTED(c10,c2);\n\t};\n}\n\ninstance sysadmin_inst_mdp__1 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__1;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf985da-680c-4ab9-a966-4883d746e7d2": {"__data__": {"id_": "abf985da-680c-4ab9-a966-4883d746e7d2", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6e518eeb2f0434605ffa037d61250734f854ac9148882d831647ddadbcecf825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de069ee8-15e0-4a77-b460-025085a77607", "node_type": "1", "metadata": {}, "hash": "6e81cb33cbbdc3226887d4c0c435ed6343fe49a3ed7983af128327bf24d67806", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__10 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.01;\n\t\tCONNECTED(c1,c49);\n\t\tCONNECTED(c1,c24);\n\t\tCONNECTED(c1,c45);\n\t\tCONNECTED(c2,c16);\n\t\tCONNECTED(c2,c37);\n\t\tCONNECTED(c2,c43);\n\t\tCONNECTED(c3,c8);\n\t\tCONNECTED(c3,c27);\n\t\tCONNECTED(c3,c28);\n\t\tCONNECTED(c4,c7);\n\t\tCONNECTED(c4,c25);\n\t\tCONNECTED(c5,c25);\n\t\tCONNECTED(c5,c31);\n\t\tCONNECTED(c5,c44);\n\t\tCONNECTED(c6,c20);\n\t\tCONNECTED(c6,c37);\n\t\tCONNECTED(c6,c28);\n\t\tCONNECTED(c7,c32);\n\t\tCONNECTED(c7,c42);\n\t\tCONNECTED(c7,c28);\n\t\tCONNECTED(c8,c4);\n\t\tCONNECTED(c8,c24);\n\t\tCONNECTED(c8,c41);\n\t\tCONNECTED(c9,c22);\n\t\tCONNECTED(c9,c10);\n\t\tCONNECTED(c9,c44);\n\t\tCONNECTED(c10,c1);\n\t\tCONNECTED(c10,c21);\n\t\tCONNECTED(c10,c20);\n\t\tCONNECTED(c11,c1);\n\t\tCONNECTED(c11,c50);\n\t\tCONNECTED(c11,c21);\n\t\tCONNECTED(c12,c2);\n\t\tCONNECTED(c12,c5);\n\t\tCONNECTED(c12,c40);\n\t\tCONNECTED(c13,c2);\n\t\tCONNECTED(c13,c7);\n\t\tCONNECTED(c14,c32);\n\t\tCONNECTED(c14,c12);\n\t\tCONNECTED(c15,c16);\n\t\tCONNECTED(c15,c23);\n\t\tCONNECTED(c15,c47);\n\t\tCONNECTED(c16,c22);\n\t\tCONNECTED(c16,c42);\n\t\tCONNECTED(c16,c8);\n\t\tCONNECTED(c17,c43);\n\t\tCONNECTED(c17,c26);\n\t\tCONNECTED(c17,c28);\n\t\tCONNECTED(c18,c1);\n\t\tCONNECTED(c18,c41);\n\t\tCONNECTED(c18,c14);\n\t\tCONNECTED(c19,c48);\n\t\tCONNECTED(c19,c21);\n\t\tCONNECTED(c19,c10);\n\t\tCONNECTED(c20,c34);\n\t\tCONNECTED(c20,c28);\n\t\tCONNECTED(c20,c14);\n\t\tCONNECTED(c21,c49);\n\t\tCONNECTED(c21,c23);\n\t\tCONNECTED(c21,c24);\n\t\tCONNECTED(c22,c34);\n\t\tCONNECTED(c22,c2);\n\t\tCONNECTED(c22,c5);\n\t\tCONNECTED(c23,c49);\n\t\tCONNECTED(c23,c21);\n\t\tCONNECTED(c23,c44);\n\t\tCONNECTED(c24,c2);\n\t\tCONNECTED(c24,c36);\n\t\tCONNECTED(c24,c46);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de069ee8-15e0-4a77-b460-025085a77607": {"__data__": {"id_": "de069ee8-15e0-4a77-b460-025085a77607", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6e518eeb2f0434605ffa037d61250734f854ac9148882d831647ddadbcecf825", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf985da-680c-4ab9-a966-4883d746e7d2", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d5372bf9073e2e4f95b5752714f6daa7dbb5d7c8ca7fb7da53e4b3be14dfed47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8cad10f-ff3c-4bc2-b99c-cdb5de573e44", "node_type": "1", "metadata": {}, "hash": "6156c711a84705c657c726c316c1227df207a2d999a1fde65cec7a7d80ffff80", "class_name": "RelatedNodeInfo"}}, "text": "CONNECTED(c19,c48);\n\t\tCONNECTED(c19,c21);\n\t\tCONNECTED(c19,c10);\n\t\tCONNECTED(c20,c34);\n\t\tCONNECTED(c20,c28);\n\t\tCONNECTED(c20,c14);\n\t\tCONNECTED(c21,c49);\n\t\tCONNECTED(c21,c23);\n\t\tCONNECTED(c21,c24);\n\t\tCONNECTED(c22,c34);\n\t\tCONNECTED(c22,c2);\n\t\tCONNECTED(c22,c5);\n\t\tCONNECTED(c23,c49);\n\t\tCONNECTED(c23,c21);\n\t\tCONNECTED(c23,c44);\n\t\tCONNECTED(c24,c2);\n\t\tCONNECTED(c24,c36);\n\t\tCONNECTED(c24,c46);\n\t\tCONNECTED(c25,c19);\n\t\tCONNECTED(c25,c10);\n\t\tCONNECTED(c25,c31);\n\t\tCONNECTED(c26,c22);\n\t\tCONNECTED(c26,c43);\n\t\tCONNECTED(c26,c44);\n\t\tCONNECTED(c27,c19);\n\t\tCONNECTED(c27,c42);\n\t\tCONNECTED(c27,c44);\n\t\tCONNECTED(c28,c2);\n\t\tCONNECTED(c28,c19);\n\t\tCONNECTED(c28,c14);\n\t\tCONNECTED(c29,c16);\n\t\tCONNECTED(c29,c42);\n\t\tCONNECTED(c29,c27);\n\t\tCONNECTED(c30,c40);\n\t\tCONNECTED(c30,c46);\n\t\tCONNECTED(c30,c13);\n\t\tCONNECTED(c31,c35);\n\t\tCONNECTED(c31,c25);\n\t\tCONNECTED(c31,c44);\n\t\tCONNECTED(c32,c18);\n\t\tCONNECTED(c32,c22);\n\t\tCONNECTED(c32,c45);\n\t\tCONNECTED(c33,c3);\n\t\tCONNECTED(c33,c31);\n\t\tCONNECTED(c33,c15);\n\t\tCONNECTED(c34,c33);\n\t\tCONNECTED(c34,c42);\n\t\tCONNECTED(c34,c14);\n\t\tCONNECTED(c35,c39);\n\t\tCONNECTED(c35,c36);\n\t\tCONNECTED(c35,c13);\n\t\tCONNECTED(c36,c50);\n\t\tCONNECTED(c36,c8);\n\t\tCONNECTED(c36,c30);\n\t\tCONNECTED(c37,c33);\n\t\tCONNECTED(c37,c13);\n\t\tCONNECTED(c37,c15);\n\t\tCONNECTED(c38,c20);\n\t\tCONNECTED(c38,c25);\n\t\tCONNECTED(c38,c29);\n\t\tCONNECTED(c39,c1);\n\t\tCONNECTED(c39,c10);\n\t\tCONNECTED(c39,c11);\n\t\tCONNECTED(c40,c1);\n\t\tCONNECTED(c40,c18);\n\t\tCONNECTED(c40,c9);\n\t\tCONNECTED(c41,c8);\n\t\tCONNECTED(c41,c43);\n\t\tCONNECTED(c41,c13);\n\t\tCONNECTED(c42,c9);\n\t\tCONNECTED(c42,c30);\n\t\tCONNECTED(c42,c15);\n\t\tCONNECTED(c43,c34);\n\t\tCONNECTED(c43,c37);\n\t\tCONNECTED(c43,c25);\n\t\tCONNECTED(c44,c35);\n\t\tCONNECTED(c44,c43);\n\t\tCONNECTED(c44,c26);\n\t\tCONNECTED(c45,c48);\n\t\tCONNECTED(c45,c4);\n\t\tCONNECTED(c45,c41);\n\t\tCONNECTED(c46,c7);\n\t\tCONNECTED(c46,c43);\n\t\tCONNECTED(c47,c33);\n\t\tCONNECTED(c47,c20);\n\t\tCONNECTED(c47,c30);\n\t\tCONNECTED(c48,c38);", "mimetype": "text/plain", "start_char_idx": 1409, "end_char_idx": 3307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8cad10f-ff3c-4bc2-b99c-cdb5de573e44": {"__data__": {"id_": "e8cad10f-ff3c-4bc2-b99c-cdb5de573e44", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a04119e-10eb-4d21-ac85-6a31aa62323c", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6e518eeb2f0434605ffa037d61250734f854ac9148882d831647ddadbcecf825", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de069ee8-15e0-4a77-b460-025085a77607", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "29f5eb8233fdf5d3cdbae4a3b5bc01d0dc298255438ce8e9333d4816398bb5e7", "class_name": "RelatedNodeInfo"}}, "text": "CONNECTED(c42,c9);\n\t\tCONNECTED(c42,c30);\n\t\tCONNECTED(c42,c15);\n\t\tCONNECTED(c43,c34);\n\t\tCONNECTED(c43,c37);\n\t\tCONNECTED(c43,c25);\n\t\tCONNECTED(c44,c35);\n\t\tCONNECTED(c44,c43);\n\t\tCONNECTED(c44,c26);\n\t\tCONNECTED(c45,c48);\n\t\tCONNECTED(c45,c4);\n\t\tCONNECTED(c45,c41);\n\t\tCONNECTED(c46,c7);\n\t\tCONNECTED(c46,c43);\n\t\tCONNECTED(c47,c33);\n\t\tCONNECTED(c47,c20);\n\t\tCONNECTED(c47,c30);\n\t\tCONNECTED(c48,c38);\n\t\tCONNECTED(c48,c29);\n\t\tCONNECTED(c48,c44);\n\t\tCONNECTED(c49,c37);\n\t\tCONNECTED(c49,c43);\n\t\tCONNECTED(c49,c44);\n\t\tCONNECTED(c50,c34);\n\t\tCONNECTED(c50,c37);\n\t\tCONNECTED(c50,c42);\n\t};\n}\n\ninstance sysadmin_inst_mdp__10 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__10;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t\trunning(c31);\n\t\trunning(c32);\n\t\trunning(c33);\n\t\trunning(c34);\n\t\trunning(c35);\n\t\trunning(c36);\n\t\trunning(c37);\n\t\trunning(c38);\n\t\trunning(c39);\n\t\trunning(c40);\n\t\trunning(c41);\n\t\trunning(c42);\n\t\trunning(c43);\n\t\trunning(c44);\n\t\trunning(c45);\n\t\trunning(c46);\n\t\trunning(c47);\n\t\trunning(c48);\n\t\trunning(c49);\n\t\trunning(c50);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2917, "end_char_idx": 4458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34e80170-48f2-49e7-bb72-69e0534ad3b9": {"__data__": {"id_": "34e80170-48f2-49e7-bb72-69e0534ad3b9", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd045767-fee8-42ca-8c2d-f37d35d96f27", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "1654039863612671777b41bd8d593c735bad6f17cfaae0d4f6900485ff8ad5ed", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__2 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.05;\n\t\tCONNECTED(c1,c5);\n\t\tCONNECTED(c1,c10);\n\t\tCONNECTED(c2,c1);\n\t\tCONNECTED(c2,c7);\n\t\tCONNECTED(c2,c8);\n\t\tCONNECTED(c3,c4);\n\t\tCONNECTED(c3,c7);\n\t\tCONNECTED(c4,c3);\n\t\tCONNECTED(c4,c5);\n\t\tCONNECTED(c4,c6);\n\t\tCONNECTED(c5,c1);\n\t\tCONNECTED(c5,c2);\n\t\tCONNECTED(c5,c8);\n\t\tCONNECTED(c6,c3);\n\t\tCONNECTED(c6,c7);\n\t\tCONNECTED(c6,c10);\n\t\tCONNECTED(c7,c2);\n\t\tCONNECTED(c7,c8);\n\t\tCONNECTED(c7,c9);\n\t\tCONNECTED(c8,c1);\n\t\tCONNECTED(c8,c4);\n\t\tCONNECTED(c8,c5);\n\t\tCONNECTED(c9,c2);\n\t\tCONNECTED(c9,c5);\n\t\tCONNECTED(c9,c6);\n\t\tCONNECTED(c10,c2);\n\t\tCONNECTED(c10,c4);\n\t\tCONNECTED(c10,c8);\n\t};\n}\n\ninstance sysadmin_inst_mdp__2 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__2;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9ddbd84-288b-48d8-b701-c981fdd012ab": {"__data__": {"id_": "d9ddbd84-288b-48d8-b701-c981fdd012ab", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b50c87bf-bbeb-421f-9d26-e7977876a719", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "c0b0468290918ed11e1d2a529b2d763187c2cc46d577d09992f8ed2e5450d4d9", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__3 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.04;\n\t\tCONNECTED(c1,c15);\n\t\tCONNECTED(c2,c16);\n\t\tCONNECTED(c2,c15);\n\t\tCONNECTED(c3,c16);\n\t\tCONNECTED(c3,c6);\n\t\tCONNECTED(c4,c16);\n\t\tCONNECTED(c4,c1);\n\t\tCONNECTED(c5,c16);\n\t\tCONNECTED(c5,c20);\n\t\tCONNECTED(c6,c3);\n\t\tCONNECTED(c7,c4);\n\t\tCONNECTED(c7,c11);\n\t\tCONNECTED(c8,c9);\n\t\tCONNECTED(c8,c12);\n\t\tCONNECTED(c9,c1);\n\t\tCONNECTED(c9,c6);\n\t\tCONNECTED(c10,c4);\n\t\tCONNECTED(c10,c6);\n\t\tCONNECTED(c11,c17);\n\t\tCONNECTED(c11,c19);\n\t\tCONNECTED(c12,c19);\n\t\tCONNECTED(c12,c11);\n\t\tCONNECTED(c13,c1);\n\t\tCONNECTED(c13,c9);\n\t\tCONNECTED(c14,c2);\n\t\tCONNECTED(c14,c7);\n\t\tCONNECTED(c15,c17);\n\t\tCONNECTED(c15,c6);\n\t\tCONNECTED(c16,c8);\n\t\tCONNECTED(c16,c15);\n\t\tCONNECTED(c17,c2);\n\t\tCONNECTED(c17,c10);\n\t\tCONNECTED(c18,c1);\n\t\tCONNECTED(c18,c3);\n\t\tCONNECTED(c19,c1);\n\t\tCONNECTED(c19,c13);\n\t\tCONNECTED(c20,c6);\n\t\tCONNECTED(c20,c9);\n\t};\n}\n\ninstance sysadmin_inst_mdp__3 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__3;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b75435b-e088-436c-89d9-b1086b434674": {"__data__": {"id_": "0b75435b-e088-436c-89d9-b1086b434674", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27f46b58-61aa-4430-9c6f-873c68eb215d", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "005faa8c025c31e312025bddf6288073b083ee4d3df3853f8c9e3b84891b3474", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__4 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.04;\n\t\tCONNECTED(c1,c16);\n\t\tCONNECTED(c1,c8);\n\t\tCONNECTED(c1,c11);\n\t\tCONNECTED(c2,c16);\n\t\tCONNECTED(c2,c18);\n\t\tCONNECTED(c2,c6);\n\t\tCONNECTED(c3,c17);\n\t\tCONNECTED(c4,c17);\n\t\tCONNECTED(c4,c2);\n\t\tCONNECTED(c5,c17);\n\t\tCONNECTED(c5,c3);\n\t\tCONNECTED(c5,c12);\n\t\tCONNECTED(c6,c16);\n\t\tCONNECTED(c6,c5);\n\t\tCONNECTED(c6,c11);\n\t\tCONNECTED(c7,c16);\n\t\tCONNECTED(c7,c12);\n\t\tCONNECTED(c7,c13);\n\t\tCONNECTED(c8,c1);\n\t\tCONNECTED(c8,c18);\n\t\tCONNECTED(c8,c3);\n\t\tCONNECTED(c9,c3);\n\t\tCONNECTED(c9,c8);\n\t\tCONNECTED(c9,c11);\n\t\tCONNECTED(c10,c8);\n\t\tCONNECTED(c10,c11);\n\t\tCONNECTED(c10,c13);\n\t\tCONNECTED(c11,c2);\n\t\tCONNECTED(c11,c6);\n\t\tCONNECTED(c11,c7);\n\t\tCONNECTED(c12,c17);\n\t\tCONNECTED(c12,c6);\n\t\tCONNECTED(c12,c9);\n\t\tCONNECTED(c13,c1);\n\t\tCONNECTED(c13,c6);\n\t\tCONNECTED(c13,c11);\n\t\tCONNECTED(c14,c5);\n\t\tCONNECTED(c14,c6);\n\t\tCONNECTED(c14,c10);\n\t\tCONNECTED(c15,c1);\n\t\tCONNECTED(c15,c8);\n\t\tCONNECTED(c15,c12);\n\t\tCONNECTED(c16,c3);\n\t\tCONNECTED(c16,c11);\n\t\tCONNECTED(c16,c13);\n\t\tCONNECTED(c17,c20);\n\t\tCONNECTED(c17,c6);\n\t\tCONNECTED(c17,c11);\n\t\tCONNECTED(c18,c16);\n\t\tCONNECTED(c18,c3);\n\t\tCONNECTED(c18,c4);\n\t\tCONNECTED(c19,c2);\n\t\tCONNECTED(c19,c7);\n\t\tCONNECTED(c19,c12);\n\t\tCONNECTED(c20,c17);\n\t\tCONNECTED(c20,c16);\n\t\tCONNECTED(c20,c19);\n\t};\n}\n\ninstance sysadmin_inst_mdp__4 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__4;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1896, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e3f65eb-97ce-4094-b82b-2f6e3821a049": {"__data__": {"id_": "9e3f65eb-97ce-4094-b82b-2f6e3821a049", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b1ed82c-a0d9-4eb1-8eac-7fd7d6affd80", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "7d83c57056a532898c04110f362a085281f590f0b213579ff7e6247aa73782e3", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__5 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.03;\n\t\tCONNECTED(c1,c5);\n\t\tCONNECTED(c1,c13);\n\t\tCONNECTED(c2,c3);\n\t\tCONNECTED(c2,c25);\n\t\tCONNECTED(c3,c24);\n\t\tCONNECTED(c3,c13);\n\t\tCONNECTED(c4,c20);\n\t\tCONNECTED(c4,c15);\n\t\tCONNECTED(c5,c17);\n\t\tCONNECTED(c6,c3);\n\t\tCONNECTED(c6,c11);\n\t\tCONNECTED(c7,c22);\n\t\tCONNECTED(c7,c11);\n\t\tCONNECTED(c8,c19);\n\t\tCONNECTED(c8,c28);\n\t\tCONNECTED(c9,c5);\n\t\tCONNECTED(c9,c30);\n\t\tCONNECTED(c10,c17);\n\t\tCONNECTED(c10,c20);\n\t\tCONNECTED(c11,c16);\n\t\tCONNECTED(c11,c19);\n\t\tCONNECTED(c12,c27);\n\t\tCONNECTED(c12,c13);\n\t\tCONNECTED(c13,c18);\n\t\tCONNECTED(c13,c22);\n\t\tCONNECTED(c14,c16);\n\t\tCONNECTED(c14,c29);\n\t\tCONNECTED(c15,c2);\n\t\tCONNECTED(c15,c25);\n\t\tCONNECTED(c16,c24);\n\t\tCONNECTED(c17,c6);\n\t\tCONNECTED(c17,c22);\n\t\tCONNECTED(c18,c4);\n\t\tCONNECTED(c18,c9);\n\t\tCONNECTED(c19,c27);\n\t\tCONNECTED(c19,c15);\n\t\tCONNECTED(c20,c8);\n\t\tCONNECTED(c20,c29);\n\t\tCONNECTED(c21,c23);\n\t\tCONNECTED(c21,c9);\n\t\tCONNECTED(c22,c4);\n\t\tCONNECTED(c22,c21);\n\t\tCONNECTED(c23,c24);\n\t\tCONNECTED(c24,c1);\n\t\tCONNECTED(c24,c6);\n\t\tCONNECTED(c25,c1);\n\t\tCONNECTED(c25,c28);\n\t\tCONNECTED(c26,c16);\n\t\tCONNECTED(c27,c4);\n\t\tCONNECTED(c27,c9);\n\t\tCONNECTED(c28,c24);\n\t\tCONNECTED(c28,c27);\n\t\tCONNECTED(c29,c2);\n\t\tCONNECTED(c29,c25);\n\t\tCONNECTED(c30,c20);\n\t\tCONNECTED(c30,c9);\n\t};\n}\n\ninstance sysadmin_inst_mdp__5 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__5;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "480a2e20-07f5-40c3-a2ae-7545892c6f50": {"__data__": {"id_": "480a2e20-07f5-40c3-a2ae-7545892c6f50", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e0fb88f-703b-4650-ab94-c22f631a0b06", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "2775d715022787d3194f22b79368ce2fad59702bd83401db2efe0f4eb9ee40d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ef5e339-cb13-41c2-a2f7-578d1ac83ba1", "node_type": "1", "metadata": {}, "hash": "30877557ac4ba84e2b164ffec816a73033d8ec7c09d60506b19539c5c54c9f5f", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__6 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.03;\n\t\tCONNECTED(c1,c17);\n\t\tCONNECTED(c1,c8);\n\t\tCONNECTED(c1,c29);\n\t\tCONNECTED(c2,c3);\n\t\tCONNECTED(c2,c21);\n\t\tCONNECTED(c2,c27);\n\t\tCONNECTED(c3,c21);\n\t\tCONNECTED(c3,c25);\n\t\tCONNECTED(c4,c26);\n\t\tCONNECTED(c4,c11);\n\t\tCONNECTED(c4,c28);\n\t\tCONNECTED(c5,c18);\n\t\tCONNECTED(c5,c3);\n\t\tCONNECTED(c5,c29);\n\t\tCONNECTED(c6,c17);\n\t\tCONNECTED(c6,c27);\n\t\tCONNECTED(c6,c12);\n\t\tCONNECTED(c7,c18);\n\t\tCONNECTED(c7,c20);\n\t\tCONNECTED(c7,c10);\n\t\tCONNECTED(c8,c20);\n\t\tCONNECTED(c8,c7);\n\t\tCONNECTED(c8,c12);\n\t\tCONNECTED(c9,c3);\n\t\tCONNECTED(c9,c13);\n\t\tCONNECTED(c10,c18);\n\t\tCONNECTED(c10,c25);\n\t\tCONNECTED(c10,c9);\n\t\tCONNECTED(c11,c3);\n\t\tCONNECTED(c11,c10);\n\t\tCONNECTED(c11,c15);\n\t\tCONNECTED(c12,c21);\n\t\tCONNECTED(c12,c25);\n\t\tCONNECTED(c12,c15);\n\t\tCONNECTED(c13,c21);\n\t\tCONNECTED(c13,c28);\n\t\tCONNECTED(c13,c15);\n\t\tCONNECTED(c14,c2);\n\t\tCONNECTED(c14,c26);\n\t\tCONNECTED(c15,c25);\n\t\tCONNECTED(c16,c19);\n\t\tCONNECTED(c16,c10);\n\t\tCONNECTED(c16,c28);\n\t\tCONNECTED(c17,c21);\n\t\tCONNECTED(c17,c23);\n\t\tCONNECTED(c17,c27);\n\t\tCONNECTED(c18,c19);\n\t\tCONNECTED(c18,c4);\n\t\tCONNECTED(c18,c30);\n\t\tCONNECTED(c19,c3);\n\t\tCONNECTED(c19,c9);\n\t\tCONNECTED(c19,c26);\n\t\tCONNECTED(c20,c3);\n\t\tCONNECTED(c20,c25);\n\t\tCONNECTED(c20,c27);\n\t\tCONNECTED(c21,c26);\n\t\tCONNECTED(c22,c16);\n\t\tCONNECTED(c22,c4);\n\t\tCONNECTED(c22,c6);\n\t\tCONNECTED(c23,c5);\n\t\tCONNECTED(c23,c7);\n\t\tCONNECTED(c23,c27);\n\t\tCONNECTED(c24,c2);\n\t\tCONNECTED(c24,c18);\n\t\tCONNECTED(c25,c1);\n\t\tCONNECTED(c25,c15);\n\t\tCONNECTED(c26,c19);\n\t\tCONNECTED(c26,c27);\n\t\tCONNECTED(c26,c29);\n\t\tCONNECTED(c27,c3);\n\t\tCONNECTED(c27,c29);\n\t\tCONNECTED(c27,c13);\n\t\tCONNECTED(c28,c19);\n\t\tCONNECTED(c28,c22);\n\t\tCONNECTED(c28,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ef5e339-cb13-41c2-a2f7-578d1ac83ba1": {"__data__": {"id_": "6ef5e339-cb13-41c2-a2f7-578d1ac83ba1", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e0fb88f-703b-4650-ab94-c22f631a0b06", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "2775d715022787d3194f22b79368ce2fad59702bd83401db2efe0f4eb9ee40d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "480a2e20-07f5-40c3-a2ae-7545892c6f50", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d6c6aa2ad062077c2108b9a3c011f3768b18cb29b379623fb2c266a30a780086", "class_name": "RelatedNodeInfo"}}, "text": "c16);\n\t\tCONNECTED(c22,c4);\n\t\tCONNECTED(c22,c6);\n\t\tCONNECTED(c23,c5);\n\t\tCONNECTED(c23,c7);\n\t\tCONNECTED(c23,c27);\n\t\tCONNECTED(c24,c2);\n\t\tCONNECTED(c24,c18);\n\t\tCONNECTED(c25,c1);\n\t\tCONNECTED(c25,c15);\n\t\tCONNECTED(c26,c19);\n\t\tCONNECTED(c26,c27);\n\t\tCONNECTED(c26,c29);\n\t\tCONNECTED(c27,c3);\n\t\tCONNECTED(c27,c29);\n\t\tCONNECTED(c27,c13);\n\t\tCONNECTED(c28,c19);\n\t\tCONNECTED(c28,c22);\n\t\tCONNECTED(c28,c25);\n\t\tCONNECTED(c29,c17);\n\t\tCONNECTED(c29,c18);\n\t\tCONNECTED(c29,c30);\n\t\tCONNECTED(c30,c7);\n\t\tCONNECTED(c30,c28);\n\t\tCONNECTED(c30,c13);\n\t};\n}\n\ninstance sysadmin_inst_mdp__6 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__6;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1452, "end_char_idx": 2630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e928b1d1-a4b7-4da4-819a-32bdbf76b8ef": {"__data__": {"id_": "e928b1d1-a4b7-4da4-819a-32bdbf76b8ef", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "948df15a-be8c-4f88-b3e4-9355ee98ac2c", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d57d5aa7e829f18c3e2ca2a6ce5d31203fb1ec7b84d47230294552d6ca6541cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb09da69-8d87-48d3-b930-d74f1818aa3e", "node_type": "1", "metadata": {}, "hash": "10890fce51e77d67c226778d84da5453a03794cb7b4b40d4ca4a103028c3358e", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__7 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.02;\n\t\tCONNECTED(c1,c18);\n\t\tCONNECTED(c1,c5);\n\t\tCONNECTED(c2,c1);\n\t\tCONNECTED(c2,c20);\n\t\tCONNECTED(c3,c1);\n\t\tCONNECTED(c3,c24);\n\t\tCONNECTED(c4,c13);\n\t\tCONNECTED(c5,c36);\n\t\tCONNECTED(c5,c14);\n\t\tCONNECTED(c6,c16);\n\t\tCONNECTED(c6,c11);\n\t\tCONNECTED(c7,c2);\n\t\tCONNECTED(c7,c28);\n\t\tCONNECTED(c8,c24);\n\t\tCONNECTED(c8,c27);\n\t\tCONNECTED(c9,c7);\n\t\tCONNECTED(c9,c15);\n\t\tCONNECTED(c10,c35);\n\t\tCONNECTED(c10,c11);\n\t\tCONNECTED(c11,c34);\n\t\tCONNECTED(c11,c17);\n\t\tCONNECTED(c12,c29);\n\t\tCONNECTED(c12,c31);\n\t\tCONNECTED(c13,c32);\n\t\tCONNECTED(c13,c8);\n\t\tCONNECTED(c14,c38);\n\t\tCONNECTED(c14,c6);\n\t\tCONNECTED(c15,c11);\n\t\tCONNECTED(c15,c28);\n\t\tCONNECTED(c16,c5);\n\t\tCONNECTED(c16,c26);\n\t\tCONNECTED(c17,c32);\n\t\tCONNECTED(c17,c31);\n\t\tCONNECTED(c18,c10);\n\t\tCONNECTED(c18,c13);\n\t\tCONNECTED(c19,c10);\n\t\tCONNECTED(c19,c40);\n\t\tCONNECTED(c20,c19);\n\t\tCONNECTED(c20,c6);\n\t\tCONNECTED(c21,c3);\n\t\tCONNECTED(c21,c30);\n\t\tCONNECTED(c22,c25);\n\t\tCONNECTED(c22,c31);\n\t\tCONNECTED(c23,c5);\n\t\tCONNECTED(c23,c6);\n\t\tCONNECTED(c24,c35);\n\t\tCONNECTED(c24,c30);\n\t\tCONNECTED(c25,c6);\n\t\tCONNECTED(c25,c37);\n\t\tCONNECTED(c26,c39);\n\t\tCONNECTED(c26,c40);\n\t\tCONNECTED(c27,c7);\n\t\tCONNECTED(c27,c14);\n\t\tCONNECTED(c28,c6);\n\t\tCONNECTED(c28,c30);\n\t\tCONNECTED(c29,c9);\n\t\tCONNECTED(c29,c11);\n\t\tCONNECTED(c30,c26);\n\t\tCONNECTED(c30,c29);\n\t\tCONNECTED(c31,c2);\n\t\tCONNECTED(c31,c18);\n\t\tCONNECTED(c32,c4);\n\t\tCONNECTED(c32,c8);\n\t\tCONNECTED(c33,c23);\n\t\tCONNECTED(c33,c24);\n\t\tCONNECTED(c34,c36);\n\t\tCONNECTED(c34,c11);\n\t\tCONNECTED(c35,c13);\n\t\tCONNECTED(c35,c31);\n\t\tCONNECTED(c36,c8);\n\t\tCONNECTED(c36,c31);\n\t\tCONNECTED(c37,c2);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb09da69-8d87-48d3-b930-d74f1818aa3e": {"__data__": {"id_": "fb09da69-8d87-48d3-b930-d74f1818aa3e", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "948df15a-be8c-4f88-b3e4-9355ee98ac2c", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d57d5aa7e829f18c3e2ca2a6ce5d31203fb1ec7b84d47230294552d6ca6541cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e928b1d1-a4b7-4da4-819a-32bdbf76b8ef", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "afbed650543152e325d76efbfd947225fc3a9ef5f59e9bafc5b38c25137cb489", "class_name": "RelatedNodeInfo"}}, "text": "CONNECTED(c28,c30);\n\t\tCONNECTED(c29,c9);\n\t\tCONNECTED(c29,c11);\n\t\tCONNECTED(c30,c26);\n\t\tCONNECTED(c30,c29);\n\t\tCONNECTED(c31,c2);\n\t\tCONNECTED(c31,c18);\n\t\tCONNECTED(c32,c4);\n\t\tCONNECTED(c32,c8);\n\t\tCONNECTED(c33,c23);\n\t\tCONNECTED(c33,c24);\n\t\tCONNECTED(c34,c36);\n\t\tCONNECTED(c34,c11);\n\t\tCONNECTED(c35,c13);\n\t\tCONNECTED(c35,c31);\n\t\tCONNECTED(c36,c8);\n\t\tCONNECTED(c36,c31);\n\t\tCONNECTED(c37,c2);\n\t\tCONNECTED(c37,c13);\n\t\tCONNECTED(c38,c3);\n\t\tCONNECTED(c38,c23);\n\t\tCONNECTED(c39,c8);\n\t\tCONNECTED(c40,c19);\n\t\tCONNECTED(c40,c8);\n\t};\n}\n\ninstance sysadmin_inst_mdp__7 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__7;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t\trunning(c31);\n\t\trunning(c32);\n\t\trunning(c33);\n\t\trunning(c34);\n\t\trunning(c35);\n\t\trunning(c36);\n\t\trunning(c37);\n\t\trunning(c38);\n\t\trunning(c39);\n\t\trunning(c40);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1439, "end_char_idx": 2768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00d07363-1f81-4911-9c6b-fdbff23b8f1e": {"__data__": {"id_": "00d07363-1f81-4911-9c6b-fdbff23b8f1e", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "8f8ec2e8a0127a216f1881290a36959c2dae3ee9ba74b9cdd2174a05b0c6d845", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "467e535b-0cc5-41c4-8591-6aec81837bc5", "node_type": "1", "metadata": {}, "hash": "727e5834240ce284e9f44976d67270118814c6eb315cc944ba202d5a16d297ec", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__8 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.02;\n\t\tCONNECTED(c1,c35);\n\t\tCONNECTED(c1,c36);\n\t\tCONNECTED(c1,c10);\n\t\tCONNECTED(c2,c3);\n\t\tCONNECTED(c2,c10);\n\t\tCONNECTED(c2,c26);\n\t\tCONNECTED(c3,c4);\n\t\tCONNECTED(c3,c36);\n\t\tCONNECTED(c4,c16);\n\t\tCONNECTED(c4,c10);\n\t\tCONNECTED(c4,c30);\n\t\tCONNECTED(c5,c19);\n\t\tCONNECTED(c5,c24);\n\t\tCONNECTED(c5,c28);\n\t\tCONNECTED(c6,c4);\n\t\tCONNECTED(c6,c36);\n\t\tCONNECTED(c6,c11);\n\t\tCONNECTED(c7,c16);\n\t\tCONNECTED(c7,c23);\n\t\tCONNECTED(c7,c14);\n\t\tCONNECTED(c8,c21);\n\t\tCONNECTED(c8,c6);\n\t\tCONNECTED(c8,c31);\n\t\tCONNECTED(c9,c39);\n\t\tCONNECTED(c9,c20);\n\t\tCONNECTED(c9,c37);\n\t\tCONNECTED(c10,c19);\n\t\tCONNECTED(c10,c4);\n\t\tCONNECTED(c10,c9);\n\t\tCONNECTED(c11,c17);\n\t\tCONNECTED(c11,c3);\n\t\tCONNECTED(c11,c33);\n\t\tCONNECTED(c12,c33);\n\t\tCONNECTED(c12,c21);\n\t\tCONNECTED(c12,c20);\n\t\tCONNECTED(c13,c2);\n\t\tCONNECTED(c13,c11);\n\t\tCONNECTED(c14,c35);\n\t\tCONNECTED(c14,c16);\n\t\tCONNECTED(c14,c37);\n\t\tCONNECTED(c15,c1);\n\t\tCONNECTED(c15,c20);\n\t\tCONNECTED(c15,c28);\n\t\tCONNECTED(c16,c1);\n\t\tCONNECTED(c16,c38);\n\t\tCONNECTED(c16,c31);\n\t\tCONNECTED(c17,c21);\n\t\tCONNECTED(c17,c39);\n\t\tCONNECTED(c17,c30);\n\t\tCONNECTED(c18,c2);\n\t\tCONNECTED(c18,c3);\n\t\tCONNECTED(c18,c30);\n\t\tCONNECTED(c19,c5);\n\t\tCONNECTED(c19,c12);\n\t\tCONNECTED(c19,c30);\n\t\tCONNECTED(c20,c3);\n\t\tCONNECTED(c20,c7);\n\t\tCONNECTED(c20,c29);\n\t\tCONNECTED(c21,c36);\n\t\tCONNECTED(c21,c6);\n\t\tCONNECTED(c22,c2);\n\t\tCONNECTED(c22,c33);\n\t\tCONNECTED(c22,c29);\n\t\tCONNECTED(c23,c7);\n\t\tCONNECTED(c23,c22);\n\t\tCONNECTED(c23,c40);\n\t\tCONNECTED(c24,c38);\n\t\tCONNECTED(c24,c10);\n\t\tCONNECTED(c24,c13);\n\t\tCONNECTED(c25,c1);\n\t\tCONNECTED(c25,c28);\n\t\tCONNECTED(c25,c14);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467e535b-0cc5-41c4-8591-6aec81837bc5": {"__data__": {"id_": "467e535b-0cc5-41c4-8591-6aec81837bc5", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "8f8ec2e8a0127a216f1881290a36959c2dae3ee9ba74b9cdd2174a05b0c6d845", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00d07363-1f81-4911-9c6b-fdbff23b8f1e", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "231ed9025abc9b061dfb2b067a07a67082d083c79c67d449119cee8eea959faa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6db883b5-f8ea-439a-bae2-e0beba4f1737", "node_type": "1", "metadata": {}, "hash": "37b166770008385424638b6f7d39ca966dabc5457a6e9cdf8c005e92b279a026", "class_name": "RelatedNodeInfo"}}, "text": "CONNECTED(c19,c30);\n\t\tCONNECTED(c20,c3);\n\t\tCONNECTED(c20,c7);\n\t\tCONNECTED(c20,c29);\n\t\tCONNECTED(c21,c36);\n\t\tCONNECTED(c21,c6);\n\t\tCONNECTED(c22,c2);\n\t\tCONNECTED(c22,c33);\n\t\tCONNECTED(c22,c29);\n\t\tCONNECTED(c23,c7);\n\t\tCONNECTED(c23,c22);\n\t\tCONNECTED(c23,c40);\n\t\tCONNECTED(c24,c38);\n\t\tCONNECTED(c24,c10);\n\t\tCONNECTED(c24,c13);\n\t\tCONNECTED(c25,c1);\n\t\tCONNECTED(c25,c28);\n\t\tCONNECTED(c25,c14);\n\t\tCONNECTED(c26,c34);\n\t\tCONNECTED(c26,c22);\n\t\tCONNECTED(c26,c8);\n\t\tCONNECTED(c27,c35);\n\t\tCONNECTED(c27,c32);\n\t\tCONNECTED(c27,c8);\n\t\tCONNECTED(c28,c38);\n\t\tCONNECTED(c28,c23);\n\t\tCONNECTED(c28,c15);\n\t\tCONNECTED(c29,c14);\n\t\tCONNECTED(c29,c15);\n\t\tCONNECTED(c30,c19);\n\t\tCONNECTED(c30,c22);\n\t\tCONNECTED(c30,c25);\n\t\tCONNECTED(c31,c1);\n\t\tCONNECTED(c31,c21);\n\t\tCONNECTED(c31,c40);\n\t\tCONNECTED(c32,c1);\n\t\tCONNECTED(c32,c3);\n\t\tCONNECTED(c32,c26);\n\t\tCONNECTED(c33,c5);\n\t\tCONNECTED(c33,c22);\n\t\tCONNECTED(c33,c27);\n\t\tCONNECTED(c34,c4);\n\t\tCONNECTED(c34,c20);\n\t\tCONNECTED(c34,c10);\n\t\tCONNECTED(c35,c5);\n\t\tCONNECTED(c35,c30);\n\t\tCONNECTED(c35,c15);\n\t\tCONNECTED(c36,c19);\n\t\tCONNECTED(c36,c20);\n\t\tCONNECTED(c36,c26);\n\t\tCONNECTED(c37,c4);\n\t\tCONNECTED(c37,c6);\n\t\tCONNECTED(c37,c25);\n\t\tCONNECTED(c38,c4);\n\t\tCONNECTED(c38,c23);\n\t\tCONNECTED(c38,c31);\n\t\tCONNECTED(c39,c23);\n\t\tCONNECTED(c39,c28);\n\t\tCONNECTED(c39,c31);\n\t\tCONNECTED(c40,c19);\n\t\tCONNECTED(c40,c9);\n\t\tCONNECTED(c40,c28);\n\t};\n}\n\ninstance sysadmin_inst_mdp__8 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__8;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t\trunning(c31);\n\t\trunning(c32);\n\t\trunning(c33);\n\t\trunning(c34);", "mimetype": "text/plain", "start_char_idx": 1432, "end_char_idx": 3427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6db883b5-f8ea-439a-bae2-e0beba4f1737": {"__data__": {"id_": "6db883b5-f8ea-439a-bae2-e0beba4f1737", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62797bc1-3ba9-4c2d-b966-f2d4647412f5", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "8f8ec2e8a0127a216f1881290a36959c2dae3ee9ba74b9cdd2174a05b0c6d845", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "467e535b-0cc5-41c4-8591-6aec81837bc5", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "8511dcc70b127b114dde940a2833faf9f6b23e997cc2c23777f89b0f97c690cb", "class_name": "RelatedNodeInfo"}}, "text": "running(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t\trunning(c31);\n\t\trunning(c32);\n\t\trunning(c33);\n\t\trunning(c34);\n\t\trunning(c35);\n\t\trunning(c36);\n\t\trunning(c37);\n\t\trunning(c38);\n\t\trunning(c39);\n\t\trunning(c40);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2985, "end_char_idx": 3588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ced29a7b-3cd1-434c-9bf7-b8c084f66827": {"__data__": {"id_": "ced29a7b-3cd1-434c-9bf7-b8c084f66827", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1206b8e-d52b-41ed-b71f-9004c9955b76", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "9dd2b136680958fa93f61d397b4843f728f7cb56ab80e150b6540779944c99ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a469519b-76ac-4572-8a11-37d9989c63ab", "node_type": "1", "metadata": {}, "hash": "fd11b950562377e242b7084c442c88c2cf55867a45646bef644a597a0fe5e3f2", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_sysadmin_inst_mdp__9 {\n\tdomain = sysadmin_mdp;\n\tobjects {\n\t\tcomputer : {c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50};\n\t};\n\tnon-fluents {\n\t\tREBOOT-PROB = 0.01;\n\t\tCONNECTED(c1,c23);\n\t\tCONNECTED(c1,c12);\n\t\tCONNECTED(c2,c17);\n\t\tCONNECTED(c2,c14);\n\t\tCONNECTED(c3,c6);\n\t\tCONNECTED(c3,c40);\n\t\tCONNECTED(c4,c16);\n\t\tCONNECTED(c4,c31);\n\t\tCONNECTED(c5,c48);\n\t\tCONNECTED(c5,c45);\n\t\tCONNECTED(c6,c18);\n\t\tCONNECTED(c6,c45);\n\t\tCONNECTED(c7,c23);\n\t\tCONNECTED(c7,c13);\n\t\tCONNECTED(c8,c35);\n\t\tCONNECTED(c8,c45);\n\t\tCONNECTED(c9,c3);\n\t\tCONNECTED(c9,c24);\n\t\tCONNECTED(c10,c8);\n\t\tCONNECTED(c10,c11);\n\t\tCONNECTED(c11,c10);\n\t\tCONNECTED(c11,c31);\n\t\tCONNECTED(c12,c4);\n\t\tCONNECTED(c12,c8);\n\t\tCONNECTED(c13,c36);\n\t\tCONNECTED(c13,c10);\n\t\tCONNECTED(c14,c18);\n\t\tCONNECTED(c14,c27);\n\t\tCONNECTED(c15,c26);\n\t\tCONNECTED(c15,c12);\n\t\tCONNECTED(c16,c17);\n\t\tCONNECTED(c16,c42);\n\t\tCONNECTED(c17,c36);\n\t\tCONNECTED(c17,c8);\n\t\tCONNECTED(c18,c33);\n\t\tCONNECTED(c18,c39);\n\t\tCONNECTED(c19,c17);\n\t\tCONNECTED(c19,c33);\n\t\tCONNECTED(c20,c6);\n\t\tCONNECTED(c20,c13);\n\t\tCONNECTED(c21,c16);\n\t\tCONNECTED(c21,c28);\n\t\tCONNECTED(c22,c2);\n\t\tCONNECTED(c22,c25);\n\t\tCONNECTED(c23,c32);\n\t\tCONNECTED(c23,c25);\n\t\tCONNECTED(c24,c27);\n\t\tCONNECTED(c24,c28);\n\t\tCONNECTED(c25,c42);\n\t\tCONNECTED(c25,c13);\n\t\tCONNECTED(c26,c3);\n\t\tCONNECTED(c26,c10);\n\t\tCONNECTED(c27,c33);\n\t\tCONNECTED(c27,c25);\n\t\tCONNECTED(c28,c5);\n\t\tCONNECTED(c28,c20);\n\t\tCONNECTED(c29,c2);\n\t\tCONNECTED(c29,c27);\n\t\tCONNECTED(c30,c9);\n\t\tCONNECTED(c30,c40);\n\t\tCONNECTED(c31,c20);\n\t\tCONNECTED(c31,c25);\n\t\tCONNECTED(c32,c16);\n\t\tCONNECTED(c32,c28);\n\t\tCONNECTED(c33,c23);\n\t\tCONNECTED(c33,c14);\n\t\tCONNECTED(c34,c32);\n\t\tCONNECTED(c34,c39);\n\t\tCONNECTED(c35,c23);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a469519b-76ac-4572-8a11-37d9989c63ab": {"__data__": {"id_": "a469519b-76ac-4572-8a11-37d9989c63ab", "embedding": null, "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1206b8e-d52b-41ed-b71f-9004c9955b76", "node_type": "4", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "9dd2b136680958fa93f61d397b4843f728f7cb56ab80e150b6540779944c99ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ced29a7b-3cd1-434c-9bf7-b8c084f66827", "node_type": "1", "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "37a3e9a0c2dc9fdbd8a7191d221c96b6fbd25321ad3c72518bf9aa60d91bc651", "class_name": "RelatedNodeInfo"}}, "text": "CONNECTED(c26,c10);\n\t\tCONNECTED(c27,c33);\n\t\tCONNECTED(c27,c25);\n\t\tCONNECTED(c28,c5);\n\t\tCONNECTED(c28,c20);\n\t\tCONNECTED(c29,c2);\n\t\tCONNECTED(c29,c27);\n\t\tCONNECTED(c30,c9);\n\t\tCONNECTED(c30,c40);\n\t\tCONNECTED(c31,c20);\n\t\tCONNECTED(c31,c25);\n\t\tCONNECTED(c32,c16);\n\t\tCONNECTED(c32,c28);\n\t\tCONNECTED(c33,c23);\n\t\tCONNECTED(c33,c14);\n\t\tCONNECTED(c34,c32);\n\t\tCONNECTED(c34,c39);\n\t\tCONNECTED(c35,c23);\n\t\tCONNECTED(c35,c14);\n\t\tCONNECTED(c36,c13);\n\t\tCONNECTED(c36,c30);\n\t\tCONNECTED(c37,c20);\n\t\tCONNECTED(c37,c27);\n\t\tCONNECTED(c38,c32);\n\t\tCONNECTED(c38,c10);\n\t\tCONNECTED(c39,c22);\n\t\tCONNECTED(c39,c14);\n\t\tCONNECTED(c40,c48);\n\t\tCONNECTED(c40,c12);\n\t\tCONNECTED(c41,c33);\n\t\tCONNECTED(c41,c29);\n\t\tCONNECTED(c42,c19);\n\t\tCONNECTED(c42,c41);\n\t\tCONNECTED(c43,c39);\n\t\tCONNECTED(c43,c29);\n\t\tCONNECTED(c44,c10);\n\t\tCONNECTED(c44,c14);\n\t\tCONNECTED(c45,c4);\n\t\tCONNECTED(c45,c7);\n\t\tCONNECTED(c46,c34);\n\t\tCONNECTED(c46,c40);\n\t\tCONNECTED(c47,c19);\n\t\tCONNECTED(c47,c31);\n\t\tCONNECTED(c48,c21);\n\t\tCONNECTED(c48,c7);\n\t\tCONNECTED(c49,c2);\n\t\tCONNECTED(c49,c29);\n\t\tCONNECTED(c50,c20);\n\t\tCONNECTED(c50,c45);\n\t};\n}\n\ninstance sysadmin_inst_mdp__9 {\n\tdomain = sysadmin_mdp;\n\tnon-fluents = nf_sysadmin_inst_mdp__9;\n\tinit-state {\n\t\trunning(c1);\n\t\trunning(c2);\n\t\trunning(c3);\n\t\trunning(c4);\n\t\trunning(c5);\n\t\trunning(c6);\n\t\trunning(c7);\n\t\trunning(c8);\n\t\trunning(c9);\n\t\trunning(c10);\n\t\trunning(c11);\n\t\trunning(c12);\n\t\trunning(c13);\n\t\trunning(c14);\n\t\trunning(c15);\n\t\trunning(c16);\n\t\trunning(c17);\n\t\trunning(c18);\n\t\trunning(c19);\n\t\trunning(c20);\n\t\trunning(c21);\n\t\trunning(c22);\n\t\trunning(c23);\n\t\trunning(c24);\n\t\trunning(c25);\n\t\trunning(c26);\n\t\trunning(c27);\n\t\trunning(c28);\n\t\trunning(c29);\n\t\trunning(c30);\n\t\trunning(c31);\n\t\trunning(c32);\n\t\trunning(c33);\n\t\trunning(c34);\n\t\trunning(c35);\n\t\trunning(c36);\n\t\trunning(c37);\n\t\trunning(c38);\n\t\trunning(c39);\n\t\trunning(c40);\n\t\trunning(c41);\n\t\trunning(c42);\n\t\trunning(c43);\n\t\trunning(c44);\n\t\trunning(c45);\n\t\trunning(c46);\n\t\trunning(c47);\n\t\trunning(c48);\n\t\trunning(c49);\n\t\trunning(c50);\n\t};\n\n\tmax-nondef-actions = 1;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1418, "end_char_idx": 3459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ca6df4c-9ebe-456e-ab22-8b6f7564a99b": {"__data__": {"id_": "5ca6df4c-9ebe-456e-ab22-8b6f7564a99b", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16d4c693-e855-46f8-8439-c17d78b39714", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "05fa79fed3a7d93b3333abcaa2de154b5da570699804e97c143069a323d11d8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4896015e-29c6-4834-bf04-6663361e199a", "node_type": "1", "metadata": {}, "hash": "1314f876a0bcf782032538f1f29b1e711520b9c5652f274d6a6d7fab44d5f7e8", "class_name": "RelatedNodeInfo"}}, "text": "domain traffic_mdp {\n  \n\trequirements = { \n\t\treward-deterministic, // this domain does not use a stochastic reward\n\t\tconstrained-state,    // this domain uses state constraints\n\t\tconcurrent            // this domain permits multiple non-default actions\n\t};\n\n\ttypes {\n  \t\tcell : object;\n\t\tintersection : object;\n\t};\n \t\n\tpvariables { \n\n\t\t// Specify which cells are perimeter input cells and their input rates\n\t\tPERIMETER-INPUT-CELL(cell) :  { non-fluent, bool, default = false };\n\t\tPERIMETER-INPUT-RATE(cell) :  { non-fluent, real, default = 1.0 };\n\n\t\t// Specify which cells are exit cells\n\t\tPERIMETER-EXIT-CELL(cell) :   { non-fluent, bool, default = false };\n\n\t\t// Specify which cells flow into other cells\n\t\tFLOWS-INTO-CELL(cell, cell) : { non-fluent, bool, default = false }; \n\n\t\t// Specify which cells can pass into intersection on a signal phase\n\t\tFLOWS-INTO-INTERSECTION-NS(cell, intersection) : { non-fluent, bool, default = false }; \n\t\tFLOWS-INTO-INTERSECTION-EW(cell, intersection) : { non-fluent, bool, default = false }; \n\n\t\t// This is a simple boolean encoding of signal state for an intersection\n\t\t//\n\t\t// light-signal 1 2 -> effective light state\n\t\t// =========================================\n\t\t//              0 0 -> all red\n\t\t//              0 1 -> green for north-south traffic flow \n\t\t//              1 1 -> all red\n\t\t//              1 0 -> green for east-west traffic flow\n\t\tlight-signal1(intersection) : { state-fluent, bool, default = false };\n\t\tlight-signal2(intersection) : { state-fluent, bool, default = false };\n\n\t\t// Binary cell transition model (CTM): cell is either occupied or not\n\t\toccupied(cell) : { state-fluent, bool, default = false };\n\n\t\t// Do we advance this signal for an intersection to its next sequence?\n\t\tadvance(intersection) : { action-fluent, bool, default = false }; \n\t};\n  \n\tcpfs {\n  \n  \t\t// Just use a finite state machine for the light-signals\n  \t\t// Note: a light signal that is red *must* advance to the next state...\n  \t\t//       there would be no reason to hold a red signal indefinitely.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4896015e-29c6-4834-bf04-6663361e199a": {"__data__": {"id_": "4896015e-29c6-4834-bf04-6663361e199a", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16d4c693-e855-46f8-8439-c17d78b39714", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "05fa79fed3a7d93b3333abcaa2de154b5da570699804e97c143069a323d11d8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ca6df4c-9ebe-456e-ab22-8b6f7564a99b", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "a32475116730ec15483bf394e55ebf64b46ee2c483a864d01fe889613b2339cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01bd8dc9-3e21-41fb-84f6-fb6d5b8e56b7", "node_type": "1", "metadata": {}, "hash": "c40ccf196a1499f5b3c9069eb820aedf819eadfcfcf15fb813bafd87de122646", "class_name": "RelatedNodeInfo"}}, "text": "advance(intersection) : { action-fluent, bool, default = false }; \n\t};\n  \n\tcpfs {\n  \n  \t\t// Just use a finite state machine for the light-signals\n  \t\t// Note: a light signal that is red *must* advance to the next state...\n  \t\t//       there would be no reason to hold a red signal indefinitely. \n  \t\tlight-signal1'(?i) = \n  \t\t\tif (advance(?i) | (light-signal1(?i) ^ light-signal2(?i)) | (~light-signal1(?i) ^ ~light-signal2(?i)))\n \t\t\tthen // Advance to next state (see table above)\n  \t\t\t\tKronDelta( light-signal2(?i) )\n  \t\t\telse // No change \n  \t\t\t\tKronDelta( light-signal1(?i) );\n \n  \t\tlight-signal2'(?i) = \n  \t\t\tif (advance(?i) | (light-signal1(?i) ^ light-signal2(?i)) | (~light-signal1(?i) ^ ~light-signal2(?i)))\n  \t\t\tthen // Advance to next state (see table above)\n  \t\t\t\tKronDelta( ~light-signal1(?i) )\n  \t\t\telse // No change \n  \t\t\t\tKronDelta( light-signal2(?i) );\n    \n  \t\t// Update a cell's occupation status according to CTM rules\n  \t\toccupied'(?c) = // Check for perimeter cell\n  \t\t\t\t\t\tif (PERIMETER-INPUT-CELL(?c))\n  \t\t\t\t\t\tthen [if (~occupied(?c))\n  \t\t\t\t\t\t\t\tthen Bernoulli(\tPERIMETER-INPUT-RATE(?c) ) // Empty\n  \t\t\t\t\t\t\telse if (exists_{?c2 : cell} [FLOWS-INTO-CELL(?c, ?c2) ^ ~occupied(?c2)])\n  \t\t\t\t\t\t\t\tthen KronDelta(\tfalse ) // Vacated  \n  \t\t\t\t\t\t\telse KronDelta( true )] // Stopped\n  \t\t\t\t\t\t\t\n  \t\t\t\t\t\t// Check for cell entering intersection on green light\n  \t\t\t\t\t\telse if ([exists_{?i : intersection} [light-signal2(?i) ^ ~light-signal1(?i) ^ FLOWS-INTO-INTERSECTION-NS(?c,?i) ^ exists_{?c2 : cell} [FLOWS-INTO-CELL(?c, ?c2) ^ ~occupied(?c2)]]]\n  \t\t   \t\t\t\t\t\t | [exists_{?i : intersection} [light-signal1(?i) ^ ~light-signal2(?i) ^ FLOWS-INTO-INTERSECTION-EW(?c,?i) ^ exists_{?c2 : cell} [FLOWS-INTO-CELL(?c, ?c2) ^ ~occupied(?c2)]]])\n\t\t\t\t\t\tthen [if (~occupied(?c))\n  \t\t\t\t\t\t\tthen KronDelta( exists_{?c2 : cell} [FLOWS-INTO-CELL(?c2, ?c) ^ occupied(?c2)] )\n  \t\t\t\t\t\t\telse KronDelta( false )] // Vacated since cell enters intersection\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t// Check for occupied cell entering intersection (if get here, must be red)\n  \t\t\t\t\t\telse if (exists_{?i : intersection} ((FLOWS-INTO-INTERSECTION-NS(?c,?i) | FLOWS-INTO-INTERSECTION-EW(?c,?i)) ^ occupied(?c))) \n  \t\t   \t\t\t\t\tthen KronDelta( true ) // car stuck at red light\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t// Check cells ?c that take traffic exiting an intersection\n\t\t\t\t\t\telse if ( exists_{?i : intersection, ?c2 : cell} (FLOWS-INTO-INTERSECTION-NS(?c2, ?i) | FLOWS-INTO-INTERSECTION-EW(?c2, ?i)) ^ FLOWS-INTO-CELL(?c2, ?c) )\n\t\t\t\t\t\tthen [if (occupied(?c))\n\t\t\t\t\t\t\t// Can car go forward?", "mimetype": "text/plain", "start_char_idx": 1747, "end_char_idx": 4270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01bd8dc9-3e21-41fb-84f6-fb6d5b8e56b7": {"__data__": {"id_": "01bd8dc9-3e21-41fb-84f6-fb6d5b8e56b7", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16d4c693-e855-46f8-8439-c17d78b39714", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "05fa79fed3a7d93b3333abcaa2de154b5da570699804e97c143069a323d11d8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4896015e-29c6-4834-bf04-6663361e199a", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "5596b1b8212ff2c68fdd007f1c44b4ea8c89a6d8f018e79d5d43f4122636911d", "class_name": "RelatedNodeInfo"}}, "text": "then KronDelta( ~(exists_{?c2 : cell} FLOWS-INTO-CELL(?c, ?c2) ^ ~occupied(?c2)) ) \n\t\t\t\t\t\t\t// Did a car enter from intersection?\n\t\t\t\t\t\t\telse KronDelta( \n\t\t\t\t\t\t\t\t[exists_{?i : intersection} [light-signal2(?i) ^ ~light-signal1(?i) ^ exists_{?c2 : cell} [FLOWS-INTO-INTERSECTION-NS(?c2,?i) ^ FLOWS-INTO-CELL(?c2, ?c) ^ occupied(?c2)]]]\n  \t\t   \t\t\t\t\t\t | [exists_{?i : intersection} [light-signal1(?i) ^ ~light-signal2(?i) ^ exists_{?c2 : cell} [FLOWS-INTO-INTERSECTION-EW(?c2,?i) ^ FLOWS-INTO-CELL(?c2, ?c) ^ occupied(?c2)]]]\n  \t\t   \t\t\t\t\t)] \n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t// Must be a normal cell - normal transition rules apply\n\t\t\t\t\t\telse if (occupied(?c)) // Does it empty?\n\t\t\t\t\t\tthen KronDelta ( ~PERIMETER-EXIT-CELL(?c) ^ ~(exists_{?c2 : cell} [FLOWS-INTO-CELL(?c, ?c2) ^ ~occupied(?c2)]))\n\t\t\t\t\t\telse // Does it fill?\n  \t\t\t\t\t\t\tKronDelta ( exists_{?c2 : cell} [FLOWS-INTO-CELL(?c2, ?c) ^ occupied(?c2)] );\n  \n\t};\n\n\t// Minimize congestion: this reward penalizes congested traffic defined as pairs \n\t// of *consecutive* occupied cells\n\treward = [sum_{?c : cell} -[occupied(?c) ^ exists_{?c2 : cell} (FLOWS-INTO-CELL(?c2, ?c) ^ occupied(?c2))]];\n\t\n//\tstate-action-constraints {\n//\t\t// Make sure probabilities are in correct range\n//\t\tforall_{?c : cell} (PERIMETER-INPUT-RATE(?c) >= 0.0); \n//\t\tforall_{?c : cell} (PERIMETER-INPUT-RATE(?c) <= 1.0); \n//\t\t\n//\t\t// Make sure all non-entry cells have a unique cell feeding into them\n//\t\tforall_{?c : cell} [~PERIMETER-INPUT-CELL(?c) => ((sum_{?c2 : cell} FLOWS-INTO-CELL(?c2, ?c)) == 1)];\n//\t\t\n//\t\t// Make sure all non-exit cells feed into a unique cell\n//\t\tforall_{?c : cell} [~PERIMETER-EXIT-CELL(?c) => ((sum_{?c2 : cell} FLOWS-INTO-CELL(?c, ?c2)) == 1)];\n//\t\t\n//\t\t// Each intersection must have at least one cell flow into it\n//\t\tforall_{?i : intersection} [(sum_{?c : cell} \n//\t\t\t(FLOWS-INTO-INTERSECTION-NS(?c, ?i) | FLOWS-INTO-INTERSECTION-EW(?c, ?i))) >= 1];\n//\t};\n}", "mimetype": "text/plain", "start_char_idx": 4278, "end_char_idx": 6176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1897c040-236d-447e-ae6f-bb8bc9e91de3": {"__data__": {"id_": "1897c040-236d-447e-ae6f-bb8bc9e91de3", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f57e34bb-b97f-44dc-8f63-38cb2a3086ce", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ce9d246ab0a3019aaf4f2f192ef539c64c523d32f0cf4373c48bd816a38b41eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5ed5758-34db-4f3d-9f4a-6c30293b846a", "node_type": "1", "metadata": {}, "hash": "560e56bac582912c522ad24d3c87f59c19dc5fbe924449d70d79c8a4b4bce7c5", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__1 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia3a3,ia3a6,ia6a3,ia6a6};\n\t\tcell : {ca3a1,ca1a3,ca3a2,ca2a3,ca3a4,ca4a3,ca3a5,ca5a3,ca3a7,ca7a3,ca3a8,ca8a3,ca6a1,ca1a6,ca6a2,ca2a6,ca6a4,ca4a6,ca6a5,ca5a6,ca6a7,ca7a6,ca6a8,ca8a6};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca3a1);\n\t\tPERIMETER-INPUT-CELL(ca6a1);\n\t\tPERIMETER-INPUT-CELL(ca1a3);\n\t\tPERIMETER-INPUT-CELL(ca1a6);\n\n\t\tPERIMETER-INPUT-RATE(ca3a1) = 0.12959473;\n\t\tPERIMETER-INPUT-RATE(ca6a1) = 0.17356908;\n\t\tPERIMETER-INPUT-RATE(ca1a3) = 0.16023976;\n\t\tPERIMETER-INPUT-RATE(ca1a6) = 0.1621306;\n\n\t\tPERIMETER-EXIT-CELL(ca3a8);\n\t\tPERIMETER-EXIT-CELL(ca6a8);\n\t\tPERIMETER-EXIT-CELL(ca8a3);\n\t\tPERIMETER-EXIT-CELL(ca8a6);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca3a2,ia3a3);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca3a5,ia3a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a2,ia6a3);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a5,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca2a3,ia3a3);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a3,ia6a3);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca2a6,ia3a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\n\t\tFLOWS-INTO-CELL(ca3a1,ca3a2);\n\t\tFLOWS-INTO-CELL(ca1a3,ca2a3);\n\t\tFLOWS-INTO-CELL(ca3a2,ca3a4);\n\t\tFLOWS-INTO-CELL(ca2a3,ca4a3);\n\t\tFLOWS-INTO-CELL(ca3a4,ca3a5);\n\t\tFLOWS-INTO-CELL(ca4a3,ca5a3);\n\t\tFLOWS-INTO-CELL(ca3a5,ca3a7);\n\t\tFLOWS-INTO-CELL(ca5a3,ca7a3);\n\t\tFLOWS-INTO-CELL(ca3a7,ca3a8);\n\t\tFLOWS-INTO-CELL(ca7a3,ca8a3);\n\t\tFLOWS-INTO-CELL(ca6a1,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5ed5758-34db-4f3d-9f4a-6c30293b846a": {"__data__": {"id_": "d5ed5758-34db-4f3d-9f4a-6c30293b846a", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f57e34bb-b97f-44dc-8f63-38cb2a3086ce", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ce9d246ab0a3019aaf4f2f192ef539c64c523d32f0cf4373c48bd816a38b41eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1897c040-236d-447e-ae6f-bb8bc9e91de3", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "68697dd2dce665ab871c7fd6061b3c95792315ae890d61e9d2f3c2fbfa9d7d7e", "class_name": "RelatedNodeInfo"}}, "text": "ca3a2);\n\t\tFLOWS-INTO-CELL(ca1a3,ca2a3);\n\t\tFLOWS-INTO-CELL(ca3a2,ca3a4);\n\t\tFLOWS-INTO-CELL(ca2a3,ca4a3);\n\t\tFLOWS-INTO-CELL(ca3a4,ca3a5);\n\t\tFLOWS-INTO-CELL(ca4a3,ca5a3);\n\t\tFLOWS-INTO-CELL(ca3a5,ca3a7);\n\t\tFLOWS-INTO-CELL(ca5a3,ca7a3);\n\t\tFLOWS-INTO-CELL(ca3a7,ca3a8);\n\t\tFLOWS-INTO-CELL(ca7a3,ca8a3);\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,ca2a6);\n\t\tFLOWS-INTO-CELL(ca6a2,ca6a4);\n\t\tFLOWS-INTO-CELL(ca2a6,ca4a6);\n\t\tFLOWS-INTO-CELL(ca6a4,ca6a5);\n\t\tFLOWS-INTO-CELL(ca4a6,ca5a6);\n\t\tFLOWS-INTO-CELL(ca6a5,ca6a7);\n\t\tFLOWS-INTO-CELL(ca5a6,ca7a6);\n\t\tFLOWS-INTO-CELL(ca6a7,ca6a8);\n\t\tFLOWS-INTO-CELL(ca7a6,ca8a6);\n\t};\n}\n\ninstance traffic_inst_mdp__1 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__1;\n\tinit-state {\n\t\toccupied(ca3a7);\n\t\toccupied(ca6a7);\n\t\toccupied(ca8a6);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1082, "end_char_idx": 1933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b2570f0-a410-48f7-9374-c353541636a1": {"__data__": {"id_": "5b2570f0-a410-48f7-9374-c353541636a1", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e166bb60326e3a24bf1ba78fd5e22576067bf99b2c673ddd01bb3050118ff23b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd7682c-ca4f-4dac-9811-2acaf94365ea", "node_type": "1", "metadata": {}, "hash": "a655a792a9500047c650db9b11afc40a6897b34629a29224517ce52da53b18f9", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__10 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia7a7,ia7a14,ia14a7,ia14a14};\n\t\tcell : {ca7a1,ca1a7,ca7a2,ca2a7,ca7a3,ca3a7,ca7a4,ca4a7,ca7a5,ca5a7,ca7a6,ca6a7,ca7a8,ca8a7,ca7a9,ca9a7,ca7a10,ca10a7,ca7a11,ca11a7,ca7a12,ca12a7,ca7a13,ca13a7,ca7a15,ca15a7,ca7a16,ca16a7,ca7a17,ca17a7,ca7a18,ca18a7,ca7a19,ca19a7,ca7a20,ca20a7,ca14a1,ca1a14,ca14a2,ca2a14,ca14a3,ca3a14,ca14a4,ca4a14,ca14a5,ca5a14,ca14a6,ca6a14,ca14a8,ca8a14,ca14a9,ca9a14,ca14a10,ca10a14,ca14a11,ca11a14,ca14a12,ca12a14,ca14a13,ca13a14,ca14a15,ca15a14,ca14a16,ca16a14,ca14a17,ca17a14,ca14a18,ca18a14,ca14a19,ca19a14,ca14a20,ca20a14};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca7a1);\n\t\tPERIMETER-INPUT-CELL(ca14a1);\n\t\tPERIMETER-INPUT-CELL(ca1a7);\n\t\tPERIMETER-INPUT-CELL(ca1a14);\n\n\t\tPERIMETER-INPUT-RATE(ca7a1) = 0.18904275;\n\t\tPERIMETER-INPUT-RATE(ca14a1) = 0.2034848;\n\t\tPERIMETER-INPUT-RATE(ca1a7) = 0.26932672;\n\t\tPERIMETER-INPUT-RATE(ca1a14) = 0.48115665;\n\n\t\tPERIMETER-EXIT-CELL(ca7a20);\n\t\tPERIMETER-EXIT-CELL(ca14a20);\n\t\tPERIMETER-EXIT-CELL(ca20a7);\n\t\tPERIMETER-EXIT-CELL(ca20a14);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a6,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a13,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a6,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a13,ia14a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a7,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a7,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a14,ia7a14);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fd7682c-ca4f-4dac-9811-2acaf94365ea": {"__data__": {"id_": "1fd7682c-ca4f-4dac-9811-2acaf94365ea", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e166bb60326e3a24bf1ba78fd5e22576067bf99b2c673ddd01bb3050118ff23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b2570f0-a410-48f7-9374-c353541636a1", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4e55310721ae9c31710d509df41ff6a7910c72bbee62f9d92c25657e787ae3d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79be6e7a-a858-444b-a485-947c9f10fa6d", "node_type": "1", "metadata": {}, "hash": "ea0235e6507c4fb1ccad3a5902d931ba172aae641a378cf0da51af2ccbbe4bcc", "class_name": "RelatedNodeInfo"}}, "text": "PERIMETER-EXIT-CELL(ca20a7);\n\t\tPERIMETER-EXIT-CELL(ca20a14);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a6,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a13,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a6,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a13,ia14a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a7,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a7,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a14,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a14,ia14a14);\n\n\t\tFLOWS-INTO-CELL(ca7a1,ca7a2);\n\t\tFLOWS-INTO-CELL(ca1a7,ca2a7);\n\t\tFLOWS-INTO-CELL(ca7a2,ca7a3);\n\t\tFLOWS-INTO-CELL(ca2a7,ca3a7);\n\t\tFLOWS-INTO-CELL(ca7a3,ca7a4);\n\t\tFLOWS-INTO-CELL(ca3a7,ca4a7);\n\t\tFLOWS-INTO-CELL(ca7a4,ca7a5);\n\t\tFLOWS-INTO-CELL(ca4a7,ca5a7);\n\t\tFLOWS-INTO-CELL(ca7a5,ca7a6);\n\t\tFLOWS-INTO-CELL(ca5a7,ca6a7);\n\t\tFLOWS-INTO-CELL(ca7a6,ca7a8);\n\t\tFLOWS-INTO-CELL(ca6a7,ca8a7);\n\t\tFLOWS-INTO-CELL(ca7a8,ca7a9);\n\t\tFLOWS-INTO-CELL(ca8a7,ca9a7);\n\t\tFLOWS-INTO-CELL(ca7a9,ca7a10);\n\t\tFLOWS-INTO-CELL(ca9a7,ca10a7);\n\t\tFLOWS-INTO-CELL(ca7a10,ca7a11);\n\t\tFLOWS-INTO-CELL(ca10a7,ca11a7);\n\t\tFLOWS-INTO-CELL(ca7a11,ca7a12);\n\t\tFLOWS-INTO-CELL(ca11a7,ca12a7);\n\t\tFLOWS-INTO-CELL(ca7a12,ca7a13);\n\t\tFLOWS-INTO-CELL(ca12a7,ca13a7);\n\t\tFLOWS-INTO-CELL(ca7a13,ca7a15);\n\t\tFLOWS-INTO-CELL(ca13a7,ca15a7);\n\t\tFLOWS-INTO-CELL(ca7a15,ca7a16);\n\t\tFLOWS-INTO-CELL(ca15a7,ca16a7);\n\t\tFLOWS-INTO-CELL(ca7a16,ca7a17);\n\t\tFLOWS-INTO-CELL(ca16a7,ca17a7);\n\t\tFLOWS-INTO-CELL(ca7a17,ca7a18);", "mimetype": "text/plain", "start_char_idx": 1030, "end_char_idx": 2408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79be6e7a-a858-444b-a485-947c9f10fa6d": {"__data__": {"id_": "79be6e7a-a858-444b-a485-947c9f10fa6d", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e166bb60326e3a24bf1ba78fd5e22576067bf99b2c673ddd01bb3050118ff23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd7682c-ca4f-4dac-9811-2acaf94365ea", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "eafcef60dab93995aeceaa736c7d574e6c0eaadd3e2376a1c4998f5e407e048c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7e7d15d-540a-4439-a376-2938a16e888b", "node_type": "1", "metadata": {}, "hash": "ba4434112db9dffe13897d8d7b33ea973c56344e371f6f647a996e796647ab89", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-CELL(ca11a7,ca12a7);\n\t\tFLOWS-INTO-CELL(ca7a12,ca7a13);\n\t\tFLOWS-INTO-CELL(ca12a7,ca13a7);\n\t\tFLOWS-INTO-CELL(ca7a13,ca7a15);\n\t\tFLOWS-INTO-CELL(ca13a7,ca15a7);\n\t\tFLOWS-INTO-CELL(ca7a15,ca7a16);\n\t\tFLOWS-INTO-CELL(ca15a7,ca16a7);\n\t\tFLOWS-INTO-CELL(ca7a16,ca7a17);\n\t\tFLOWS-INTO-CELL(ca16a7,ca17a7);\n\t\tFLOWS-INTO-CELL(ca7a17,ca7a18);\n\t\tFLOWS-INTO-CELL(ca17a7,ca18a7);\n\t\tFLOWS-INTO-CELL(ca7a18,ca7a19);\n\t\tFLOWS-INTO-CELL(ca18a7,ca19a7);\n\t\tFLOWS-INTO-CELL(ca7a19,ca7a20);\n\t\tFLOWS-INTO-CELL(ca19a7,ca20a7);\n\t\tFLOWS-INTO-CELL(ca14a1,ca14a2);\n\t\tFLOWS-INTO-CELL(ca1a14,ca2a14);\n\t\tFLOWS-INTO-CELL(ca14a2,ca14a3);\n\t\tFLOWS-INTO-CELL(ca2a14,ca3a14);\n\t\tFLOWS-INTO-CELL(ca14a3,ca14a4);\n\t\tFLOWS-INTO-CELL(ca3a14,ca4a14);\n\t\tFLOWS-INTO-CELL(ca14a4,ca14a5);\n\t\tFLOWS-INTO-CELL(ca4a14,ca5a14);\n\t\tFLOWS-INTO-CELL(ca14a5,ca14a6);\n\t\tFLOWS-INTO-CELL(ca5a14,ca6a14);\n\t\tFLOWS-INTO-CELL(ca14a6,ca14a8);\n\t\tFLOWS-INTO-CELL(ca6a14,ca8a14);\n\t\tFLOWS-INTO-CELL(ca14a8,ca14a9);\n\t\tFLOWS-INTO-CELL(ca8a14,ca9a14);\n\t\tFLOWS-INTO-CELL(ca14a9,ca14a10);\n\t\tFLOWS-INTO-CELL(ca9a14,ca10a14);\n\t\tFLOWS-INTO-CELL(ca14a10,ca14a11);\n\t\tFLOWS-INTO-CELL(ca10a14,ca11a14);\n\t\tFLOWS-INTO-CELL(ca14a11,ca14a12);\n\t\tFLOWS-INTO-CELL(ca11a14,ca12a14);\n\t\tFLOWS-INTO-CELL(ca14a12,ca14a13);\n\t\tFLOWS-INTO-CELL(ca12a14,ca13a14);\n\t\tFLOWS-INTO-CELL(ca14a13,ca14a15);\n\t\tFLOWS-INTO-CELL(ca13a14,ca15a14);\n\t\tFLOWS-INTO-CELL(ca14a15,", "mimetype": "text/plain", "start_char_idx": 2071, "end_char_idx": 3439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7e7d15d-540a-4439-a376-2938a16e888b": {"__data__": {"id_": "d7e7d15d-540a-4439-a376-2938a16e888b", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b3b2f75-4fad-430c-a8cb-b92f356a8be0", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "e166bb60326e3a24bf1ba78fd5e22576067bf99b2c673ddd01bb3050118ff23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79be6e7a-a858-444b-a485-947c9f10fa6d", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6c53ed490d521c410ce07b89cf787ce62a02596cfa9cf7ca5e13158bc378706f", "class_name": "RelatedNodeInfo"}}, "text": "ca14a10);\n\t\tFLOWS-INTO-CELL(ca9a14,ca10a14);\n\t\tFLOWS-INTO-CELL(ca14a10,ca14a11);\n\t\tFLOWS-INTO-CELL(ca10a14,ca11a14);\n\t\tFLOWS-INTO-CELL(ca14a11,ca14a12);\n\t\tFLOWS-INTO-CELL(ca11a14,ca12a14);\n\t\tFLOWS-INTO-CELL(ca14a12,ca14a13);\n\t\tFLOWS-INTO-CELL(ca12a14,ca13a14);\n\t\tFLOWS-INTO-CELL(ca14a13,ca14a15);\n\t\tFLOWS-INTO-CELL(ca13a14,ca15a14);\n\t\tFLOWS-INTO-CELL(ca14a15,ca14a16);\n\t\tFLOWS-INTO-CELL(ca15a14,ca16a14);\n\t\tFLOWS-INTO-CELL(ca14a16,ca14a17);\n\t\tFLOWS-INTO-CELL(ca16a14,ca17a14);\n\t\tFLOWS-INTO-CELL(ca14a17,ca14a18);\n\t\tFLOWS-INTO-CELL(ca17a14,ca18a14);\n\t\tFLOWS-INTO-CELL(ca14a18,ca14a19);\n\t\tFLOWS-INTO-CELL(ca18a14,ca19a14);\n\t\tFLOWS-INTO-CELL(ca14a19,ca14a20);\n\t\tFLOWS-INTO-CELL(ca19a14,ca20a14);\n\t};\n}\n\ninstance traffic_inst_mdp__10 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__10;\n\tinit-state {\n\t\toccupied(ca7a1);\n\t\toccupied(ca7a5);\n\t\toccupied(ca7a8);\n\t\toccupied(ca7a10);\n\t\toccupied(ca11a7);\n\t\toccupied(ca7a12);\n\t\toccupied(ca7a13);\n\t\toccupied(ca7a15);\n\t\toccupied(ca7a18);\n\t\toccupied(ca1a14);\n\t\toccupied(ca14a4);\n\t\toccupied(ca4a14);\n\t\toccupied(ca14a5);\n\t\toccupied(ca14a9);\n\t\toccupied(ca14a10);\n\t\toccupied(ca10a14);\n\t\toccupied(ca14a11);\n\t\toccupied(ca12a14);\n\t\toccupied(ca14a16);\n\t\toccupied(ca14a17);\n\t\toccupied(ca19a14);\n\t\toccupied(ca20a14);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 3080, "end_char_idx": 4398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6c523c5-4a40-4f62-8aa9-11ffc3c634c6": {"__data__": {"id_": "d6c523c5-4a40-4f62-8aa9-11ffc3c634c6", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23af0791-bc75-461b-b544-b96bfcefad33", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "b0614ebd322edbd6b4915b9b7c7a43dbf33098bf2f6b9a70c22126bc1a647eab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f5a9f63-51d1-4780-be50-7a515ebe9b86", "node_type": "1", "metadata": {}, "hash": "c35aa77746bebbbacfdd54703c388c76b83fd030852f3a27119d996e92f9ddaa", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__2 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia3a3,ia3a6,ia6a3,ia6a6};\n\t\tcell : {ca3a1,ca1a3,ca3a2,ca2a3,ca3a4,ca4a3,ca3a5,ca5a3,ca3a7,ca7a3,ca3a8,ca8a3,ca6a1,ca1a6,ca6a2,ca2a6,ca6a4,ca4a6,ca6a5,ca5a6,ca6a7,ca7a6,ca6a8,ca8a6};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca3a1);\n\t\tPERIMETER-INPUT-CELL(ca6a1);\n\t\tPERIMETER-INPUT-CELL(ca1a3);\n\t\tPERIMETER-INPUT-CELL(ca1a6);\n\n\t\tPERIMETER-INPUT-RATE(ca3a1) = 0.12509413;\n\t\tPERIMETER-INPUT-RATE(ca6a1) = 0.11733327;\n\t\tPERIMETER-INPUT-RATE(ca1a3) = 0.31230262;\n\t\tPERIMETER-INPUT-RATE(ca1a6) = 0.4061942;\n\n\t\tPERIMETER-EXIT-CELL(ca3a8);\n\t\tPERIMETER-EXIT-CELL(ca6a8);\n\t\tPERIMETER-EXIT-CELL(ca8a3);\n\t\tPERIMETER-EXIT-CELL(ca8a6);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca3a2,ia3a3);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca3a5,ia3a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a2,ia6a3);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a5,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca2a3,ia3a3);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a3,ia6a3);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca2a6,ia3a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\n\t\tFLOWS-INTO-CELL(ca3a1,ca3a2);\n\t\tFLOWS-INTO-CELL(ca1a3,ca2a3);\n\t\tFLOWS-INTO-CELL(ca3a2,ca3a4);\n\t\tFLOWS-INTO-CELL(ca2a3,ca4a3);\n\t\tFLOWS-INTO-CELL(ca3a4,ca3a5);\n\t\tFLOWS-INTO-CELL(ca4a3,ca5a3);\n\t\tFLOWS-INTO-CELL(ca3a5,ca3a7);\n\t\tFLOWS-INTO-CELL(ca5a3,ca7a3);\n\t\tFLOWS-INTO-CELL(ca3a7,ca3a8);\n\t\tFLOWS-INTO-CELL(ca7a3,ca8a3);\n\t\tFLOWS-INTO-CELL(ca6a1,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f5a9f63-51d1-4780-be50-7a515ebe9b86": {"__data__": {"id_": "6f5a9f63-51d1-4780-be50-7a515ebe9b86", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23af0791-bc75-461b-b544-b96bfcefad33", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "b0614ebd322edbd6b4915b9b7c7a43dbf33098bf2f6b9a70c22126bc1a647eab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6c523c5-4a40-4f62-8aa9-11ffc3c634c6", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "8a0a24189c64486da4b578c018081d02dd9d7055aa24049f7b24a260938cb8a8", "class_name": "RelatedNodeInfo"}}, "text": "ca3a2);\n\t\tFLOWS-INTO-CELL(ca1a3,ca2a3);\n\t\tFLOWS-INTO-CELL(ca3a2,ca3a4);\n\t\tFLOWS-INTO-CELL(ca2a3,ca4a3);\n\t\tFLOWS-INTO-CELL(ca3a4,ca3a5);\n\t\tFLOWS-INTO-CELL(ca4a3,ca5a3);\n\t\tFLOWS-INTO-CELL(ca3a5,ca3a7);\n\t\tFLOWS-INTO-CELL(ca5a3,ca7a3);\n\t\tFLOWS-INTO-CELL(ca3a7,ca3a8);\n\t\tFLOWS-INTO-CELL(ca7a3,ca8a3);\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,ca2a6);\n\t\tFLOWS-INTO-CELL(ca6a2,ca6a4);\n\t\tFLOWS-INTO-CELL(ca2a6,ca4a6);\n\t\tFLOWS-INTO-CELL(ca6a4,ca6a5);\n\t\tFLOWS-INTO-CELL(ca4a6,ca5a6);\n\t\tFLOWS-INTO-CELL(ca6a5,ca6a7);\n\t\tFLOWS-INTO-CELL(ca5a6,ca7a6);\n\t\tFLOWS-INTO-CELL(ca6a7,ca6a8);\n\t\tFLOWS-INTO-CELL(ca7a6,ca8a6);\n\t};\n}\n\ninstance traffic_inst_mdp__2 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__2;\n\tinit-state {\n\t\toccupied(ca3a2);\n\t\toccupied(ca3a4);\n\t\toccupied(ca4a3);\n\t\toccupied(ca3a7);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1082, "end_char_idx": 1952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8e3b501-714b-4c95-b5e4-ee363f0bfb9e": {"__data__": {"id_": "e8e3b501-714b-4c95-b5e4-ee363f0bfb9e", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d65309a-30f3-4042-9418-7357a09a55fb", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "c03010834e2188d0e4a359405c0e86adfdb6a7fafe43798c8ba4c51e3e2c7605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a00a814d-74d9-40b5-8149-c35d9f3d6d92", "node_type": "1", "metadata": {}, "hash": "ed4ba332a55043ca000f598a3144d9cb03b9f7aad462f83580ca00a605bbf272", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__3 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia4a4,ia4a8,ia8a4,ia8a8};\n\t\tcell : {ca4a1,ca1a4,ca4a2,ca2a4,ca4a3,ca3a4,ca4a5,ca5a4,ca4a6,ca6a4,ca4a7,ca7a4,ca4a9,ca9a4,ca4a10,ca10a4,ca4a11,ca11a4,ca8a1,ca1a8,ca8a2,ca2a8,ca8a3,ca3a8,ca8a5,ca5a8,ca8a6,ca6a8,ca8a7,ca7a8,ca8a9,ca9a8,ca8a10,ca10a8,ca8a11,ca11a8};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca4a1);\n\t\tPERIMETER-INPUT-CELL(ca8a1);\n\t\tPERIMETER-INPUT-CELL(ca1a4);\n\t\tPERIMETER-INPUT-CELL(ca1a8);\n\n\t\tPERIMETER-INPUT-RATE(ca4a1) = 0.14061362;\n\t\tPERIMETER-INPUT-RATE(ca8a1) = 0.23580506;\n\t\tPERIMETER-INPUT-RATE(ca1a4) = 0.25750378;\n\t\tPERIMETER-INPUT-RATE(ca1a8) = 0.28344724;\n\n\t\tPERIMETER-EXIT-CELL(ca4a11);\n\t\tPERIMETER-EXIT-CELL(ca8a11);\n\t\tPERIMETER-EXIT-CELL(ca11a4);\n\t\tPERIMETER-EXIT-CELL(ca11a8);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca4a3,ia4a4);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca4a7,ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca8a3,ia8a4);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca8a7,ia8a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca3a4,ia4a4);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a4,ia8a4);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca3a8,ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a8,ia8a8);\n\n\t\tFLOWS-INTO-CELL(ca4a1,ca4a2);\n\t\tFLOWS-INTO-CELL(ca1a4,ca2a4);\n\t\tFLOWS-INTO-CELL(ca4a2,ca4a3);\n\t\tFLOWS-INTO-CELL(ca2a4,ca3a4);\n\t\tFLOWS-INTO-CELL(ca4a3,ca4a5);\n\t\tFLOWS-INTO-CELL(ca3a4,ca5a4);\n\t\tFLOWS-INTO-CELL(ca4a5,ca4a6);\n\t\tFLOWS-INTO-CELL(ca5a4,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a00a814d-74d9-40b5-8149-c35d9f3d6d92": {"__data__": {"id_": "a00a814d-74d9-40b5-8149-c35d9f3d6d92", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d65309a-30f3-4042-9418-7357a09a55fb", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "c03010834e2188d0e4a359405c0e86adfdb6a7fafe43798c8ba4c51e3e2c7605", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8e3b501-714b-4c95-b5e4-ee363f0bfb9e", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6f42e6e51090fef53b7b1540bc30399d305ee827cd74dc0667e80e1a169a7160", "class_name": "RelatedNodeInfo"}}, "text": "ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a8,ia8a8);\n\n\t\tFLOWS-INTO-CELL(ca4a1,ca4a2);\n\t\tFLOWS-INTO-CELL(ca1a4,ca2a4);\n\t\tFLOWS-INTO-CELL(ca4a2,ca4a3);\n\t\tFLOWS-INTO-CELL(ca2a4,ca3a4);\n\t\tFLOWS-INTO-CELL(ca4a3,ca4a5);\n\t\tFLOWS-INTO-CELL(ca3a4,ca5a4);\n\t\tFLOWS-INTO-CELL(ca4a5,ca4a6);\n\t\tFLOWS-INTO-CELL(ca5a4,ca6a4);\n\t\tFLOWS-INTO-CELL(ca4a6,ca4a7);\n\t\tFLOWS-INTO-CELL(ca6a4,ca7a4);\n\t\tFLOWS-INTO-CELL(ca4a7,ca4a9);\n\t\tFLOWS-INTO-CELL(ca7a4,ca9a4);\n\t\tFLOWS-INTO-CELL(ca4a9,ca4a10);\n\t\tFLOWS-INTO-CELL(ca9a4,ca10a4);\n\t\tFLOWS-INTO-CELL(ca4a10,ca4a11);\n\t\tFLOWS-INTO-CELL(ca10a4,ca11a4);\n\t\tFLOWS-INTO-CELL(ca8a1,ca8a2);\n\t\tFLOWS-INTO-CELL(ca1a8,ca2a8);\n\t\tFLOWS-INTO-CELL(ca8a2,ca8a3);\n\t\tFLOWS-INTO-CELL(ca2a8,ca3a8);\n\t\tFLOWS-INTO-CELL(ca8a3,ca8a5);\n\t\tFLOWS-INTO-CELL(ca3a8,ca5a8);\n\t\tFLOWS-INTO-CELL(ca8a5,ca8a6);\n\t\tFLOWS-INTO-CELL(ca5a8,ca6a8);\n\t\tFLOWS-INTO-CELL(ca8a6,ca8a7);\n\t\tFLOWS-INTO-CELL(ca6a8,ca7a8);\n\t\tFLOWS-INTO-CELL(ca8a7,ca8a9);\n\t\tFLOWS-INTO-CELL(ca7a8,ca9a8);\n\t\tFLOWS-INTO-CELL(ca8a9,ca8a10);\n\t\tFLOWS-INTO-CELL(ca9a8,ca10a8);\n\t\tFLOWS-INTO-CELL(ca8a10,ca8a11);\n\t\tFLOWS-INTO-CELL(ca10a8,ca11a8);\n\t};\n}\n\ninstance traffic_inst_mdp__3 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__3;\n\tinit-state {\n\t\toccupied(ca4a7);\n\t\toccupied(ca4a11);\n\t\toccupied(ca1a8);\n\t\toccupied(ca8a2);\n\t\toccupied(ca6a8);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1091, "end_char_idx": 2453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45fbc532-0c50-45df-92e0-517a89809400": {"__data__": {"id_": "45fbc532-0c50-45df-92e0-517a89809400", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce509453-c3c8-4755-a4af-2b30c81d511a", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ab71c52a5200f358708c29eaa92c54ca25d6702631ec74671ea72926c39df987", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e18c9b52-1936-42d9-9d21-44261de37f05", "node_type": "1", "metadata": {}, "hash": "00346fd543893b72f2ac4dc9293ace8f759037f53911c992a28413bfdde05c0d", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__4 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia4a4,ia4a8,ia8a4,ia8a8};\n\t\tcell : {ca4a1,ca1a4,ca4a2,ca2a4,ca4a3,ca3a4,ca4a5,ca5a4,ca4a6,ca6a4,ca4a7,ca7a4,ca4a9,ca9a4,ca4a10,ca10a4,ca4a11,ca11a4,ca8a1,ca1a8,ca8a2,ca2a8,ca8a3,ca3a8,ca8a5,ca5a8,ca8a6,ca6a8,ca8a7,ca7a8,ca8a9,ca9a8,ca8a10,ca10a8,ca8a11,ca11a8};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca4a1);\n\t\tPERIMETER-INPUT-CELL(ca8a1);\n\t\tPERIMETER-INPUT-CELL(ca1a4);\n\t\tPERIMETER-INPUT-CELL(ca1a8);\n\n\t\tPERIMETER-INPUT-RATE(ca4a1) = 0.35959944;\n\t\tPERIMETER-INPUT-RATE(ca8a1) = 0.2662118;\n\t\tPERIMETER-INPUT-RATE(ca1a4) = 0.44496357;\n\t\tPERIMETER-INPUT-RATE(ca1a8) = 0.47477323;\n\n\t\tPERIMETER-EXIT-CELL(ca4a11);\n\t\tPERIMETER-EXIT-CELL(ca8a11);\n\t\tPERIMETER-EXIT-CELL(ca11a4);\n\t\tPERIMETER-EXIT-CELL(ca11a8);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca4a3,ia4a4);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca4a7,ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca8a3,ia8a4);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca8a7,ia8a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca3a4,ia4a4);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a4,ia8a4);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca3a8,ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a8,ia8a8);\n\n\t\tFLOWS-INTO-CELL(ca4a1,ca4a2);\n\t\tFLOWS-INTO-CELL(ca1a4,ca2a4);\n\t\tFLOWS-INTO-CELL(ca4a2,ca4a3);\n\t\tFLOWS-INTO-CELL(ca2a4,ca3a4);\n\t\tFLOWS-INTO-CELL(ca4a3,ca4a5);\n\t\tFLOWS-INTO-CELL(ca3a4,ca5a4);\n\t\tFLOWS-INTO-CELL(ca4a5,ca4a6);\n\t\tFLOWS-INTO-CELL(ca5a4,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e18c9b52-1936-42d9-9d21-44261de37f05": {"__data__": {"id_": "e18c9b52-1936-42d9-9d21-44261de37f05", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce509453-c3c8-4755-a4af-2b30c81d511a", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ab71c52a5200f358708c29eaa92c54ca25d6702631ec74671ea72926c39df987", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45fbc532-0c50-45df-92e0-517a89809400", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "ff142d14153bc9f3971cf4b8529d1c21b61eccda43140748ee722739498f0c2d", "class_name": "RelatedNodeInfo"}}, "text": "ia4a8);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca7a8,ia8a8);\n\n\t\tFLOWS-INTO-CELL(ca4a1,ca4a2);\n\t\tFLOWS-INTO-CELL(ca1a4,ca2a4);\n\t\tFLOWS-INTO-CELL(ca4a2,ca4a3);\n\t\tFLOWS-INTO-CELL(ca2a4,ca3a4);\n\t\tFLOWS-INTO-CELL(ca4a3,ca4a5);\n\t\tFLOWS-INTO-CELL(ca3a4,ca5a4);\n\t\tFLOWS-INTO-CELL(ca4a5,ca4a6);\n\t\tFLOWS-INTO-CELL(ca5a4,ca6a4);\n\t\tFLOWS-INTO-CELL(ca4a6,ca4a7);\n\t\tFLOWS-INTO-CELL(ca6a4,ca7a4);\n\t\tFLOWS-INTO-CELL(ca4a7,ca4a9);\n\t\tFLOWS-INTO-CELL(ca7a4,ca9a4);\n\t\tFLOWS-INTO-CELL(ca4a9,ca4a10);\n\t\tFLOWS-INTO-CELL(ca9a4,ca10a4);\n\t\tFLOWS-INTO-CELL(ca4a10,ca4a11);\n\t\tFLOWS-INTO-CELL(ca10a4,ca11a4);\n\t\tFLOWS-INTO-CELL(ca8a1,ca8a2);\n\t\tFLOWS-INTO-CELL(ca1a8,ca2a8);\n\t\tFLOWS-INTO-CELL(ca8a2,ca8a3);\n\t\tFLOWS-INTO-CELL(ca2a8,ca3a8);\n\t\tFLOWS-INTO-CELL(ca8a3,ca8a5);\n\t\tFLOWS-INTO-CELL(ca3a8,ca5a8);\n\t\tFLOWS-INTO-CELL(ca8a5,ca8a6);\n\t\tFLOWS-INTO-CELL(ca5a8,ca6a8);\n\t\tFLOWS-INTO-CELL(ca8a6,ca8a7);\n\t\tFLOWS-INTO-CELL(ca6a8,ca7a8);\n\t\tFLOWS-INTO-CELL(ca8a7,ca8a9);\n\t\tFLOWS-INTO-CELL(ca7a8,ca9a8);\n\t\tFLOWS-INTO-CELL(ca8a9,ca8a10);\n\t\tFLOWS-INTO-CELL(ca9a8,ca10a8);\n\t\tFLOWS-INTO-CELL(ca8a10,ca8a11);\n\t\tFLOWS-INTO-CELL(ca10a8,ca11a8);\n\t};\n}\n\ninstance traffic_inst_mdp__4 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__4;\n\tinit-state {\n\t\toccupied(ca4a6);\n\t\toccupied(ca9a4);\n\t\toccupied(ca3a8);\n\t\toccupied(ca8a9);\n\t\toccupied(ca9a8);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 1090, "end_char_idx": 2451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c08946c8-2884-4f54-aa95-bdde7eaa708f": {"__data__": {"id_": "c08946c8-2884-4f54-aa95-bdde7eaa708f", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0ac460b3980e89486e186236e4f53e4b6985db57dde899ff7369ed958595a13a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d4c6473-e909-4d95-a17f-76ad7eaebc32", "node_type": "1", "metadata": {}, "hash": "b0009615831c1a668c7ef6c34f4963283aa775817bab35118b4d676ac24c89bc", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__5 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia5a5,ia5a10,ia10a5,ia10a10};\n\t\tcell : {ca5a1,ca1a5,ca5a2,ca2a5,ca5a3,ca3a5,ca5a4,ca4a5,ca5a6,ca6a5,ca5a7,ca7a5,ca5a8,ca8a5,ca5a9,ca9a5,ca5a11,ca11a5,ca5a12,ca12a5,ca5a13,ca13a5,ca5a14,ca14a5,ca10a1,ca1a10,ca10a2,ca2a10,ca10a3,ca3a10,ca10a4,ca4a10,ca10a6,ca6a10,ca10a7,ca7a10,ca10a8,ca8a10,ca10a9,ca9a10,ca10a11,ca11a10,ca10a12,ca12a10,ca10a13,ca13a10,ca10a14,ca14a10};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca5a1);\n\t\tPERIMETER-INPUT-CELL(ca10a1);\n\t\tPERIMETER-INPUT-CELL(ca1a5);\n\t\tPERIMETER-INPUT-CELL(ca1a10);\n\n\t\tPERIMETER-INPUT-RATE(ca5a1) = 0.2629145;\n\t\tPERIMETER-INPUT-RATE(ca10a1) = 0.17818838;\n\t\tPERIMETER-INPUT-RATE(ca1a5) = 0.17750853;\n\t\tPERIMETER-INPUT-RATE(ca1a10) = 0.16130137;\n\n\t\tPERIMETER-EXIT-CELL(ca5a14);\n\t\tPERIMETER-EXIT-CELL(ca10a14);\n\t\tPERIMETER-EXIT-CELL(ca14a5);\n\t\tPERIMETER-EXIT-CELL(ca14a10);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca5a4,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca5a9,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca10a4,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca10a9,ia10a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a5,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a5,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a10,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a10,ia10a10);\n\n\t\tFLOWS-INTO-CELL(ca5a1,ca5a2);\n\t\tFLOWS-INTO-CELL(ca1a5,ca2a5);\n\t\tFLOWS-INTO-CELL(ca5a2,ca5a3);\n\t\tFLOWS-INTO-CELL(ca2a5,ca3a5);\n\t\tFLOWS-INTO-CELL(ca5a3,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d4c6473-e909-4d95-a17f-76ad7eaebc32": {"__data__": {"id_": "3d4c6473-e909-4d95-a17f-76ad7eaebc32", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0ac460b3980e89486e186236e4f53e4b6985db57dde899ff7369ed958595a13a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c08946c8-2884-4f54-aa95-bdde7eaa708f", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "f4a3d229d9599ddac53f9b72e847a102d45fc18d261f85c5bfa1356ad8d20ffc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ec41f89-cb7d-4c05-99c2-a987a86a61c3", "node_type": "1", "metadata": {}, "hash": "a2a2695cadcc14ec81426daf880e25a0e89627817847504c72fd6d52d23540f3", "class_name": "RelatedNodeInfo"}}, "text": "ia10a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a5,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a5,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a10,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a10,ia10a10);\n\n\t\tFLOWS-INTO-CELL(ca5a1,ca5a2);\n\t\tFLOWS-INTO-CELL(ca1a5,ca2a5);\n\t\tFLOWS-INTO-CELL(ca5a2,ca5a3);\n\t\tFLOWS-INTO-CELL(ca2a5,ca3a5);\n\t\tFLOWS-INTO-CELL(ca5a3,ca5a4);\n\t\tFLOWS-INTO-CELL(ca3a5,ca4a5);\n\t\tFLOWS-INTO-CELL(ca5a4,ca5a6);\n\t\tFLOWS-INTO-CELL(ca4a5,ca6a5);\n\t\tFLOWS-INTO-CELL(ca5a6,ca5a7);\n\t\tFLOWS-INTO-CELL(ca6a5,ca7a5);\n\t\tFLOWS-INTO-CELL(ca5a7,ca5a8);\n\t\tFLOWS-INTO-CELL(ca7a5,ca8a5);\n\t\tFLOWS-INTO-CELL(ca5a8,ca5a9);\n\t\tFLOWS-INTO-CELL(ca8a5,ca9a5);\n\t\tFLOWS-INTO-CELL(ca5a9,ca5a11);\n\t\tFLOWS-INTO-CELL(ca9a5,ca11a5);\n\t\tFLOWS-INTO-CELL(ca5a11,ca5a12);\n\t\tFLOWS-INTO-CELL(ca11a5,ca12a5);\n\t\tFLOWS-INTO-CELL(ca5a12,ca5a13);\n\t\tFLOWS-INTO-CELL(ca12a5,ca13a5);\n\t\tFLOWS-INTO-CELL(ca5a13,ca5a14);\n\t\tFLOWS-INTO-CELL(ca13a5,ca14a5);\n\t\tFLOWS-INTO-CELL(ca10a1,ca10a2);\n\t\tFLOWS-INTO-CELL(ca1a10,ca2a10);\n\t\tFLOWS-INTO-CELL(ca10a2,ca10a3);\n\t\tFLOWS-INTO-CELL(ca2a10,ca3a10);\n\t\tFLOWS-INTO-CELL(ca10a3,ca10a4);\n\t\tFLOWS-INTO-CELL(ca3a10,ca4a10);\n\t\tFLOWS-INTO-CELL(ca10a4,ca10a6);\n\t\tFLOWS-INTO-CELL(ca4a10,ca6a10);\n\t\tFLOWS-INTO-CELL(ca10a6,ca10a7);\n\t\tFLOWS-INTO-CELL(ca6a10,ca7a10);\n\t\tFLOWS-INTO-CELL(ca10a7,ca10a8);\n\t\tFLOWS-INTO-CELL(ca7a10,ca8a10);\n\t\tFLOWS-INTO-CELL(ca10a8,", "mimetype": "text/plain", "start_char_idx": 1079, "end_char_idx": 2419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ec41f89-cb7d-4c05-99c2-a987a86a61c3": {"__data__": {"id_": "0ec41f89-cb7d-4c05-99c2-a987a86a61c3", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0ac460b3980e89486e186236e4f53e4b6985db57dde899ff7369ed958595a13a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d4c6473-e909-4d95-a17f-76ad7eaebc32", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b0df29de66d6e50c035323fcdda7e46b0397a4622354b6d4927132a2f0e2ffc", "class_name": "RelatedNodeInfo"}}, "text": "ca10a3);\n\t\tFLOWS-INTO-CELL(ca2a10,ca3a10);\n\t\tFLOWS-INTO-CELL(ca10a3,ca10a4);\n\t\tFLOWS-INTO-CELL(ca3a10,ca4a10);\n\t\tFLOWS-INTO-CELL(ca10a4,ca10a6);\n\t\tFLOWS-INTO-CELL(ca4a10,ca6a10);\n\t\tFLOWS-INTO-CELL(ca10a6,ca10a7);\n\t\tFLOWS-INTO-CELL(ca6a10,ca7a10);\n\t\tFLOWS-INTO-CELL(ca10a7,ca10a8);\n\t\tFLOWS-INTO-CELL(ca7a10,ca8a10);\n\t\tFLOWS-INTO-CELL(ca10a8,ca10a9);\n\t\tFLOWS-INTO-CELL(ca8a10,ca9a10);\n\t\tFLOWS-INTO-CELL(ca10a9,ca10a11);\n\t\tFLOWS-INTO-CELL(ca9a10,ca11a10);\n\t\tFLOWS-INTO-CELL(ca10a11,ca10a12);\n\t\tFLOWS-INTO-CELL(ca11a10,ca12a10);\n\t\tFLOWS-INTO-CELL(ca10a12,ca10a13);\n\t\tFLOWS-INTO-CELL(ca12a10,ca13a10);\n\t\tFLOWS-INTO-CELL(ca10a13,ca10a14);\n\t\tFLOWS-INTO-CELL(ca13a10,ca14a10);\n\t};\n}\n\ninstance traffic_inst_mdp__5 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__5;\n\tinit-state {\n\t\toccupied(ca5a1);\n\t\toccupied(ca1a5);\n\t\toccupied(ca5a3);\n\t\toccupied(ca3a5);\n\t\toccupied(ca5a4);\n\t\toccupied(ca5a6);\n\t\toccupied(ca8a5);\n\t\toccupied(ca9a5);\n\t\toccupied(ca11a5);\n\t\toccupied(ca13a5);\n\t\toccupied(ca14a5);\n\t\toccupied(ca1a10);\n\t\toccupied(ca10a3);\n\t\toccupied(ca10a4);\n\t\toccupied(ca4a10);\n\t\toccupied(ca8a10);\n\t\toccupied(ca10a11);\n\t\toccupied(ca13a10);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2079, "end_char_idx": 3280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcd40d9e-1ffb-413a-a322-3942959ae138": {"__data__": {"id_": "dcd40d9e-1ffb-413a-a322-3942959ae138", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "b29187684c9b799a3795c53db9cb8ee281109baa8cd3c5de222a7b8eca231909", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3974065c-d586-45f5-9973-c393091b3fa8", "node_type": "1", "metadata": {}, "hash": "b0009615831c1a668c7ef6c34f4963283aa775817bab35118b4d676ac24c89bc", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__6 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia5a5,ia5a10,ia10a5,ia10a10};\n\t\tcell : {ca5a1,ca1a5,ca5a2,ca2a5,ca5a3,ca3a5,ca5a4,ca4a5,ca5a6,ca6a5,ca5a7,ca7a5,ca5a8,ca8a5,ca5a9,ca9a5,ca5a11,ca11a5,ca5a12,ca12a5,ca5a13,ca13a5,ca5a14,ca14a5,ca10a1,ca1a10,ca10a2,ca2a10,ca10a3,ca3a10,ca10a4,ca4a10,ca10a6,ca6a10,ca10a7,ca7a10,ca10a8,ca8a10,ca10a9,ca9a10,ca10a11,ca11a10,ca10a12,ca12a10,ca10a13,ca13a10,ca10a14,ca14a10};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca5a1);\n\t\tPERIMETER-INPUT-CELL(ca10a1);\n\t\tPERIMETER-INPUT-CELL(ca1a5);\n\t\tPERIMETER-INPUT-CELL(ca1a10);\n\n\t\tPERIMETER-INPUT-RATE(ca5a1) = 0.14510834;\n\t\tPERIMETER-INPUT-RATE(ca10a1) = 0.38991386;\n\t\tPERIMETER-INPUT-RATE(ca1a5) = 0.25346324;\n\t\tPERIMETER-INPUT-RATE(ca1a10) = 0.20154408;\n\n\t\tPERIMETER-EXIT-CELL(ca5a14);\n\t\tPERIMETER-EXIT-CELL(ca10a14);\n\t\tPERIMETER-EXIT-CELL(ca14a5);\n\t\tPERIMETER-EXIT-CELL(ca14a10);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca5a4,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca5a9,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca10a4,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca10a9,ia10a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a5,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a5,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a10,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a10,ia10a10);\n\n\t\tFLOWS-INTO-CELL(ca5a1,ca5a2);\n\t\tFLOWS-INTO-CELL(ca1a5,ca2a5);\n\t\tFLOWS-INTO-CELL(ca5a2,ca5a3);\n\t\tFLOWS-INTO-CELL(ca2a5,ca3a5);\n\t\tFLOWS-INTO-CELL(ca5a3,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3974065c-d586-45f5-9973-c393091b3fa8": {"__data__": {"id_": "3974065c-d586-45f5-9973-c393091b3fa8", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "b29187684c9b799a3795c53db9cb8ee281109baa8cd3c5de222a7b8eca231909", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcd40d9e-1ffb-413a-a322-3942959ae138", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "7ebafe0f1281e1de595fe37abf1bcacad7253a9871a10aad04256ff660cc481b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f54352d-70a9-4611-b913-392e982160c0", "node_type": "1", "metadata": {}, "hash": "19e8c9c3af181930849dadbeae618e291fb992b5e28161747f536923b45a7f87", "class_name": "RelatedNodeInfo"}}, "text": "ia10a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a5,ia5a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a5,ia10a5);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca4a10,ia5a10);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca9a10,ia10a10);\n\n\t\tFLOWS-INTO-CELL(ca5a1,ca5a2);\n\t\tFLOWS-INTO-CELL(ca1a5,ca2a5);\n\t\tFLOWS-INTO-CELL(ca5a2,ca5a3);\n\t\tFLOWS-INTO-CELL(ca2a5,ca3a5);\n\t\tFLOWS-INTO-CELL(ca5a3,ca5a4);\n\t\tFLOWS-INTO-CELL(ca3a5,ca4a5);\n\t\tFLOWS-INTO-CELL(ca5a4,ca5a6);\n\t\tFLOWS-INTO-CELL(ca4a5,ca6a5);\n\t\tFLOWS-INTO-CELL(ca5a6,ca5a7);\n\t\tFLOWS-INTO-CELL(ca6a5,ca7a5);\n\t\tFLOWS-INTO-CELL(ca5a7,ca5a8);\n\t\tFLOWS-INTO-CELL(ca7a5,ca8a5);\n\t\tFLOWS-INTO-CELL(ca5a8,ca5a9);\n\t\tFLOWS-INTO-CELL(ca8a5,ca9a5);\n\t\tFLOWS-INTO-CELL(ca5a9,ca5a11);\n\t\tFLOWS-INTO-CELL(ca9a5,ca11a5);\n\t\tFLOWS-INTO-CELL(ca5a11,ca5a12);\n\t\tFLOWS-INTO-CELL(ca11a5,ca12a5);\n\t\tFLOWS-INTO-CELL(ca5a12,ca5a13);\n\t\tFLOWS-INTO-CELL(ca12a5,ca13a5);\n\t\tFLOWS-INTO-CELL(ca5a13,ca5a14);\n\t\tFLOWS-INTO-CELL(ca13a5,ca14a5);\n\t\tFLOWS-INTO-CELL(ca10a1,ca10a2);\n\t\tFLOWS-INTO-CELL(ca1a10,ca2a10);\n\t\tFLOWS-INTO-CELL(ca10a2,ca10a3);\n\t\tFLOWS-INTO-CELL(ca2a10,ca3a10);\n\t\tFLOWS-INTO-CELL(ca10a3,ca10a4);\n\t\tFLOWS-INTO-CELL(ca3a10,ca4a10);\n\t\tFLOWS-INTO-CELL(ca10a4,ca10a6);\n\t\tFLOWS-INTO-CELL(ca4a10,ca6a10);\n\t\tFLOWS-INTO-CELL(ca10a6,ca10a7);\n\t\tFLOWS-INTO-CELL(ca6a10,ca7a10);\n\t\tFLOWS-INTO-CELL(ca10a7,ca10a8);\n\t\tFLOWS-INTO-CELL(ca7a10,ca8a10);\n\t\tFLOWS-INTO-CELL(ca10a8,", "mimetype": "text/plain", "start_char_idx": 1080, "end_char_idx": 2420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f54352d-70a9-4611-b913-392e982160c0": {"__data__": {"id_": "4f54352d-70a9-4611-b913-392e982160c0", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6df731ef-d2b7-4d19-9a5d-228650b089d4", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "b29187684c9b799a3795c53db9cb8ee281109baa8cd3c5de222a7b8eca231909", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3974065c-d586-45f5-9973-c393091b3fa8", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "4b0df29de66d6e50c035323fcdda7e46b0397a4622354b6d4927132a2f0e2ffc", "class_name": "RelatedNodeInfo"}}, "text": "ca10a3);\n\t\tFLOWS-INTO-CELL(ca2a10,ca3a10);\n\t\tFLOWS-INTO-CELL(ca10a3,ca10a4);\n\t\tFLOWS-INTO-CELL(ca3a10,ca4a10);\n\t\tFLOWS-INTO-CELL(ca10a4,ca10a6);\n\t\tFLOWS-INTO-CELL(ca4a10,ca6a10);\n\t\tFLOWS-INTO-CELL(ca10a6,ca10a7);\n\t\tFLOWS-INTO-CELL(ca6a10,ca7a10);\n\t\tFLOWS-INTO-CELL(ca10a7,ca10a8);\n\t\tFLOWS-INTO-CELL(ca7a10,ca8a10);\n\t\tFLOWS-INTO-CELL(ca10a8,ca10a9);\n\t\tFLOWS-INTO-CELL(ca8a10,ca9a10);\n\t\tFLOWS-INTO-CELL(ca10a9,ca10a11);\n\t\tFLOWS-INTO-CELL(ca9a10,ca11a10);\n\t\tFLOWS-INTO-CELL(ca10a11,ca10a12);\n\t\tFLOWS-INTO-CELL(ca11a10,ca12a10);\n\t\tFLOWS-INTO-CELL(ca10a12,ca10a13);\n\t\tFLOWS-INTO-CELL(ca12a10,ca13a10);\n\t\tFLOWS-INTO-CELL(ca10a13,ca10a14);\n\t\tFLOWS-INTO-CELL(ca13a10,ca14a10);\n\t};\n}\n\ninstance traffic_inst_mdp__6 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__6;\n\tinit-state {\n\t\toccupied(ca1a5);\n\t\toccupied(ca5a2);\n\t\toccupied(ca5a3);\n\t\toccupied(ca5a4);\n\t\toccupied(ca11a5);\n\t\toccupied(ca5a12);\n\t\toccupied(ca5a13);\n\t\toccupied(ca5a14);\n\t\toccupied(ca2a10);\n\t\toccupied(ca10a3);\n\t\toccupied(ca4a10);\n\t\toccupied(ca10a7);\n\t\toccupied(ca7a10);\n\t\toccupied(ca8a10);\n\t\toccupied(ca10a9);\n\t\toccupied(ca12a10);\n\t\toccupied(ca14a10);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 2080, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0076127e-a61a-4746-9a49-a6a3165dd4c1": {"__data__": {"id_": "0076127e-a61a-4746-9a49-a6a3165dd4c1", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88627a8d-7413-4d6e-98a5-abec56874b3d", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "1e96a8ecf61fb925c7ece6491c67b39aa32de3e4b7bac1a91cdd2f951866215e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d8d73a2-270a-48d5-8b39-7102d1eb81fc", "node_type": "1", "metadata": {}, "hash": "720c5b5e2ff5d14584af96a541324943f7a65028d22a3f16568b9815f45e36b7", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__7 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia6a6,ia6a12,ia12a6,ia12a12};\n\t\tcell : {ca6a1,ca1a6,ca6a2,ca2a6,ca6a3,ca3a6,ca6a4,ca4a6,ca6a5,ca5a6,ca6a7,ca7a6,ca6a8,ca8a6,ca6a9,ca9a6,ca6a10,ca10a6,ca6a11,ca11a6,ca6a13,ca13a6,ca6a14,ca14a6,ca6a15,ca15a6,ca6a16,ca16a6,ca6a17,ca17a6,ca12a1,ca1a12,ca12a2,ca2a12,ca12a3,ca3a12,ca12a4,ca4a12,ca12a5,ca5a12,ca12a7,ca7a12,ca12a8,ca8a12,ca12a9,ca9a12,ca12a10,ca10a12,ca12a11,ca11a12,ca12a13,ca13a12,ca12a14,ca14a12,ca12a15,ca15a12,ca12a16,ca16a12,ca12a17,ca17a12};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca6a1);\n\t\tPERIMETER-INPUT-CELL(ca12a1);\n\t\tPERIMETER-INPUT-CELL(ca1a6);\n\t\tPERIMETER-INPUT-CELL(ca1a12);\n\n\t\tPERIMETER-INPUT-RATE(ca6a1) = 0.115196005;\n\t\tPERIMETER-INPUT-RATE(ca12a1) = 0.18195823;\n\t\tPERIMETER-INPUT-RATE(ca1a6) = 0.19617182;\n\t\tPERIMETER-INPUT-RATE(ca1a12) = 0.22611672;\n\n\t\tPERIMETER-EXIT-CELL(ca6a17);\n\t\tPERIMETER-EXIT-CELL(ca12a17);\n\t\tPERIMETER-EXIT-CELL(ca17a6);\n\t\tPERIMETER-EXIT-CELL(ca17a12);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a5,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a11,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a5,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a11,ia12a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a6,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a12,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a12,ia12a12);\n\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d8d73a2-270a-48d5-8b39-7102d1eb81fc": {"__data__": {"id_": "0d8d73a2-270a-48d5-8b39-7102d1eb81fc", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88627a8d-7413-4d6e-98a5-abec56874b3d", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "1e96a8ecf61fb925c7ece6491c67b39aa32de3e4b7bac1a91cdd2f951866215e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0076127e-a61a-4746-9a49-a6a3165dd4c1", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d18bb3e1c41c905545440327713fef4505b4d045c6adf316a11ef741a36d6466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff855896-fac4-4b18-bd18-1345ae2f2f15", "node_type": "1", "metadata": {}, "hash": "0426f79a2bb016cdebf7a14589da24bbc2bb7dc9b45246daba1fc7862aaf85b9", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-INTERSECTION-EW(ca6a11,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a5,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a11,ia12a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a6,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a12,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a12,ia12a12);\n\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,ca2a6);\n\t\tFLOWS-INTO-CELL(ca6a2,ca6a3);\n\t\tFLOWS-INTO-CELL(ca2a6,ca3a6);\n\t\tFLOWS-INTO-CELL(ca6a3,ca6a4);\n\t\tFLOWS-INTO-CELL(ca3a6,ca4a6);\n\t\tFLOWS-INTO-CELL(ca6a4,ca6a5);\n\t\tFLOWS-INTO-CELL(ca4a6,ca5a6);\n\t\tFLOWS-INTO-CELL(ca6a5,ca6a7);\n\t\tFLOWS-INTO-CELL(ca5a6,ca7a6);\n\t\tFLOWS-INTO-CELL(ca6a7,ca6a8);\n\t\tFLOWS-INTO-CELL(ca7a6,ca8a6);\n\t\tFLOWS-INTO-CELL(ca6a8,ca6a9);\n\t\tFLOWS-INTO-CELL(ca8a6,ca9a6);\n\t\tFLOWS-INTO-CELL(ca6a9,ca6a10);\n\t\tFLOWS-INTO-CELL(ca9a6,ca10a6);\n\t\tFLOWS-INTO-CELL(ca6a10,ca6a11);\n\t\tFLOWS-INTO-CELL(ca10a6,ca11a6);\n\t\tFLOWS-INTO-CELL(ca6a11,ca6a13);\n\t\tFLOWS-INTO-CELL(ca11a6,ca13a6);\n\t\tFLOWS-INTO-CELL(ca6a13,ca6a14);\n\t\tFLOWS-INTO-CELL(ca13a6,ca14a6);\n\t\tFLOWS-INTO-CELL(ca6a14,ca6a15);\n\t\tFLOWS-INTO-CELL(ca14a6,ca15a6);\n\t\tFLOWS-INTO-CELL(ca6a15,ca6a16);\n\t\tFLOWS-INTO-CELL(ca15a6,ca16a6);\n\t\tFLOWS-INTO-CELL(ca6a16,ca6a17);\n\t\tFLOWS-INTO-CELL(ca16a6,ca17a6);\n\t\tFLOWS-INTO-CELL(ca12a1,ca12a2);\n\t\tFLOWS-INTO-CELL(ca1a12,ca2a12);\n\t\tFLOWS-INTO-CELL(ca12a2,ca12a3);", "mimetype": "text/plain", "start_char_idx": 1048, "end_char_idx": 2387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff855896-fac4-4b18-bd18-1345ae2f2f15": {"__data__": {"id_": "ff855896-fac4-4b18-bd18-1345ae2f2f15", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88627a8d-7413-4d6e-98a5-abec56874b3d", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "1e96a8ecf61fb925c7ece6491c67b39aa32de3e4b7bac1a91cdd2f951866215e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d8d73a2-270a-48d5-8b39-7102d1eb81fc", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "cd99ae05ad66aeb32a168a4c5aa9e2baeb9dd6006fd0d9fb98cd96500e14b9a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67026b4b-9a29-4ee6-9f9a-a9225bfc2905", "node_type": "1", "metadata": {}, "hash": "a4754e17c6fae2d2eb643250a993295b60815a00e381460097ccb43981485085", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-CELL(ca13a6,ca14a6);\n\t\tFLOWS-INTO-CELL(ca6a14,ca6a15);\n\t\tFLOWS-INTO-CELL(ca14a6,ca15a6);\n\t\tFLOWS-INTO-CELL(ca6a15,ca6a16);\n\t\tFLOWS-INTO-CELL(ca15a6,ca16a6);\n\t\tFLOWS-INTO-CELL(ca6a16,ca6a17);\n\t\tFLOWS-INTO-CELL(ca16a6,ca17a6);\n\t\tFLOWS-INTO-CELL(ca12a1,ca12a2);\n\t\tFLOWS-INTO-CELL(ca1a12,ca2a12);\n\t\tFLOWS-INTO-CELL(ca12a2,ca12a3);\n\t\tFLOWS-INTO-CELL(ca2a12,ca3a12);\n\t\tFLOWS-INTO-CELL(ca12a3,ca12a4);\n\t\tFLOWS-INTO-CELL(ca3a12,ca4a12);\n\t\tFLOWS-INTO-CELL(ca12a4,ca12a5);\n\t\tFLOWS-INTO-CELL(ca4a12,ca5a12);\n\t\tFLOWS-INTO-CELL(ca12a5,ca12a7);\n\t\tFLOWS-INTO-CELL(ca5a12,ca7a12);\n\t\tFLOWS-INTO-CELL(ca12a7,ca12a8);\n\t\tFLOWS-INTO-CELL(ca7a12,ca8a12);\n\t\tFLOWS-INTO-CELL(ca12a8,ca12a9);\n\t\tFLOWS-INTO-CELL(ca8a12,ca9a12);\n\t\tFLOWS-INTO-CELL(ca12a9,ca12a10);\n\t\tFLOWS-INTO-CELL(ca9a12,ca10a12);\n\t\tFLOWS-INTO-CELL(ca12a10,ca12a11);\n\t\tFLOWS-INTO-CELL(ca10a12,ca11a12);\n\t\tFLOWS-INTO-CELL(ca12a11,ca12a13);\n\t\tFLOWS-INTO-CELL(ca11a12,ca13a12);\n\t\tFLOWS-INTO-CELL(ca12a13,ca12a14);\n\t\tFLOWS-INTO-CELL(ca13a12,ca14a12);\n\t\tFLOWS-INTO-CELL(ca12a14,ca12a15);\n\t\tFLOWS-INTO-CELL(ca14a12,ca15a12);\n\t\tFLOWS-INTO-CELL(ca12a15,ca12a16);\n\t\tFLOWS-INTO-CELL(ca15a12,ca16a12);\n\t\tFLOWS-INTO-CELL(ca12a16,ca12a17);\n\t\tFLOWS-INTO-CELL(ca16a12,ca17a12);\n\t};\n}\n\ninstance traffic_inst_mdp__7 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__7;\n\tinit-state {\n\t\toccupied(ca6a1);\n\t\toccupied(ca4a6);\n\t\toccupied(ca5a6);\n\t\toccupied(ca6a7);\n\t\toccupied(ca6a8);\n\t\toccupied(ca6a9);", "mimetype": "text/plain", "start_char_idx": 2050, "end_char_idx": 3491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67026b4b-9a29-4ee6-9f9a-a9225bfc2905": {"__data__": {"id_": "67026b4b-9a29-4ee6-9f9a-a9225bfc2905", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88627a8d-7413-4d6e-98a5-abec56874b3d", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "1e96a8ecf61fb925c7ece6491c67b39aa32de3e4b7bac1a91cdd2f951866215e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff855896-fac4-4b18-bd18-1345ae2f2f15", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "d2f6f2a6aa057bb0611c25eb36c53abefceff9719661dfa12cc3b08b8b34e750", "class_name": "RelatedNodeInfo"}}, "text": "ca12a15);\n\t\tFLOWS-INTO-CELL(ca14a12,ca15a12);\n\t\tFLOWS-INTO-CELL(ca12a15,ca12a16);\n\t\tFLOWS-INTO-CELL(ca15a12,ca16a12);\n\t\tFLOWS-INTO-CELL(ca12a16,ca12a17);\n\t\tFLOWS-INTO-CELL(ca16a12,ca17a12);\n\t};\n}\n\ninstance traffic_inst_mdp__7 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__7;\n\tinit-state {\n\t\toccupied(ca6a1);\n\t\toccupied(ca4a6);\n\t\toccupied(ca5a6);\n\t\toccupied(ca6a7);\n\t\toccupied(ca6a8);\n\t\toccupied(ca6a9);\n\t\toccupied(ca6a13);\n\t\toccupied(ca6a14);\n\t\toccupied(ca1a12);\n\t\toccupied(ca12a2);\n\t\toccupied(ca3a12);\n\t\toccupied(ca4a12);\n\t\toccupied(ca5a12);\n\t\toccupied(ca7a12);\n\t\toccupied(ca9a12);\n\t\toccupied(ca13a12);\n\t\toccupied(ca14a12);\n\t\toccupied(ca17a12);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 3074, "end_char_idx": 3799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312c279d-ded8-4747-9202-05446d614c2d": {"__data__": {"id_": "312c279d-ded8-4747-9202-05446d614c2d", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0c673820765bc53ac84768f0f86f6a615810d7f61dcbc79176e4102a4dcded30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25ed184e-4b92-4430-bec7-bfe9eea868a8", "node_type": "1", "metadata": {}, "hash": "720c5b5e2ff5d14584af96a541324943f7a65028d22a3f16568b9815f45e36b7", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__8 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia6a6,ia6a12,ia12a6,ia12a12};\n\t\tcell : {ca6a1,ca1a6,ca6a2,ca2a6,ca6a3,ca3a6,ca6a4,ca4a6,ca6a5,ca5a6,ca6a7,ca7a6,ca6a8,ca8a6,ca6a9,ca9a6,ca6a10,ca10a6,ca6a11,ca11a6,ca6a13,ca13a6,ca6a14,ca14a6,ca6a15,ca15a6,ca6a16,ca16a6,ca6a17,ca17a6,ca12a1,ca1a12,ca12a2,ca2a12,ca12a3,ca3a12,ca12a4,ca4a12,ca12a5,ca5a12,ca12a7,ca7a12,ca12a8,ca8a12,ca12a9,ca9a12,ca12a10,ca10a12,ca12a11,ca11a12,ca12a13,ca13a12,ca12a14,ca14a12,ca12a15,ca15a12,ca12a16,ca16a12,ca12a17,ca17a12};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca6a1);\n\t\tPERIMETER-INPUT-CELL(ca12a1);\n\t\tPERIMETER-INPUT-CELL(ca1a6);\n\t\tPERIMETER-INPUT-CELL(ca1a12);\n\n\t\tPERIMETER-INPUT-RATE(ca6a1) = 0.36422506;\n\t\tPERIMETER-INPUT-RATE(ca12a1) = 0.2814253;\n\t\tPERIMETER-INPUT-RATE(ca1a6) = 0.107539035;\n\t\tPERIMETER-INPUT-RATE(ca1a12) = 0.39031354;\n\n\t\tPERIMETER-EXIT-CELL(ca6a17);\n\t\tPERIMETER-EXIT-CELL(ca12a17);\n\t\tPERIMETER-EXIT-CELL(ca17a6);\n\t\tPERIMETER-EXIT-CELL(ca17a12);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a5,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca6a11,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a5,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a11,ia12a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a6,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a12,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a12,ia12a12);\n\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25ed184e-4b92-4430-bec7-bfe9eea868a8": {"__data__": {"id_": "25ed184e-4b92-4430-bec7-bfe9eea868a8", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0c673820765bc53ac84768f0f86f6a615810d7f61dcbc79176e4102a4dcded30", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312c279d-ded8-4747-9202-05446d614c2d", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "73fc7f1dee9997973b565b0e114de3db8a6d9aaa5c74acc87945cc3c32e133ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17a23ce6-88ff-4ef8-8df4-6b083d6a825f", "node_type": "1", "metadata": {}, "hash": "963d27611c688d4c6071f4e1afce1bffaf64744a7e967e0dce46634e6a6ea280", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-INTERSECTION-EW(ca6a11,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a5,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca12a11,ia12a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a6,ia6a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a6,ia12a6);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca5a12,ia6a12);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca11a12,ia12a12);\n\n\t\tFLOWS-INTO-CELL(ca6a1,ca6a2);\n\t\tFLOWS-INTO-CELL(ca1a6,ca2a6);\n\t\tFLOWS-INTO-CELL(ca6a2,ca6a3);\n\t\tFLOWS-INTO-CELL(ca2a6,ca3a6);\n\t\tFLOWS-INTO-CELL(ca6a3,ca6a4);\n\t\tFLOWS-INTO-CELL(ca3a6,ca4a6);\n\t\tFLOWS-INTO-CELL(ca6a4,ca6a5);\n\t\tFLOWS-INTO-CELL(ca4a6,ca5a6);\n\t\tFLOWS-INTO-CELL(ca6a5,ca6a7);\n\t\tFLOWS-INTO-CELL(ca5a6,ca7a6);\n\t\tFLOWS-INTO-CELL(ca6a7,ca6a8);\n\t\tFLOWS-INTO-CELL(ca7a6,ca8a6);\n\t\tFLOWS-INTO-CELL(ca6a8,ca6a9);\n\t\tFLOWS-INTO-CELL(ca8a6,ca9a6);\n\t\tFLOWS-INTO-CELL(ca6a9,ca6a10);\n\t\tFLOWS-INTO-CELL(ca9a6,ca10a6);\n\t\tFLOWS-INTO-CELL(ca6a10,ca6a11);\n\t\tFLOWS-INTO-CELL(ca10a6,ca11a6);\n\t\tFLOWS-INTO-CELL(ca6a11,ca6a13);\n\t\tFLOWS-INTO-CELL(ca11a6,ca13a6);\n\t\tFLOWS-INTO-CELL(ca6a13,ca6a14);\n\t\tFLOWS-INTO-CELL(ca13a6,ca14a6);\n\t\tFLOWS-INTO-CELL(ca6a14,ca6a15);\n\t\tFLOWS-INTO-CELL(ca14a6,ca15a6);\n\t\tFLOWS-INTO-CELL(ca6a15,ca6a16);\n\t\tFLOWS-INTO-CELL(ca15a6,ca16a6);\n\t\tFLOWS-INTO-CELL(ca6a16,ca6a17);\n\t\tFLOWS-INTO-CELL(ca16a6,ca17a6);\n\t\tFLOWS-INTO-CELL(ca12a1,ca12a2);\n\t\tFLOWS-INTO-CELL(ca1a12,ca2a12);\n\t\tFLOWS-INTO-CELL(ca12a2,ca12a3);", "mimetype": "text/plain", "start_char_idx": 1047, "end_char_idx": 2386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17a23ce6-88ff-4ef8-8df4-6b083d6a825f": {"__data__": {"id_": "17a23ce6-88ff-4ef8-8df4-6b083d6a825f", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0c673820765bc53ac84768f0f86f6a615810d7f61dcbc79176e4102a4dcded30", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25ed184e-4b92-4430-bec7-bfe9eea868a8", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "cd99ae05ad66aeb32a168a4c5aa9e2baeb9dd6006fd0d9fb98cd96500e14b9a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "715490fc-5e71-4eb1-a421-cd770317c340", "node_type": "1", "metadata": {}, "hash": "ed285ce637265ff1313ff7de46efac43deb2fa6f6973bdd37bff6490390ac0a8", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-CELL(ca13a6,ca14a6);\n\t\tFLOWS-INTO-CELL(ca6a14,ca6a15);\n\t\tFLOWS-INTO-CELL(ca14a6,ca15a6);\n\t\tFLOWS-INTO-CELL(ca6a15,ca6a16);\n\t\tFLOWS-INTO-CELL(ca15a6,ca16a6);\n\t\tFLOWS-INTO-CELL(ca6a16,ca6a17);\n\t\tFLOWS-INTO-CELL(ca16a6,ca17a6);\n\t\tFLOWS-INTO-CELL(ca12a1,ca12a2);\n\t\tFLOWS-INTO-CELL(ca1a12,ca2a12);\n\t\tFLOWS-INTO-CELL(ca12a2,ca12a3);\n\t\tFLOWS-INTO-CELL(ca2a12,ca3a12);\n\t\tFLOWS-INTO-CELL(ca12a3,ca12a4);\n\t\tFLOWS-INTO-CELL(ca3a12,ca4a12);\n\t\tFLOWS-INTO-CELL(ca12a4,ca12a5);\n\t\tFLOWS-INTO-CELL(ca4a12,ca5a12);\n\t\tFLOWS-INTO-CELL(ca12a5,ca12a7);\n\t\tFLOWS-INTO-CELL(ca5a12,ca7a12);\n\t\tFLOWS-INTO-CELL(ca12a7,ca12a8);\n\t\tFLOWS-INTO-CELL(ca7a12,ca8a12);\n\t\tFLOWS-INTO-CELL(ca12a8,ca12a9);\n\t\tFLOWS-INTO-CELL(ca8a12,ca9a12);\n\t\tFLOWS-INTO-CELL(ca12a9,ca12a10);\n\t\tFLOWS-INTO-CELL(ca9a12,ca10a12);\n\t\tFLOWS-INTO-CELL(ca12a10,ca12a11);\n\t\tFLOWS-INTO-CELL(ca10a12,ca11a12);\n\t\tFLOWS-INTO-CELL(ca12a11,ca12a13);\n\t\tFLOWS-INTO-CELL(ca11a12,ca13a12);\n\t\tFLOWS-INTO-CELL(ca12a13,ca12a14);\n\t\tFLOWS-INTO-CELL(ca13a12,ca14a12);\n\t\tFLOWS-INTO-CELL(ca12a14,ca12a15);\n\t\tFLOWS-INTO-CELL(ca14a12,ca15a12);\n\t\tFLOWS-INTO-CELL(ca12a15,ca12a16);\n\t\tFLOWS-INTO-CELL(ca15a12,ca16a12);\n\t\tFLOWS-INTO-CELL(ca12a16,ca12a17);\n\t\tFLOWS-INTO-CELL(ca16a12,ca17a12);\n\t};\n}\n\ninstance traffic_inst_mdp__8 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__8;\n\tinit-state {\n\t\toccupied(ca6a1);\n\t\toccupied(ca6a5);\n\t\toccupied(ca6a8);\n\t\toccupied(ca6a11);\n\t\toccupied(ca6a14);\n\t\toccupied(ca15a6);", "mimetype": "text/plain", "start_char_idx": 2049, "end_char_idx": 3493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "715490fc-5e71-4eb1-a421-cd770317c340": {"__data__": {"id_": "715490fc-5e71-4eb1-a421-cd770317c340", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8553a56e-f227-4bb6-9150-43d1dd6ebcba", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "0c673820765bc53ac84768f0f86f6a615810d7f61dcbc79176e4102a4dcded30", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17a23ce6-88ff-4ef8-8df4-6b083d6a825f", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6d0fd258be6d73ead73dd58f4596fd9a14069ba1bf1b873bb7434a3c334ab082", "class_name": "RelatedNodeInfo"}}, "text": "ca12a15);\n\t\tFLOWS-INTO-CELL(ca14a12,ca15a12);\n\t\tFLOWS-INTO-CELL(ca12a15,ca12a16);\n\t\tFLOWS-INTO-CELL(ca15a12,ca16a12);\n\t\tFLOWS-INTO-CELL(ca12a16,ca12a17);\n\t\tFLOWS-INTO-CELL(ca16a12,ca17a12);\n\t};\n}\n\ninstance traffic_inst_mdp__8 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__8;\n\tinit-state {\n\t\toccupied(ca6a1);\n\t\toccupied(ca6a5);\n\t\toccupied(ca6a8);\n\t\toccupied(ca6a11);\n\t\toccupied(ca6a14);\n\t\toccupied(ca15a6);\n\t\toccupied(ca6a16);\n\t\toccupied(ca16a6);\n\t\toccupied(ca6a17);\n\t\toccupied(ca12a3);\n\t\toccupied(ca7a12);\n\t\toccupied(ca12a10);\n\t\toccupied(ca11a12);\n\t\toccupied(ca12a14);\n\t\toccupied(ca12a16);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 3073, "end_char_idx": 3742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e1761b3-ffce-4e80-9890-147a2c920c2d": {"__data__": {"id_": "3e1761b3-ffce-4e80-9890-147a2c920c2d", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "fdd8d66e2b93af1f5fef746d2aa43e8ce3a7fa09a3203cfc63f13d42dac84e99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0", "node_type": "1", "metadata": {}, "hash": "a655a792a9500047c650db9b11afc40a6897b34629a29224517ce52da53b18f9", "class_name": "RelatedNodeInfo"}}, "text": "non-fluents nf_traffic_inst_mdp__9 {\n\tdomain = traffic_mdp;\n\tobjects {\n\t\tintersection : {ia7a7,ia7a14,ia14a7,ia14a14};\n\t\tcell : {ca7a1,ca1a7,ca7a2,ca2a7,ca7a3,ca3a7,ca7a4,ca4a7,ca7a5,ca5a7,ca7a6,ca6a7,ca7a8,ca8a7,ca7a9,ca9a7,ca7a10,ca10a7,ca7a11,ca11a7,ca7a12,ca12a7,ca7a13,ca13a7,ca7a15,ca15a7,ca7a16,ca16a7,ca7a17,ca17a7,ca7a18,ca18a7,ca7a19,ca19a7,ca7a20,ca20a7,ca14a1,ca1a14,ca14a2,ca2a14,ca14a3,ca3a14,ca14a4,ca4a14,ca14a5,ca5a14,ca14a6,ca6a14,ca14a8,ca8a14,ca14a9,ca9a14,ca14a10,ca10a14,ca14a11,ca11a14,ca14a12,ca12a14,ca14a13,ca13a14,ca14a15,ca15a14,ca14a16,ca16a14,ca14a17,ca17a14,ca14a18,ca18a14,ca14a19,ca19a14,ca14a20,ca20a14};\n\t};\n\tnon-fluents {\n\n\t\tPERIMETER-INPUT-CELL(ca7a1);\n\t\tPERIMETER-INPUT-CELL(ca14a1);\n\t\tPERIMETER-INPUT-CELL(ca1a7);\n\t\tPERIMETER-INPUT-CELL(ca1a14);\n\n\t\tPERIMETER-INPUT-RATE(ca7a1) = 0.26824513;\n\t\tPERIMETER-INPUT-RATE(ca14a1) = 0.12978587;\n\t\tPERIMETER-INPUT-RATE(ca1a7) = 0.17321071;\n\t\tPERIMETER-INPUT-RATE(ca1a14) = 0.2657741;\n\n\t\tPERIMETER-EXIT-CELL(ca7a20);\n\t\tPERIMETER-EXIT-CELL(ca14a20);\n\t\tPERIMETER-EXIT-CELL(ca20a7);\n\t\tPERIMETER-EXIT-CELL(ca20a14);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a6,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a13,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a6,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a13,ia14a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a7,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a7,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a14,ia7a14);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0": {"__data__": {"id_": "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "fdd8d66e2b93af1f5fef746d2aa43e8ce3a7fa09a3203cfc63f13d42dac84e99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e1761b3-ffce-4e80-9890-147a2c920c2d", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "212c00abc1cacf34fe20407d095be8c84c199b12075a194d7212f0034f083bd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad03d4b4-459c-42ca-8dac-aa586bede9fd", "node_type": "1", "metadata": {}, "hash": "ea0235e6507c4fb1ccad3a5902d931ba172aae641a378cf0da51af2ccbbe4bcc", "class_name": "RelatedNodeInfo"}}, "text": "PERIMETER-EXIT-CELL(ca20a7);\n\t\tPERIMETER-EXIT-CELL(ca20a14);\n\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a6,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca7a13,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a6,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-EW(ca14a13,ia14a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a7,ia7a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a7,ia14a7);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca6a14,ia7a14);\n\t\tFLOWS-INTO-INTERSECTION-NS(ca13a14,ia14a14);\n\n\t\tFLOWS-INTO-CELL(ca7a1,ca7a2);\n\t\tFLOWS-INTO-CELL(ca1a7,ca2a7);\n\t\tFLOWS-INTO-CELL(ca7a2,ca7a3);\n\t\tFLOWS-INTO-CELL(ca2a7,ca3a7);\n\t\tFLOWS-INTO-CELL(ca7a3,ca7a4);\n\t\tFLOWS-INTO-CELL(ca3a7,ca4a7);\n\t\tFLOWS-INTO-CELL(ca7a4,ca7a5);\n\t\tFLOWS-INTO-CELL(ca4a7,ca5a7);\n\t\tFLOWS-INTO-CELL(ca7a5,ca7a6);\n\t\tFLOWS-INTO-CELL(ca5a7,ca6a7);\n\t\tFLOWS-INTO-CELL(ca7a6,ca7a8);\n\t\tFLOWS-INTO-CELL(ca6a7,ca8a7);\n\t\tFLOWS-INTO-CELL(ca7a8,ca7a9);\n\t\tFLOWS-INTO-CELL(ca8a7,ca9a7);\n\t\tFLOWS-INTO-CELL(ca7a9,ca7a10);\n\t\tFLOWS-INTO-CELL(ca9a7,ca10a7);\n\t\tFLOWS-INTO-CELL(ca7a10,ca7a11);\n\t\tFLOWS-INTO-CELL(ca10a7,ca11a7);\n\t\tFLOWS-INTO-CELL(ca7a11,ca7a12);\n\t\tFLOWS-INTO-CELL(ca11a7,ca12a7);\n\t\tFLOWS-INTO-CELL(ca7a12,ca7a13);\n\t\tFLOWS-INTO-CELL(ca12a7,ca13a7);\n\t\tFLOWS-INTO-CELL(ca7a13,ca7a15);\n\t\tFLOWS-INTO-CELL(ca13a7,ca15a7);\n\t\tFLOWS-INTO-CELL(ca7a15,ca7a16);\n\t\tFLOWS-INTO-CELL(ca15a7,ca16a7);\n\t\tFLOWS-INTO-CELL(ca7a16,ca7a17);\n\t\tFLOWS-INTO-CELL(ca16a7,ca17a7);\n\t\tFLOWS-INTO-CELL(ca7a17,ca7a18);", "mimetype": "text/plain", "start_char_idx": 1029, "end_char_idx": 2407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad03d4b4-459c-42ca-8dac-aa586bede9fd": {"__data__": {"id_": "ad03d4b4-459c-42ca-8dac-aa586bede9fd", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "fdd8d66e2b93af1f5fef746d2aa43e8ce3a7fa09a3203cfc63f13d42dac84e99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "eafcef60dab93995aeceaa736c7d574e6c0eaadd3e2376a1c4998f5e407e048c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2d3db21-d2f0-4ea6-87da-7743ac38e862", "node_type": "1", "metadata": {}, "hash": "c463064df92dd905309a8663a19da8c3a9a66dc5641e742c76c21a41668be2ba", "class_name": "RelatedNodeInfo"}}, "text": "FLOWS-INTO-CELL(ca11a7,ca12a7);\n\t\tFLOWS-INTO-CELL(ca7a12,ca7a13);\n\t\tFLOWS-INTO-CELL(ca12a7,ca13a7);\n\t\tFLOWS-INTO-CELL(ca7a13,ca7a15);\n\t\tFLOWS-INTO-CELL(ca13a7,ca15a7);\n\t\tFLOWS-INTO-CELL(ca7a15,ca7a16);\n\t\tFLOWS-INTO-CELL(ca15a7,ca16a7);\n\t\tFLOWS-INTO-CELL(ca7a16,ca7a17);\n\t\tFLOWS-INTO-CELL(ca16a7,ca17a7);\n\t\tFLOWS-INTO-CELL(ca7a17,ca7a18);\n\t\tFLOWS-INTO-CELL(ca17a7,ca18a7);\n\t\tFLOWS-INTO-CELL(ca7a18,ca7a19);\n\t\tFLOWS-INTO-CELL(ca18a7,ca19a7);\n\t\tFLOWS-INTO-CELL(ca7a19,ca7a20);\n\t\tFLOWS-INTO-CELL(ca19a7,ca20a7);\n\t\tFLOWS-INTO-CELL(ca14a1,ca14a2);\n\t\tFLOWS-INTO-CELL(ca1a14,ca2a14);\n\t\tFLOWS-INTO-CELL(ca14a2,ca14a3);\n\t\tFLOWS-INTO-CELL(ca2a14,ca3a14);\n\t\tFLOWS-INTO-CELL(ca14a3,ca14a4);\n\t\tFLOWS-INTO-CELL(ca3a14,ca4a14);\n\t\tFLOWS-INTO-CELL(ca14a4,ca14a5);\n\t\tFLOWS-INTO-CELL(ca4a14,ca5a14);\n\t\tFLOWS-INTO-CELL(ca14a5,ca14a6);\n\t\tFLOWS-INTO-CELL(ca5a14,ca6a14);\n\t\tFLOWS-INTO-CELL(ca14a6,ca14a8);\n\t\tFLOWS-INTO-CELL(ca6a14,ca8a14);\n\t\tFLOWS-INTO-CELL(ca14a8,ca14a9);\n\t\tFLOWS-INTO-CELL(ca8a14,ca9a14);\n\t\tFLOWS-INTO-CELL(ca14a9,ca14a10);\n\t\tFLOWS-INTO-CELL(ca9a14,ca10a14);\n\t\tFLOWS-INTO-CELL(ca14a10,ca14a11);\n\t\tFLOWS-INTO-CELL(ca10a14,ca11a14);\n\t\tFLOWS-INTO-CELL(ca14a11,ca14a12);\n\t\tFLOWS-INTO-CELL(ca11a14,ca12a14);\n\t\tFLOWS-INTO-CELL(ca14a12,ca14a13);\n\t\tFLOWS-INTO-CELL(ca12a14,ca13a14);\n\t\tFLOWS-INTO-CELL(ca14a13,ca14a15);\n\t\tFLOWS-INTO-CELL(ca13a14,ca15a14);\n\t\tFLOWS-INTO-CELL(ca14a15,", "mimetype": "text/plain", "start_char_idx": 2070, "end_char_idx": 3438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2d3db21-d2f0-4ea6-87da-7743ac38e862": {"__data__": {"id_": "f2d3db21-d2f0-4ea6-87da-7743ac38e862", "embedding": null, "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc6ffe91-1fa2-4a8d-a328-db99a4b52247", "node_type": "4", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "fdd8d66e2b93af1f5fef746d2aa43e8ce3a7fa09a3203cfc63f13d42dac84e99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad03d4b4-459c-42ca-8dac-aa586bede9fd", "node_type": "1", "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}, "hash": "6c53ed490d521c410ce07b89cf787ce62a02596cfa9cf7ca5e13158bc378706f", "class_name": "RelatedNodeInfo"}}, "text": "ca14a10);\n\t\tFLOWS-INTO-CELL(ca9a14,ca10a14);\n\t\tFLOWS-INTO-CELL(ca14a10,ca14a11);\n\t\tFLOWS-INTO-CELL(ca10a14,ca11a14);\n\t\tFLOWS-INTO-CELL(ca14a11,ca14a12);\n\t\tFLOWS-INTO-CELL(ca11a14,ca12a14);\n\t\tFLOWS-INTO-CELL(ca14a12,ca14a13);\n\t\tFLOWS-INTO-CELL(ca12a14,ca13a14);\n\t\tFLOWS-INTO-CELL(ca14a13,ca14a15);\n\t\tFLOWS-INTO-CELL(ca13a14,ca15a14);\n\t\tFLOWS-INTO-CELL(ca14a15,ca14a16);\n\t\tFLOWS-INTO-CELL(ca15a14,ca16a14);\n\t\tFLOWS-INTO-CELL(ca14a16,ca14a17);\n\t\tFLOWS-INTO-CELL(ca16a14,ca17a14);\n\t\tFLOWS-INTO-CELL(ca14a17,ca14a18);\n\t\tFLOWS-INTO-CELL(ca17a14,ca18a14);\n\t\tFLOWS-INTO-CELL(ca14a18,ca14a19);\n\t\tFLOWS-INTO-CELL(ca18a14,ca19a14);\n\t\tFLOWS-INTO-CELL(ca14a19,ca14a20);\n\t\tFLOWS-INTO-CELL(ca19a14,ca20a14);\n\t};\n}\n\ninstance traffic_inst_mdp__9 {\n\tdomain = traffic_mdp;\n\tnon-fluents = nf_traffic_inst_mdp__9;\n\tinit-state {\n\t\toccupied(ca2a7);\n\t\toccupied(ca4a7);\n\t\toccupied(ca7a5);\n\t\toccupied(ca7a8);\n\t\toccupied(ca10a7);\n\t\toccupied(ca7a13);\n\t\toccupied(ca16a7);\n\t\toccupied(ca17a7);\n\t\toccupied(ca19a7);\n\t\toccupied(ca2a14);\n\t\toccupied(ca6a14);\n\t\toccupied(ca14a8);\n\t\toccupied(ca10a14);\n\t\toccupied(ca11a14);\n\t\toccupied(ca14a13);\n\t\toccupied(ca14a15);\n\t\toccupied(ca14a16);\n\t\toccupied(ca17a14);\n\t\toccupied(ca14a18);\n\t\toccupied(ca18a14);\n\t};\n\n\tmax-nondef-actions = 4;\n\thorizon  = 40;\n\tdiscount = 1.0;\n}", "mimetype": "text/plain", "start_char_idx": 3079, "end_char_idx": 4354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d28eff0a-18ab-4237-8d97-144c507fbb58": {"node_ids": ["a6d77317-c6aa-44b1-99e4-34bbd4578c52", "3fa3a030-8a05-4b6d-94c5-b64404555559", "65efbccb-84d5-4dd0-bebf-d83f486dc840", "c47579a0-ad0d-4c6c-8915-9736b526418d", "eb343a12-8086-4941-9fde-e183e8cd0eff", "bc244b08-d363-4258-acd7-45d946aa851b", "937eb72f-8c84-42ff-979c-74e235cf5d2b", "3250d162-a53d-4095-b7d4-29284cef8792", "80f61b93-5c13-4c13-a94e-a89553dec243", "701acdd0-badf-4138-9b89-2d665b832cc3"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "c5ce303c-ad6c-40dd-aa49-9e12eec83bfe": {"node_ids": ["aa9c193c-d6f3-4e56-bbec-91b154dc0a2e"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "9dacc483-da87-4551-8534-45462c631eea": {"node_ids": ["76e7a31e-38b4-41dc-ae82-38664ffabcd2", "a5cc26f4-4862-4fbf-97d0-d67637bf14a2"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "a171992a-3591-4a25-83be-636a42048c9b": {"node_ids": ["ad7185ff-1ab6-4925-8217-9fb039c28f47"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "033d406e-95b0-48f7-b993-eb307b58b658": {"node_ids": ["c546e1af-8537-4161-b2da-dbb3bc04d1ec"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "2bdccaa9-8179-43af-becb-947517d6ab29": {"node_ids": ["3c5165e0-121f-4207-94e5-81a92756a531"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "1a936e59-3c43-42b2-9a75-7ce502ef62b4": {"node_ids": ["0858a46f-6a9d-4907-90a1-6386fc5fcdc5", "0ac9a1af-9b0f-4eef-8734-05f4b649c364"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "82947c5a-974e-476d-b8ff-e4ddeecfd338": {"node_ids": ["0b71cb1e-55df-4435-a5f6-9e9da521d3bb", "9f21cfc5-7a60-415b-b610-477fa0624b2f"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "0e46527e-2f04-4c43-96d0-b596de07a7de": {"node_ids": ["b8975607-154e-4eaf-a87f-c0d6ee9ee9eb", "abf9a9c9-2ea4-4ffc-b52e-01544df2a36c"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "72a9d8da-62fd-4ffc-8985-27ea2399fae8": {"node_ids": ["0c39921d-d2e4-4f64-8e60-edf35a8dc280", "0cee5212-1b72-4996-82e3-90c0f22f0c3b"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "7c19e2b4-12da-4321-826f-5b370eadcfda": {"node_ids": ["b82a2947-9dee-465a-a58a-cd5e94d57623", "d7a90f2b-6ef2-4ec8-bd47-acac86917bb5"], "metadata": {"description": "// Reconnaissance domain (contains elements of Rock Sample / Mars Rover domains)\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// In the reconnaissance MDP, there is a 2d grid with an agent, \n// a base, some hazard squares, and objects in different locations.\n// The agent is equipped with 3 tools, one for detecting water, one for detecting\n// life, and one for taking a picture.  The agent's movements are deterministic\n// but the probability of getting a good reading from the life and water sensors \n// are stochastic.  Plus, running into a hazard OR being an adjacent square to one,\n// has a probability of damaging each sensor, which causes their false negative\n// rate to increase dramatically.  If the agent returns to the base it can repair\n// each tool individually.\n//\n// In the MDP version, when tools report a negative result, they contaminate the \n// object they were used on.  With the water tool, one negative result \n// means water will never be detected on that object, and 2 negative \n// results from the life detector similarly contaminates an object. \n// Hence, there is a strong reason not to use damaged tools.  Positive reward is \n// given for taking pictures of objects where life was detected and negative\n// reward is given for pictures where life has not been detected.\n//\n// The major planning decisions in this domain are:\n// 1) Choosing which objects to try the tools on.  \n// 2) Whether or not to repair the tools.\n// 3) Whether or not to risk damage to the tools by moving through hazards.\n//\n// This domain contains elements of the Rock Sample and Mars Rover domains:\n//\n// * Mars Rover reference:\n//\n//   John L. Bresina, Richard Dearden, Nicolas Meuleau, Sailesh Ramkrishnan, \n//   David E. Smith, Richard Washington: Planning under Continuous Time and \n//   Resource Uncertainty: A Challenge for AI. UAI 2002: 77-84.\n//   http://ti.arc.nasa.gov/static/asanicms/pub-archive/archive/2002-0339.pdf\n// \n// * Rock Sample reference:\n//\n//   Trey Smith and Reid G. Simmons: Heuristic Search Value Iteration for \n//   POMDPs.  UAI 2004.  http://www.cs.cmu.edu/~trey/papers/smith04_hsvi.pdf\n//\n//"}}, "9c903ede-5c20-4748-a114-aa6676812ea7": {"node_ids": ["9ea7df02-9a12-45e3-b916-e62ac2770a28", "11fd5d26-4837-4783-a61a-b0219f25c704", "4bcf8e40-381a-4712-816b-37878a30424b"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "70b409e0-4886-46c1-af30-7609043d7cbe": {"node_ids": ["c711ea87-076a-4a57-af9a-6437ba9fb728"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "aec571bd-a084-4a8b-8c3d-80fdd38296db": {"node_ids": ["4c8adbba-9ecc-4a58-8a8b-5c136496892a"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "9d442fb3-aa81-4b8a-91cf-8539cdc3b542": {"node_ids": ["6d29d547-7660-47bf-87b4-2a0da43c65a9"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "a6f4cf13-8330-4b9f-8376-2c9faaec52d5": {"node_ids": ["c8b979a5-d94a-4032-a1eb-48472e43da59"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "ee6b0828-3aa5-46ad-a086-b2aff9f5c5b7": {"node_ids": ["da801ed9-b23a-4bd4-8b8c-3e4ef7ac8ede"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "3d92f157-095c-41c6-ad0f-5579f681822b": {"node_ids": ["c38ead00-8652-4782-80e1-3d388ef027bf"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "760e0faa-dba3-467b-a73e-bbb6e5bf9ad7": {"node_ids": ["5a7be869-f1a5-4c92-8d37-55b1a39d68a2"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "6700ab6e-e8c4-4e50-911e-60e895caf106": {"node_ids": ["71f65d53-3dc9-491f-b0ab-01fe4b8990d5"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "ae861b4c-bbb2-4f4e-b8a5-7cb33c71a5a4": {"node_ids": ["2ccc3e48-e57a-41a8-b549-0b387b2fcfd3"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "e8970bd2-ddc9-4ec7-a886-74734c577316": {"node_ids": ["9a7bc50d-adbd-49d9-9e73-9c48cefdca4c"], "metadata": {"description": "//\n// Crossing Traffic Robot Navigation\n//\n// Author: Sungwook Yoon (sungwook.yoon [at] gmail.com)\n//\n// Modified for competition and translation purposes by Scott Sanner.\n//\n// In a grid, a robot (R) must get to a goal (G) and avoid obstacles (O)\n// arriving randomly and moving left.  If an obstacle overlaps with the\n// robot, the robot disappears and can no longer move around.  The robot\n// can \"duck\" underneath a car by deliberately moving right/east when\n// a car is to the right of it (this can make the solution interesting...\n// the robot should start at the left side of the screen then).  The robot\n// receives -1 for every time step it has not reached the goal.  The goal\n// state is absorbing with 0 reward.\n//\n// ****************\n// *            R * \n// *  <-O <-O <-O *\n// *    <-O   <-O *\n// * <-O    <-O   *\n// *     <-O  <-O *\n// *            G *\n// ****************\n//\n// You can think of this as the RDDL version of Frogger:\n//\n//   http://en.wikipedia.org/wiki/Frogger\n//\n//"}}, "0e8aef4d-f33b-4707-93d9-601d9cec0b34": {"node_ids": ["f805a092-451a-4e9c-b43c-76e21f96743b", "915693c2-07c6-4bd3-bfb9-7821e5c996c4", "6b9855d3-8ca2-4a48-972d-094a329a1dae", "d67da453-7b7b-4d95-8a03-6cd2bacbf83f", "c9c804f4-9482-46e3-b3a7-e0b9a03e484a"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "3454decf-3339-4011-8950-c757b494954f": {"node_ids": ["827f7d6c-2e71-4c37-86a1-846d963fac74"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "e2b65dd1-be59-4e27-b3f3-ee7ea3b5ffed": {"node_ids": ["ea85e861-a31a-49f1-bcea-cb88aa532bef"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "dabb47b5-df7c-4d6a-ae0a-3523a3f56a71": {"node_ids": ["01d3f905-6874-490a-8df0-ffb08bed369b"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "cbc0e080-9642-4480-9dc7-3d8d3291e033": {"node_ids": ["02ddf15b-ed7b-47a2-b809-520014d427fd"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "484c4621-a67c-4865-a4b5-c0304f8f5961": {"node_ids": ["1b43f3e2-2428-4ab3-b96a-187a87b126db"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "05aede4a-45f8-4c4c-950f-e347a9e65901": {"node_ids": ["58f1fe67-ee1d-4376-96b1-8b405ac2143f"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "79ecb65a-68f9-4a0b-9e89-bca2721d0e70": {"node_ids": ["f8bb3c28-c97f-4101-8d0b-4610277d25da"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "f1d799b6-bdbc-4bd9-9ee3-09bf53fc77ad": {"node_ids": ["73bdee10-6d14-4ee7-92e7-54b32b6c5f12"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "44c8cc16-cd61-4092-a3cd-c91558f7253b": {"node_ids": ["571ef8c0-d73e-4d49-8df0-4f828c8368ac"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "43e996c9-4077-4398-b7f2-cf1fb19d243d": {"node_ids": ["f45c9a17-eb5f-4cde-a17e-c19a94884a92"], "metadata": {"description": "// Elevator Domain\n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n//\n// Edited for competition and translation purposes by Scott Sanner.\n//\n// The \"elevators\" domain has a number of elevators delivering passengers \n// to either the top or the bottom floor (the only allowable destinations).\n// Potential passengers arrive at a floor based on Bernoulli draws \n// with a potentially different arrival probability for each floor.  \n//\n// The elevator can move in its current direction if the doors are closed,\n// can remain stationary (noop), or can open its door while indicating\n// the direction that it will go in next (this allows potential passengers\n// to determine whether to board or not).  Note that the elevator can only\n// change direction by opening its door while indicating the opposite \n// direction. \n//\n// A passable plan in this domain is to pick up a passenger every time \n// they appear and take them to their destination.  A better plan includes \n// having the elevator \"hover\" near floors where passengers are likely to \n// arrive and coordinating multiple elevators for up and down passengers.\n//\n// This domain was designed to support extension to multiple elevators\n// and may be used in either single or multi-elevator mode.\n//"}}, "67520dd1-cfec-49d6-9d40-a0ea907e520b": {"node_ids": ["cbffa9d4-6e37-476f-bb51-313d7592b715"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "cf8a2b13-b8db-4bf6-8ca3-23b817085865": {"node_ids": ["c704674a-d39a-49a1-8725-ab3cb59db398"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "d3b611e2-c0e3-41d6-87da-2068c38a9da0": {"node_ids": ["29a3533c-7201-468f-b0bf-6371bdf6ff1c", "ab1d5049-59c2-484c-ab5e-9c915156cdbd", "03d7f5d1-cedc-471f-bd4b-d8ab4f0bbe30", "5c8e31d0-14cd-4d70-8449-d6f57d1a19bc", "5b5533f4-9c8c-4f90-834a-5234003f3ce1"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "3c6913da-33d4-4e1f-b2ed-aa431e0e647b": {"node_ids": ["8166559f-689f-4923-be10-3464ac53348d"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "a2a375c1-9a18-423c-bf9c-7eb83fe2e533": {"node_ids": ["5e923a96-3ea6-46de-b35d-098a3c6fdffb"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "c7b7c6c8-917c-4718-ad37-77d919bbca4a": {"node_ids": ["5fe86272-f8fe-44ea-8dea-f78f291817ae", "c7245f55-14cd-4955-8336-6e510cbffbce", "27fee787-e2b8-4b3e-a3a0-62b6947902b5"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "6cc23001-518b-4ab4-9853-e1d69e9ca2b2": {"node_ids": ["d5fd340d-ce41-4f6e-bb0a-c3a2ebcfae1a", "facc419e-78a7-419c-ac16-c23755051f8f", "6ddc8cbc-3165-46f8-a6d4-9a0d050275be"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "b373fb97-7e0f-4685-89a2-84d10a145106": {"node_ids": ["792f14e3-361a-424f-a47a-7c2c434a373a", "029a94c3-83f9-432e-b31f-43363ed39bf8", "c63a1ce9-6451-4e1f-96d0-c3a0a10ade8b"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "e1dc5def-4ba0-4fda-a3c1-ff81b8c46d32": {"node_ids": ["dc697b9b-d9a7-47e3-922c-7c2bfaf342cc", "66a64a94-fe82-4f09-af53-4b2d23d05914", "18c5c911-ac59-4517-8e56-1edf2ca7558d", "a9e2837e-078c-4189-8f23-48896e9f608e", "5b6b8481-2fa7-4ad9-b0c0-f261e7dafea5"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "0777f580-1526-4acb-94eb-331f78f1bde6": {"node_ids": ["cc12134c-79e9-4c83-ade9-f22e23555424", "29b47d7c-a151-489e-b940-2c1caaae1368", "e6fdf905-7832-4676-b275-54bc2a882bdf", "3ac3dae5-c9da-49d1-a729-1d0b3f19b541", "9bf8b686-7209-4077-a1fd-3a46f2205907"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "14ace246-d7c5-4b5d-b750-3e4795e2d6af": {"node_ids": ["42da3e91-6ad8-4ddf-9860-ebdd3d442590", "9e769163-1759-442c-a118-beb1c7a10663", "72b124c1-6962-42fc-b351-43e43cba60c0", "dbf3f779-5b03-43fe-b409-44041797e372", "236ab30c-c795-4e00-824a-eba599684cd9"], "metadata": {"description": "// Game of Life Boolean POMDP\n//\n// A simple DBN to encode Conway's cellular automata \"game of life\" \n// on a grid.  One gets a reward for generating patterns that keep \n// the most cells alive.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "18d71101-b08d-4f4b-b097-ec770f400f1b": {"node_ids": ["f67a25b4-6a7f-4eb7-bfac-b0fcfde581b4", "5093d403-fa1b-4c55-a734-309cd7e9c39f", "b74a45a1-a6d7-4c2e-a63d-ff362cca0d0e"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "4a6ba5cc-7170-41ee-bea9-ea051ed6394a": {"node_ids": ["1e5a3fc0-dd9d-45fd-9b78-d3ab674b6fd1"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "d277908f-9e75-4db2-ac71-1610cd8017f5": {"node_ids": ["f19d0c72-290e-4331-b25c-b281a83c7358", "0c5fb965-3de2-4add-98b6-1cdd6f7cc3c6", "15a5c0d7-51c4-401f-a7be-9b5dd55dc95a", "a6361a14-10d0-48ce-9edf-28ba37f8d8df", "28266af3-571a-4be1-a0c8-b109a69299f8"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "154b557d-9b90-41c7-9626-4b0adc9e788b": {"node_ids": ["c0f888e1-5c95-41bf-baca-deac6f6d5557"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "47553964-3793-41d2-a284-6165612d1a74": {"node_ids": ["d1188255-3d7d-4aba-a56d-47c677f7b037"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "695bb35a-8a71-4d1d-82da-2bc699f404ac": {"node_ids": ["7de2885f-c2f1-4538-9dd1-bf9ef5bb9f64", "ade827da-f08c-4898-b1ba-8de933f3be08"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "17fcb1f3-98fa-4246-9ddb-972116280b92": {"node_ids": ["fbfa6d89-e710-4a1a-bf56-43958cf0191e"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "557e1cdd-dc48-4cff-912a-420b02eb869e": {"node_ids": ["4d012bfa-95e0-4682-b9af-b5adf95fc1a6", "fddfdf19-0c97-4792-b0b7-cf6a784a4c88"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "63b8f18b-e3f6-45a7-8cbb-f46400494271": {"node_ids": ["521d154e-458a-4840-a373-cd6ac9839c2c", "bfcb0f92-d9ae-4655-b4bd-9b1fd237a38e", "aed0be1a-b71d-4406-9e50-5986d5e5b682"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "813c12ae-09fd-41fa-86ab-0cfe7f758fa0": {"node_ids": ["d173dbb9-852b-4462-9366-adb9df3ae483", "3545b5f8-5b4f-4adb-8fe3-a0bbec4067ee", "6109f391-6fe8-4425-b959-e2698431710d"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "8342d416-b313-49ab-9337-880afb976b83": {"node_ids": ["d4701020-2d1c-42bb-a342-abd2aae3b734", "bcd4ecbe-c4d9-44d5-9dd3-1ee5b4884f12", "4810f6a0-da22-439f-a356-b8a8c9499178", "8706dd23-37b9-44c7-b557-1a014ac1a5dd"], "metadata": {"description": "//\n// Navigation MDP\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//\n// In a grid, a robot (R) must get to a goal (G).  Every cell offers\n// the robot a (different) chance of disappearing.  The robot needs\n// to choose a path which gets it to the goal most reliably within\n// the finite horizon time.\n//\n// ***********************\n// *  0   0   0   0   R  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  * \n// * .1  .3  .5  .7  .9  *\n// * .1  .3  .5  .7  .9  *\n// *  0   0   0   0   G  * \n// ***********************\n//\n// This is a good domain to test deteminized planners because\n// one can see here that the path using the .3 chance of failure\n// leads to a 1-step most likely outcome of survival, but\n// a poor 4-step change of survival (.7^(.4)) whereas the path\n// using a .1 chance of failure is much more safe.\n//\n// The domain generators for navigation have a flag to produce slightly \n// obfuscated files to discourage hand-coded policies, but \n// rddl.viz.NavigationDisplay can properly display these grids, e.g.,\n//\n//  ./run rddl.sim.Simulator files/final_comp/rddl rddl.policy.RandomBoolPolicy \n//        navigation_inst_mdp__1 rddl.viz.NavigationDisplay\n//\n// (Movement was not made stochastic due to the lack of intermediate\n//  variables and synchronic arcs to support both the PPDDL and SPUDD \n//  translations.)\n// \n//"}}, "af595f59-96f0-4ad5-b2fe-595af188c282": {"node_ids": ["e6c5946b-a3a0-4fc7-8eeb-b4dc83b2c934", "8d24e2e7-dabd-4c64-9d0e-3defac6d10aa", "e778b564-a77d-40a9-a6be-06d9b6f686e1"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "1f63904d-b61d-4390-a64a-3d17ec05955c": {"node_ids": ["cf6572da-1693-4bcd-83e9-94c4d74420ff"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "7bd8f822-b31f-4222-9459-aa9c63ad01f3": {"node_ids": ["dfd847e6-f636-45e3-ab68-c5cc60d6c43d", "93b64e36-3259-4e10-97e3-1eba300af2d6", "5614ab58-bc2e-47c2-98cb-160db9a0d561"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "e6c8e69e-c2d2-4984-9d58-18ed3f69a0ec": {"node_ids": ["388df85a-7c6f-40dc-b614-495177d33923"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "7b636796-19bc-48ab-982d-4afb8f11f79f": {"node_ids": ["ad897de4-9733-491b-898a-f5941f6095e7"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "accdef0a-adb2-46ee-9e97-c1478718c699": {"node_ids": ["8b3f046e-ad9c-432a-82cf-b1a8b736215b"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "ac3c5d25-9808-4aa2-9bbb-d11790a2927d": {"node_ids": ["96bd7eda-0367-4214-b279-61ae73bf4ae6", "b0d3f4b1-d799-43e2-9c42-4cadbc01c333"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "a4dc3ae8-b979-47ad-af56-f1dacb9a6e22": {"node_ids": ["207a472e-dcbd-421c-8941-1e922dc01d2d", "8d474c66-4e1d-4bbf-8f94-9a7c5c3bb5ea"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "3e44ad02-ee97-4c02-9ae1-8930d56a3c02": {"node_ids": ["ab650d6f-7533-4afe-9813-3f41d37a3542", "35aff9dd-35c9-4525-8c3c-03105d4edcc7"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "e0560772-764c-463e-b831-a8178937b7d4": {"node_ids": ["d7d0ba13-a46e-40eb-87b3-ca533b8ad8e7", "a3d2c23f-d9ce-4b2f-b04f-8029ff4b9f31", "28c8e780-953d-40d7-a465-a71710f1e7db"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "77f8abb2-f02a-439c-a585-319ef7020a2b": {"node_ids": ["4fb46b48-1cd3-43de-8af5-f9b96e548faa", "f822e542-7938-4bf1-9a1c-42fe9a5232b7", "6d757296-6ed4-451b-bd32-a5c52d2318cd"], "metadata": {"description": "// Skill Teaching Domain \n//\n// Author: Tom Walsh (thomasjwalsh [at] gmail.com)\n// Special thanks to Derek Green and Paul Cohen at \n// University of Arizona for help with the design.\n//\n// In the SkillTeaching MDP domain, the agent is trying to teach a series \n// of skills to a student through the use of hints and multiple choice\n// questions.  The student has a proficiency level for each skill, which \n// indicates his ability to answer questions of that skill and positive\n// reward is given for high proficiency on skills while negative reward \n// is given for low proficiency.  Each skill also has a weight on \n// how much it is worth. \n//\n//  Many of the skills are connected in that some are\n// ``pre-conditions'' of others.  If all of a skill's \n// pre-conditions are learned, the student has some probability \n// of answering questions about it right, and each precondition\n// that is at high proficiency adds to the probability though \n// knowing all of them can lead to a probability higher than the sum\n// of the components.  Hints only work if all the preconditions \n// are known and can only get you to medium proficiency.\n//\n// student proficiency increases with questions answered right and \n// decreases with questions about a skill answered wrong and \n// sometimes decreases by chance.\n//\n// To model the teacher-student interaction, every other step in the\n// domain is the student's turn, where they answer a question.  \n//\n// The planning problems here are:\n// 1) Whether or not to teach all the prerequisites of a skill before\n//    teaching it.\n// 2) What skill to focus on next\n// 3) When to give hints and when to use multiple choice problems\n//\n//"}}, "10ad1a3b-5890-4bbe-86a4-fa5804de378a": {"node_ids": ["f7ac9ba5-d01e-4d89-9589-6be215a991dc"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "c3ba77db-2be1-4d05-af44-1c8934130a30": {"node_ids": ["dfd7e271-868e-4a7d-93e4-f20ef7da4686"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "5a04119e-10eb-4d21-ac85-6a31aa62323c": {"node_ids": ["abf985da-680c-4ab9-a966-4883d746e7d2", "de069ee8-15e0-4a77-b460-025085a77607", "e8cad10f-ff3c-4bc2-b99c-cdb5de573e44"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "fd045767-fee8-42ca-8c2d-f37d35d96f27": {"node_ids": ["34e80170-48f2-49e7-bb72-69e0534ad3b9"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "b50c87bf-bbeb-421f-9d26-e7977876a719": {"node_ids": ["d9ddbd84-288b-48d8-b701-c981fdd012ab"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "27f46b58-61aa-4430-9c6f-873c68eb215d": {"node_ids": ["0b75435b-e088-436c-89d9-b1086b434674"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "7b1ed82c-a0d9-4eb1-8eac-7fd7d6affd80": {"node_ids": ["9e3f65eb-97ce-4094-b82b-2f6e3821a049"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "7e0fb88f-703b-4650-ab94-c22f631a0b06": {"node_ids": ["480a2e20-07f5-40c3-a2ae-7545892c6f50", "6ef5e339-cb13-41c2-a2f7-578d1ac83ba1"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "948df15a-be8c-4f88-b3e4-9355ee98ac2c": {"node_ids": ["e928b1d1-a4b7-4da4-819a-32bdbf76b8ef", "fb09da69-8d87-48d3-b930-d74f1818aa3e"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "62797bc1-3ba9-4c2d-b966-f2d4647412f5": {"node_ids": ["00d07363-1f81-4911-9c6b-fdbff23b8f1e", "467e535b-0cc5-41c4-8591-6aec81837bc5", "6db883b5-f8ea-439a-bae2-e0beba4f1737"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "b1206b8e-d52b-41ed-b71f-9004c9955b76": {"node_ids": ["ced29a7b-3cd1-434c-9bf7-b8c084f66827", "a469519b-76ac-4572-8a11-37d9989c63ab"], "metadata": {"description": "// SysAdmin Boolean MDP \n//\n// An example RDDL description for the well-known SysAdmin problem\n// (Guestrin, Koller, Parr, IJCAI-01).\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "16d4c693-e855-46f8-8439-c17d78b39714": {"node_ids": ["5ca6df4c-9ebe-456e-ab22-8b6f7564a99b", "4896015e-29c6-4834-bf04-6663361e199a", "01bd8dc9-3e21-41fb-84f6-fb6d5b8e56b7"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "f57e34bb-b97f-44dc-8f63-38cb2a3086ce": {"node_ids": ["1897c040-236d-447e-ae6f-bb8bc9e91de3", "d5ed5758-34db-4f3d-9f4a-6c30293b846a"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "3b3b2f75-4fad-430c-a8cb-b92f356a8be0": {"node_ids": ["5b2570f0-a410-48f7-9374-c353541636a1", "1fd7682c-ca4f-4dac-9811-2acaf94365ea", "79be6e7a-a858-444b-a485-947c9f10fa6d", "d7e7d15d-540a-4439-a376-2938a16e888b"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "23af0791-bc75-461b-b544-b96bfcefad33": {"node_ids": ["d6c523c5-4a40-4f62-8aa9-11ffc3c634c6", "6f5a9f63-51d1-4780-be50-7a515ebe9b86"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "7d65309a-30f3-4042-9418-7357a09a55fb": {"node_ids": ["e8e3b501-714b-4c95-b5e4-ee363f0bfb9e", "a00a814d-74d9-40b5-8149-c35d9f3d6d92"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "ce509453-c3c8-4755-a4af-2b30c81d511a": {"node_ids": ["45fbc532-0c50-45df-92e0-517a89809400", "e18c9b52-1936-42d9-9d21-44261de37f05"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "2a8a161c-3e2d-4920-b05a-1aa82b3c1c33": {"node_ids": ["c08946c8-2884-4f54-aa95-bdde7eaa708f", "3d4c6473-e909-4d95-a17f-76ad7eaebc32", "0ec41f89-cb7d-4c05-99c2-a987a86a61c3"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "6df731ef-d2b7-4d19-9a5d-228650b089d4": {"node_ids": ["dcd40d9e-1ffb-413a-a322-3942959ae138", "3974065c-d586-45f5-9973-c393091b3fa8", "4f54352d-70a9-4611-b913-392e982160c0"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "88627a8d-7413-4d6e-98a5-abec56874b3d": {"node_ids": ["0076127e-a61a-4746-9a49-a6a3165dd4c1", "0d8d73a2-270a-48d5-8b39-7102d1eb81fc", "ff855896-fac4-4b18-bd18-1345ae2f2f15", "67026b4b-9a29-4ee6-9f9a-a9225bfc2905"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "8553a56e-f227-4bb6-9150-43d1dd6ebcba": {"node_ids": ["312c279d-ded8-4747-9202-05446d614c2d", "25ed184e-4b92-4430-bec7-bfe9eea868a8", "17a23ce6-88ff-4ef8-8df4-6b083d6a825f", "715490fc-5e71-4eb1-a421-cd770317c340"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}, "fc6ffe91-1fa2-4a8d-a328-db99a4b52247": {"node_ids": ["3e1761b3-ffce-4e80-9890-147a2c920c2d", "187d2c3a-6d9d-4a4e-a8fd-55f9ce93feb0", "ad03d4b4-459c-42ca-8dac-aa586bede9fd", "f2d3db21-d2f0-4ea6-87da-7743ac38e862"], "metadata": {"description": "// A simple binary version of the cell transition model (CTM) for \n// modeling traffic.  Based on the original CTM Tech Report and its \n// specification as a factored MDP in the following papers:\n//\n//   The Cell Transition Model: Network Traffic.  Daganzo; \n//   Tech Report Berkeley Institute of Transport Studies, 1994.\n//\n//   Efficient Solutions to Factored MDPs with Imprecise Transition \n//   Probabilities.  Delgado, Sanner, de Barros, Cozman; ICAPS, 2009.\n//\n// Because of the binary variable and no intermediate variable\n// restrictions for the IPPC 2011, this model is quite simplified\n// and ignores traffic aspects such as turns and turn probabilities.\n//\n// Note that this model uses concurrent actions, but that the number\n// of total actions will only ever be 2^(# intersections).  Refer to\n// the IPPC email list if you are unsure how to handle concurrent\n// actions.\n//\n// Author: Scott Sanner (ssanner [at] gmail.com)\n//"}}}}